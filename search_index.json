[
["index.html", "eRum2020 Program 1 eRum2020 Program Overview", " eRum2020 Program eRum2020 organizing Committee 2020-03-19 14:22:01 1 eRum2020 Program Overview Overview of accepted contributions organized by track and session type. "],
["lightning-talk.html", "1.1 Lightning talk", " 1.1 Lightning talk 1.1.1 An enriched disease risk assessment model based on historical blood donors records Andrea Cappozzo, PhD student at University of Milan-Bicocca Track(s): R Applications Abstract: Historically, the medical literature has largely focused on determining risk factors at an illness-specific level. Nevertheless, recent studies suggested that identical risk factors may cause the appearance of different diseases in different patients (Meijers &amp; De Boer, 2019). Thanks to the joint collaboration of Heartindata, a group of data scientists offering their passion and skills for social good, and Avis Milano, the Italian blood donor organization, an enriched disease risk assessment model is developed. Multiple risk factors and donations drop-out causes are collectively analyzed from AVIS longitudinal records, with the final aim of providing a broader and clearer overview of the interplay between risk factors and associated diseases in the blood donors population. 1.1.2 Next Generation Supply Chain Planning With R: A Case Study Benedikt Heller, Data Strategy Consultant at Continental Track(s): R Production, R Machine Learning &amp; Models Abstract: Demand forecasting is an integral part of successful Supply Chain Planning — good estimates of future demands allow businesses to produce and store their goods efficiently, which saves resources, facilitates growth, and keeps customers happy. Recent developments in forecasting made it possible to push the envelope on demand forecasting. In 2018, Makridakis and colleagues launched the 6-month-long M4 Competition, a call to researchers and enthusiasts to predict a set of time series, which provided new insights and — thanks to publicly available source code — paved the way for others to test successful algorithms in different domains. In this talk, we will present a case study of implementing an M4-inspired forecasting solution at the German automotive manufacturing company Continental, which successfully improved forecasting accuracy. The talk covers the significant findings of the M4 Competition, presents our implementation, and summarizes project insights. 1.1.3 rdwd: R interface to German Weather Service data Berry Boessenkool, R trainer &amp; consultant Track(s): R Applications Abstract: rdwd is an R package to handle data from the German Weather Service (DWD). It allows to easily select, download and read observational data from over 6k weather stations. Both current data and historical records (partially dating back to the 1860s) are handled. Since about a year, gridded data from radar measurements can be read as well. 1.1.4 tv: Show Data Frames in the Browser Christoph Sax, R-enthusiast, economist @cynkra Track(s): R World Abstract: The tv package lively displays data frames during data analysis. It modifies the print method of data frames, tibbles or data tables to also appear in a browser or in the view pane of RStudio. This is similar in spirit to the View() function in RStudio, works in other development environments, and has several advantages. Changes in data frame are shown immediately and next to the script and the console output, rather than on top of them. The display keeps the position and the width of columns if a modified data frame is shown in tv. It is updated asynchronously, without interrupting the analysis workflow. 1.1.5 Predicting the Euro 2020 results using tournament rank probabilities scores from the socceR package Claus Ekstrøm, Statistician at University of Copenhagen. Longtime R hacker. Track(s): R Applications Abstract: The 2020 UEFA European Football Championship will be played this summer. Football championships are the source of almost endless predictions about the winner and the results of the individual matches, and we will show how the recently developed tournament rank probability score can be used to compare predictions. Different statistical models form the basis for predicting the result of individual matches. We present an R framework for comparing different prediction models and for comparing predictions about the Euro results. Everyone is encouraged to contribute their own function to make predictions for the result of the Euro 2020 championship. Each contributer will be shown how to provide two functions: a function that predicts the final score for a match between two teams with different skill levels, and a function that updates the skill levels based on the results of a given match. By supplying these two functions to the R framework the prediction results can be compared and the winner of the best football predictor can be found when Euro 2020 finishes. 1.1.6 R in Medical Research: UseRs and StakeholdeRs Filippo Chiarello, University of Pisa Track(s): R World, R Life Sciences Abstract: Background: The use of R has increased over the last decade in many applied fields, including medicine, and in both academia and industry. With this study, we want to determine who the subjects in medical research are, who are involved in or affected by the R programming language. Methods: First, based on the Scopus database we identify to what extent and in which application areas within the medical field, R is used for statistical analysis. Second, applying Tidy Natural Language Processing techniques, we identify phrases that refer to actual or potential stakeholders (e.g. women, patients). Results are mapped in a graph where nodes represent stakeholders, edges the relationships (co-occurrences) between them. Further classification is done in order to discriminate between generic and specific stakeholders and to identify macro-classes of stakeholders (e.g. gender, professionals). Results: Preliminary results show an increase in the use of R in medicine, differing by research areas. A large number of stakeholders are described in the scientific literature with a different granularity of stakeholders of R. Conclusion: The results on R users and the map of stakeholders within the medical field shall be the start of further discussion within the R community about the actual (and future) impact that this language has on medical research and healthcare. 1.1.7 Ultra fast penalized regressions with R package {bigstatsr} Florian Privé, Postdoc at Aarhus University Track(s): R Machine Learning &amp; Models Abstract: In this talk, I introduce the implementations of penalized linear and logistic regressions as implemented in R package {bigstatsr}. These implementations use data stored on disk to handle very large matrices. They automatically perform a procedure similar to cross-validation to choose the two hyper-parameters, λ and α, of the elastic net regularization, in parallel. They employ an early stopping criterion to avoid fitting very expensive models, making these implementations on average 10 times faster than with {glmnet}. However, package {bigstatsr} does not implement all the many models and options provided by the excellent package {glmnet}; some are area of future development. 1.1.8 Supporting Twitter analytics application with graph-databases and the aRangodb package Gabriele Galatolo, Kode Srl, Software Developer &amp; Data Scientist Track(s): R Applications Abstract: The importance of finding efficient ways to model and to store unstructured data has incredibly grown in the last decade, in particular with the strong expansion of social-media services. Among those storing tools an increasingly important class of databases is represented by the graph-oriented databases, where relationships between data are considered first-class citizens. In order to support the analyst or the data scientist to interact and use in a simple way with this paradigm, we developed last year the package aRangodb, an interface with the graph-oriented database ArangoDB. To show the capabilities of the package and of the underlying way to model data using graphs we present Tweetmood, a tool to analyze and visualize tweets from Twitter. In this talk, we will present some of the most significant features of the package applied in the Tweetmood context, such as functionalities to traverse the graph and some examples in which the user can elaborate those graphs to get new information that can easily be stored using the functions and the tools available in the package. 1.1.9 Deep learning and time series approaches for improvement of vehicle distribution process Julia Fumbarev, BMW Group, Data scientist Track(s): R Applications, R Machine Learning &amp; Models Abstract: Accurate planning of transport- and stock duration is a ubiquitous problem in vehicle distribution. As BMW Group faces the annual delivery volumes of 2.7 million vehicles, the importance of process optimization through scaling up to such high volumes becomes self-evident. Precise forecasts not only ensure efficient resource allocation, but can play a crucial role in process delay resolution: If we are aware of a delay in the delivery of a vehicle to a customer at an early stage, high short-term costs can be avoided by taking corresponding measures e.g. acceleration of the follow-up process, early customer’s delay notice etc. To improve predictions regarding the process times and arrival events, we relied on state-of-the-art approaches of deep learning and time series analysis. The methodology was applied to two use cases: predictions of inflows into ports and event date predictions. For the former, we were able to achieve more efficient utilization of the ships while for the latter, a more reliable prognosis ensures higher customer satisfaction which is the focus of BMW. The algorithm’s performance was evaluated by computing prediction accuracy for the respective use case. The main contribution of our work is delivering a more reliable forecast that has a positive impact on the entire supply chain and, consequently, can generate large business value. 1.1.10 What are the potato eaters eating Keshav Bhatt, R-fan and independent researcher Track(s): R Applications, R Dataviz &amp; Shiny Abstract: Although stereotypes can quite useful they are often not correct. For instance, the Dutch are stereotyped as being potato eaters. While this might have been historically correct, it is not currently accurate. The Dutch sparingly eat potatoes and this paper uses data to disprove the stereotype. To get an impression of Dutch food habits, a popular local website was scraped. Besides its popularity, the website hosts user-generated content, giving a good proxy of Dutch taste-buds. While it was apparent on the website, lasagna is the most popular dish. Detailed NLP analysis of more than 50,000 recipes showed that potato based dishes are in fact nowhere at the top. This vindicated my belief. Moreover, it shows that the Dutch kitchen is globalizing. Tomato, a hallmark of South Europe is more popular than the Dutch potato. Also observed is the popularity of many herbs in the recipes, which are not a traditional component of the Dutch kitchen. The world is changing and our kitchens too. This trend will also be explored for other countries also. 1.1.11 dm: working with relational data models in R Kirill Müller, Clean code, tidy data. Consulting for cynkra, coding in the open. Track(s): R Applications, R Production, R World Abstract: Storing all data related to a problem in a single table or data frame (“the dataset”) can result in many repetitive values. Separation into multiple tables helps data quality but requires “merge” or “join” operations. {dm} is a new package that fills a gap in the R ecosystem: it makes working with multiple tables just as easy as working with a single table. A “data model” consists of tables (both the definition and the data), and primary and foreign keys. The {dm} package combines these concepts with data manipulation powered by the tidyverse: entire data models are handled in a single entity, a “dm” object. Three principal use cases for {dm} can be identified: When you consume a data model, {dm} helps access and manipulate a dataset consisting of multiple tables (database or local data frames) through a consistent interface. When you use a third-party dataset, {dm} helps normalizing the data to remove redundancies as part of the cleaning process. To create a relational data model, you can prepare the data using R and familiar tools and seamlessly export to a database. The presentation revolves around these use cases and shows a few applications. The {dm} package is available on GitHub and will be submitted to CRAN in early February. 1.1.12 Explaining black-box models with xspliner to make deliberate business decisions Krystian Igras, Data Scientists and Software Engineer at Appsilon Track(s): R Machine Learning &amp; Models Abstract: A vast majority of the state of the art ML algorithms are black boxes, meaning it is difficult to understand their inner workings. The more that algorithms are used as decision support systems in everyday life, the greater the necessity of understanding the underlying decision rules. This is important for many reasons, including regulatory issues as well as making sure that the model learned sensible features. You can achieve all that with the xspliner R package that I have created. One of the most promising methods to explain models is building surrogate models. This can be achieved by inferring Partial Dependence Plot (PDP) curves from the black box model and building Generalised Linear Models based on these curves. The advantage of this approach is that it is model agnostic, which means you can use it regardless of what methods you used to create your model. From this presentation, you will learn what PDP curves and GLMs are and how you can calculate them based on black box models. We will take a look at an interesting business use case in which we’ll find out whether the original black box model or the surrogate one is a better decision system for our needs. Finally, we will see an example of how you can explain your models using this approach with the xspliner package for R (already available on CRAN!). 1.1.13 Using open-access data to derive genome composition of emerging viruses Liam Brierley, MRC Skills Development Fellow, University of Liverpool Track(s): R Life Sciences Abstract: Outbreaks of new viruses continue to threaten global health, including pandemic influenza, Ebola virus, and the novel coronavirus ‘nCoV-2019’. Advances in genome sequencing allow access to virus RNA sequences on an unprecedented scale, representing a powerful tool for epidemiologists to understand new viral outbreaks. We use NCBI’s GenBank, a curated open-access repository containing &gt;200 million genetic sequences (3 million viral sequences) directly submitted by users, representing many individual studies. However, the resulting breadth of data and inconsistencies in metadata present consistent challenges. We demonstrate our approach using R to address these challenges and a need for reproducibility as data increases. Firstly, we use rentrez to programmatically search, filter, and obtain virus sequences from GenBank. Secondly, we use taxize to resolve pervasive problems of naming conflicts, as virus names are often recorded differently between entries, partly because virus classification is complex and regularly revised. We successfully resolve 428 mammal and bird RNA viruses to species level before extracting sequences. Obtaining genome sequences of a large inventory of viruses allows us to estimate genomic composition biases, which show promise in predicting virus epidemiology. Ultimately, this pathway will allow better quantification of future epidemic threats. 1.1.14 A principal component analysis based method to detect biomarker captation from vibrational spectra Marco Calderisi, Kode srl, CTO Track(s): R Life Sciences Abstract: BRAIKER is a microfluidics-Based biosensor aimed to detect biomarkers. The device is responsive to changes of mass and viscosity over its surface. When selected markers react with the sensor, a variation of resonant acoustic frequencies (called harmonics) is produced. A serious problem when examining the data produced by biosensors is the subjectivity of standard method to evaluate the pattern of harmonics. In our research, a method based on the principal component analysis has been applied on vibrational data. An R-Shiny application was developed in order to present data visualizations and multivariate analyses of vibrational spectra. The Shiny application allows to clean and explore data by using interactive data visualisation tools. The principal component analysis is applied to analyse simultaneously the full set of frequencies for multiple experimental runs, reducing the multivariate data set into a small number of components accounting for a component of variance near to that the original data. Functionalised and non-functionalised resonating foils of biosensor can be classified in order to validate the capability of the device to detect biomarkers, lowering the LOD and increasing sensitivity and resolution. 1.1.15 Emojis; show your emotions! 😀 Maria Prokofieva, RLadies Melbourne Australia, Victoria University (Australia) Track(s): R World Abstract: This talk will share the experience of working with emojis in R. Emoji are ideograms and smileys that are used EVERYWHERE these days, so if you have a text to analyse, you have to FACE 😀them. This talk will share a research project of analysing business related communication with emojis and ways to integrate emojis in your text analysis. We will talk briefly about common ways to deal with emojis as well as R packages available. Importantly, how to derive meaning from emojis? 👍🧐👿 Trust words or emojis? ❤️😡. This will be a lightening talk that everyone will enjoy 😂😇😂 1.1.16 An innovative way to support your sales force Matilde Grecchi, Head of Data Science &amp; Innovation @ZucchettiSpa Track(s): R Production, R Dataviz &amp; Shiny, R Machine Learning &amp; Models, R Applications Abstract: Explanation of the web application realized in Shiny and deployed in production to support the sales force of Zucchetti. An overview of the overall step followed from data ingestion to modeling, from validation of the model to shiny web-app realization, from deployment in production to continous learning thanks to feedbacks coming from sales force and redemption of customers. All the code is written in R using RStudio. The deployment of the app is done with ShinyProxy.io 1.1.17 ptmixed: an R package for flexible modelling of longitudinal overdispersed count data Mirko Signorelli, Dept. of Biomedical Data Sciences, Leiden University Medical Center Track(s): R Machine Learning &amp; Models, R Life Sciences Abstract: Overdispersion is a commonly encountered feature of count data, and it is usually modelled using the negative binomial (NB) distribution. However, not all overdispersed distributions are created equal: while some are severely zero-inflated, other exhibit heavy tails. Mounting evidence from many research fields suggests that often NB models cannot fit sufficiently well heavy-tailed or zero-inflated counts. It has been proposed to solve this problem by using the more flexible Poisson-Tweedie (PT) family of distributions, of which the NB is special case. However, current methods based on the PT can only handle cross-sectional datasets and no extension for correlated data is available. To overcome this limitation we propose a PT mixed-effects model that can be used to flexibly model longitudinal overdispersed counts. To estimate this model we develop a computational pipeline that uses adaptive quadratures to accurately approximate the likelihood of the model, and numeric optimization methods to maximize it. We have implemented this approach in the R package ptmixed, which is published on CRAN. Besides showcasing the package’s functionalities, we will present an assessment of the accuracy of our estimation procedure, and provide an example application where we analyse longitudinal RNA-seq data, which often exhibit high levels of zero-inflation and heavy tails. 1.1.18 One-way non-normal ANOVA in reliability analysis using with doex Mustafa CAVUS, PhD Student @Eskisehir Technical University Track(s): R Production, R Life Sciences, R Applications Abstract: One-way ANOVA is used for testing equality of several population means in statistics, and current packages in R provides functions to apply it. However, the violation of its assumptions are normality and variance heterogeneity limits its use, also not possible in some cases. doex provides alternative statistical methods to solve this problem. It has several tests based on generalized p-value, parametric bootstrap and fiducial approaches for the violation of variance heterogeneity and normality. Moreover, it provides the newly proposed methods for testing equality of mean lifetimes under different failure rates. This talk introduces doex package provides has several methods for testing equality of population means independently the strict assumptions of ANOVA. An illustrative example is given for testing equality of mean of product lifetimes under different failure rates. 1.1.19 Towards more structured data quality assessment in the process mining field: the DaQAPO package Niels Martin, Postdoctoral researcher Research Foundation Flanders (FWO) - Hasselt University Track(s): R Applications Abstract: Process mining is a research field focusing on the extraction of insights on business processes from process execution data embedded in files called event logs. Event logs are a specific data structure originating from information systems supporting a business process such as an Enterprise Resource Planning System or a Hospital Information System. As a research field, process mining predominantly focused on the development of algorithms to retrieve process insights from an event log. However, consistent with the “garbage in - garbage out”-principle, the reliability of the algorithm’s outcomes strongly depends upon the data quality of the event log. It has been widely recognized that real-life event logs typically suffer from a multitude of data quality issues, stressing the need for thorough data quality assessment. Currently, event log quality is often judged on an ad-hoc basis, entailing the risk that important issues are overlooked. Hence, the need for a more structured data quality assessment approach within the process mining field. Therefore, the DaQAPO package has been developed, which is an acronym for Data Quality Assessment of Process-Oriented data. It offers an extensive set of functions to automatically identify common data quality problems in process execution data. In this way, it is the first R-package which supports systematic data quality assessment for event data. 1.1.20 Analyzing Preference Data with the BayesMallows Package Øystein Sørensen, Associate Professor, University of Oslo Track(s): R Machine Learning &amp; Models Abstract: BayesMallows is an R package for analyzing preference data in the form of rankings with the Mallows rank model, and its finite mixture extension, in a Bayesian framework. The model is grounded on the idea that the probability density of an observed ranking decreases exponentially with the distance to the location parameter. It is the first Bayesian implementation that allows wide choices of distances, and it works well with a large number of items to be ranked. BayesMallows handles non-standard data: partial rankings and pairwise comparisons, even in cases including non-transitive preference patterns. The Bayesian paradigm allows coherent quantification of posterior uncertainties of estimates of any quantity of interest. These posteriors are fully available to the user, and the package comes with convenient tools for summarizing and visualizing the posterior distributions. This talk will focus on how the BayesMallows package can be used to analyze preference data, in particular how the Bayesian paradigm allows endless possibilities in answering questions of interest with the help of visualization of posterior distributions. Such posterior summaries can easily be communicated with scientific collaborators and business stakeholders who may not be machine learning experts themselves. 1.1.21 Supporting R in the Binder Community Sarah Gibson, Research Software Engineer and 2020 Software Sustainability Institute Fellow Track(s): R World Abstract: Binder is a project developed for the community by the community. It supports the sharing of reproducible computational analyses over the cloud in a variety of programming languages. The users of mybinder.org (the free and public Binder service) are in a privileged position to suggest the features they want to see, and even contribute them to the project. That being said, the maintainers and operators of mybinder.org are primarily skilled in Python. I would like to use my lightning talk session to begin a conversation with the R community on how Binder can better support the needs of R language users, and how they can become involved in the maintenance and development of such a project. 1.1.22 Flexible deep learning via the JuliaConnectoR Stefan Lenz, Statistician at the Institute of Medical Biometry and Statistics (IMBI), Faculty of Medicine and Medical Center – University of Freiburg Track(s): R Machine Learning &amp; Models Abstract: For deep learning in R, frameworks from other languages, e. g. from Python, are widely used. Julia is another language which offers computational speed and a growing ecosystem for machine learning, e. g. with the package “Flux”. Integrating functionality of Julia in R is especially promising due to the many commonalities of Julia and R. We take advantage of these in the design of our “JuliaConnectoR” R package, which aims at a tight integration of Julia in R. We would like to present our package, together with some deep learning examples. The JuliaConnectoR can import Julia functions, also from whole packages, and make them directly callable in R. Values and data structures are translated between the two languages. This includes the management of objects holding external resources such as memory pointers. The possibility to pass R functions as arguments to Julia functions makes the JuliaConnectoR a truly functional interface. Such callback functions can, e. g., be used to interactively display the learning process of a neural network in R while it is trained in Julia. Among others, this feature sets the JuliaConnectoR apart from the other R packages for integrating Julia in R, “XRJulia” and “JuliaCall”. This becomes possible with an optimized communication protocol, which also allows a highly efficient data transfer, leveraging the similarities in the binary representation of values in Julia and R. 1.1.23 Time Series Missing Data Visualizations Steffen Moritz, Institute for Data Science, Engineering, and Analytics, TH Köln Track(s): R Dataviz &amp; Shiny, R Applications Abstract: Missing data is a quite common problem for time series, which usually also complicates later analysis steps. In order to deal with this problem, visualizing the missing data is a very good start. Visualizing the patterns in the missing data can provide more information about the reasons for the missing data and give hints on how to best proceed with the analysis. This talk gives a short intro into the new plotting functions being introduced with the 3.1 version of the imputeTS CRAN package. 1.1.24 effectclass: an R package to interpret effects and visualise uncertainty Thierry Onkelinx, Statistician at the Research Institute for Nature and Forest Track(s): R Dataviz &amp; Shiny Abstract: The package classifies effects by comparing their confidence interval with a reference, a lower and an upper threshold, all of which are set by the user a priori. The null hypothesis is a good choice as reference. The lower and upper threshold define a region around the reference in which the effect is small enough to be irrelevant. These thresholds are ideally based on the effect size used in the statistical power analysis of the design. Otherwise they can be based on expert judgement. The result is a ten-scale classification of the effect. Three classes exist for significant effects above the reference and three classes for significant effects below the reference. The remaining four classes split the non-significant effects. The most important distinction is between “no effect” and “unknown effect”. effectclass provides ggplot2 add-ons stat_effect() and scale_effect() to visualise the effects as points with shapes depending on the classification. It provides stat_fan() which displays the uncertainty as multiple overlapping intervals with different confidence probability. stat_fan() is inspired by Britton, E.; Fisher, P. &amp; J. Whitley (1998) More details on the package website: https://effectclass.netlify.com/ Britton, E.; Fisher, P. &amp; J. Whitley (1998). The Inflation Report Projections: Understanding the Fan Chart. Bank of England Quarterly Bulletin. "],
["poster.html", "1.2 Poster", " 1.2 Poster 1.2.1 Modified likelihood ratio model for handwriting recognition in forensic science. Adeyinka Abiodun, The University of Ibadan, Oyo state Nigeria, Research student Track(s): R Machine Learning &amp; Models Abstract: My session will focus on Forensic Handwriting recognition in which I adopted the Modified Likelihood Ratio (LR) Method in order to quantify the strength of evidence in a forensic investigation. Existing methods for estimating LR in handwriting identification employed nuisance parameters resulting into high rate of inconclusiveness and disagreement among forensic investigators. Currently, LR procedures rely on the choice of appropriate denominators that limit the repeatability and reproducibility of the estimated LR. Therefore, my work adopted a modified LR devoid of nuisance parameter and capable of generating consistent estimate. A total of 230 document writers were purposively selected to produce 10 paged true and disguised documents over a period of six months. Similar procedure was carried out to produce forged document for the corresponding true counterparts. I used C-means to cluster handwriting into characters based on segmented words. Local binary pattern was used to extract features from the clustered characters and extracted features were fed into a Back Propagation Neural Network (BPNN) to learn the handwriting pattern. I also developed an exhaustive mapping algorithm with bias function to replace the hitherto randomly selected denominator for the LR estimation. My model was trained and tested for repeatability and reproducibility via accuracy and discriminating power for both hypothesis of prosecutor (Hp) and hypothesis of defense (Hd).The modified LR using Kernel Density Estimator (KDE) and Logistic Regression (LoR) estimator outperformed the existing procedures in literature. 1.2.2 The R-package ‘FlexReg’: regression mixture models for bounded responses Agnese Maria Di Brisco, Postdoctoral researcher Track(s): R Machine Learning &amp; Models Abstract: The analysis of data defined on bounded intervals (such as proportions or rates) is challenging since classical linear regression models are unsuitable. A fruitful alternative to data transformation is the definition of a regression model based on distributions with proper support, like the beta, the flexible beta (FB) and the variance inflated beta (VIB), where the latter two are mixtures of betas showing a good fit even in the presence of multimodality, heavy tails and/or outliers. This talk illustrates the FlexReg package, which allows to fit Beta, FB, and VIB regression models. The core function of the package is ‘flexreg’, which performs a Bayesian estimation via Hamiltonian Monte Carlo (HMC) algorithm through rstan package. Among the many arguments of the function, there are the model (Beta, FB or VIB) and the link functions for the mean and for the precision parameters. Also, the function allows specifying several prior distributions, the hyperparameters, and many settings of the HMC algorithm such as the number of iterations and of chains. The FlexReg package includes several functions to compute fitting criteria, posterior predictive distributions and residuals. At last, functions that provide simple and clear plots of the regression curves, posterior predictive and residuals are available. All the features of the FlexReg package are illustrated through data from the literature. 1.2.3 Using R for analysis of microscopic images of Streptomyces growth and chromosome distribution Agnieszka Strzałka, University of Wrocław Track(s): R Life Sciences Abstract: Streptomyces are soil dwelling bacterias known as producers of antibiotics. However, Streptomyces life cycle is more complicated than most model bacterial species. It starts from a single spore, which after germination creates a branched mycelium comprised of elongated cells possessing multiple copies of chromosome. Only after nutrients depletion another type of cell is produced which divides into spores containing only one copy of the chromosome. In order to study chromosomes distribution we have used a fluorescently labelled protein which binds to the chromosome. This allowed us to follow the movement of chromosomes in the cell. Most software dedicated for analysis of fluorescent microscopy data were created for rod shaped bacteria and are unsuitable for branching bacteria such as Streptomyces. To solve this problem our workflow combined usage of ImageJ and R, which allowed us to collect fluorescence intensity data along the cell and then to find local maximas of fluorescence signal using R Peaks package. We have also developed a method for analysis of segregation defects occurring at the last stage of Streptomyces growth. With this approach we have shown that Streptomyces chromosomes follow the tip of the growing cell, with the first chromosome being the most tightly tethered, and that various factors such as DNA supercoiling can affect chromosome distribution in the cell. 1.2.4 A flexible dashboard for monitoring platform trials Alessio Crippa, Karolinska Institutet, postdoc Track(s): R Applications Abstract: The Data and Safety Monitoring Board (DSMB) is an essential component for a successful clinical trial. It consists of an independent group of experts that periodically revise and evaluate the accumulating data from an ongoing trial to assess patients’ safety, study progress, and drug efficacy. Based on their evaluation, a recommendation to continue, modify or stop the trial will be delivered to the trial’s sponsor. It is essential to provide the DSMB with the best delivery visualization tools for monitoring on a regular basis the live data from the study trial. We designed and developed an interactive dashboard using flexdashboard for R as a helping tool for assisting the DSMB in the evaluation of the results of the ProBio study, a clinical platform for improving treatment decision in patients with metastatic castrate resistant prostate cancer. We will focus on the customized structure for best displaying the most interesting variables and the adoption of interactive tools as a particularly useful aid for the assessment of the ongoing data. We will also cover the connection to the data sources, the automatic generation process, and the selected permission for the people in the DSMB to access the dashboard. 1.2.5 PRDA package: Enhancing Statistical Inference via Prospective and Retrospective Design Analysis. Angela Andreella, University of Padua Track(s): R Life Sciences Abstract: There is a growing recognition of the importance of power analysis and calculation of the appropriate sample size when planning a research experiment. However, power analysis is not the only relevant aspect of the design of an experiment. Other inferential risks, such as the probability of estimating the effect in the wrong direction or the average overestimation of the actual effects, are also important. The evaluation of these inferential risks as well as the statistical power, in what Gelman and Carlin (2014) defined as Design Analysis, may help researchers to make informed choices both when planning an experiment or evaluating study results. We introduce the PRDA (Prospective and Retrospective Design Analysis) package that allows researchers to carry a Design Analysis under different experimental scenarios (Altoè et al., 2020). Considering a plausible effect size (or its prior distribution) researchers can evaluate either the inferential risks for given sample size or the required sampled size to obtain a given statistical power. Previously, PRDA functions were limited to mean differences between groups considering Cohen’s d in the Null significance Hypothesis Testing (NHST) framework. Now, we present the newly developed features that include other effect sizes (such as Pearson’s correlation) as well as Bayes Factor hypothesis testing. 1.2.6 CorpFinder- a new application to identify Large Corporate Risks Antoine Logean, Swiss Re, Casualty R&amp;D Department, Senior Data Scientist Track(s): R Production, R Dataviz &amp; Shiny, R Applications Abstract: Large corporations are often viewed as being responsible for major losses to business, nature, or our societies overall. The importance of large institutions is not limited to a handful of market competitors and well-known brands. Fueled by major scandals, their subsidiaries are also considered to be harmful. Recently, this perception has notably increased the willingness to file lawsuits against little-known companies whose ultimate parent entities are well-known brands. From the insurance perspective, this trend is also alarming. The impact of large corporations materializes through exposures to potentially high monetary losses. Because of the complex corporate ownership relationships, the full extent of exposure to the so-called Large Corporate Risks (LCR), however, may remain hidden to insurance analysts. To tackle this problem, Swiss Re’s Casualty R&amp;D department is developing a new R-Shiny-based application that allows to uncover these Large Corporate Risks in insurance portfolios. The application uses a fuzzy matching algorithm to analyze and compare the company names with an internal list of well-known corporate institutions and their daughter companies. The insurance analysts can download summary statistics and visualizations of the LCR results. During the talk, we will present the development and data analysis steps required for the application and discuss the initial quantitative findings to Large Corporate Risks. 1.2.7 Automate flexdashboard with GitHub Binod Jung Bogati, Data Analyst Intern at VIN Track(s): R Dataviz &amp; Shiny Abstract: flexdashboard is a great tool for building an interactive dashboard in R. We can host it for free on GitHub Pages, Rpubs and many other places. Hosted flexdashboard is static so changes in our data we need to manually update and publish every time. If we want to auto-update we may need to integrate Shiny. However, it may not be suitable for every case. To overcome this, we have a solution called GitHub Action. It’s a feature from GitHub which automates our tasks in a convenient way. With the help of GitHub Actions, we can automate our flexdashboard (Rmarkdown) updates. It builds a container that runs our R scripts. We can trigger it every time we push on GitHub or schedule it every X minutes/hours/days/month. If you want to learn more about the GitHub Action. And also know how to automate updates on your flexdashboard. Please do come and join me. 1.2.8 IT Process Optimization in Data Lake via R Ceyda Sol, INGTECH NL , Data analyst Track(s): R Production, R Applications, R World Abstract: This poster refers to an IT case study application in the area of finance industry. A different machine learning models are applied in order to estimate the file arrival times to the servers of a Data Lake. The aim is to be able to trigger alert mechanisms in case of a missing delivery and optimize the workload of the servers. This study is significant in the sense of satisfying the on-time regulatory reporting requirements of the other departments in the bank. The results indicates that the model is able to estimate the time arrivals with a 90% correctness. 1.2.9 Level up your tables with tableHTML in R Dana Jomer, IT Power Services, Data Scientist Track(s): R Dataviz &amp; Shiny Abstract: One of the most important tasks in data science is to communicate findings to a non-technical audience. Data visualisation is a powerful tool to make insights actionable. The R ecosystem has wonderful tools, such as ggplot2, to create powerful visualisations. While working on a project that would automate a planning process done manually by a business analyst using Excel, we were faced with a challenge to come up with a report that would look and feel like Excel but could be easily automated. Since the planning algorithm was implemented in R, it was only natural to use it for this purpose. In the process of creating the report, we found that when it comes to showing tables that can be easily styled to make them visually appealing or even to highlight certain values based on some logic, that should not be hard coded and ready to be automated, tableHTML would be the perfect package to do just that. tableHTML is an R package to create and style HTML tables with CSS from a data.frame or matrix. These tables can be exported and used in any application that accepts HTML, e.g. shiny or rmarkdown. In this talk, we will show how tableHTML can be used in a business context where Excel is commonly used in the business to interact with data in tabular form, how to adjust the appearance to align it to the corporate identity guidelines, and how conditional formatting can help to put emphasis on the most important information. 1.2.10 Guidance for teaching R to non-programmers Dean Langan, Senior Teaching Fellow (University College London, UK) Track(s): R Life Sciences, R World Abstract: The Centre for Applied Statistics Courses (CASC) at University College London (UCL) provide short courses on statistics and statistical software packages. Popular day-courses include a well-established ‘Introduction to R’ course and the newly developed ‘Further Topics in R’. In the latter, attendees are taught intermediate-level topics such as loops and conditional statements. Attendees range from postgraduate students, academic researchers and data analysts in the private sector without a strong background in statistics or programming. First, we highlight some issues with providing our training course to this demographic, derived from our experience and from anonymous online feedback. Second, we discuss some of our solutions to these issues that have shaped our course over time. For example, one issue is catering to a wide audience from differing fields, different levels of computer literacy and approaches to learning. To address this, we prepare for a high level of flexibility on the day and include intermittent practical exercises to get real time feedback on the abilities of attendees. Finally, we reviewed the experiences of other teachers on similar courses documented online and compared these experiences with our own. We offer guidance to other teachers running or developing courses for intermediate-level R programming. 1.2.11 Transparent presentation of uncertain lotteries using {deals} Dmytro Perepolkin, Lund University Track(s): R Dataviz &amp; Shiny Abstract: Making decisions under uncertainty is hard. Scholars of human decision making have identified a number of decision paradoxes, related to how we perceive and choose among uncertain prospects (e.g. Allais Paradox and Ellsberg Paradox). Recently a few remedies have been proposed which help alleviate some risk and/or ambiguity aversion by careful choice of visual presentation of uncertainty. We introduce R package which makes it easy to create, transform and visualize uncertain lotteries and extend the previously published results for continuous variables. 1.2.12 NewWave: a scalable R package for the dimensionality reduction of single-cell RNA-seq Federico Agostinis, Università degli studi di Padova, Fellowship Track(s): R Life Sciences Abstract: The fast development of single cell sequencing technologies in the recent years has generated a gap between the throughput of the experiments and the capability of analizing the generated data. One recent method for dimensionality reduction of single-cell RNA-seq data is zinbwave, it uses zero inflated negative binomial likelihood function optimization to find biological meaningful latent factors and remove batch effect. Zinbwave has optimal performance but has some scalability issues due to large memory usage. To address this, we developed an R package with new software architec- ture extending zinbwave. In this package, we implement mini-batch stochastic gradient descent and the possibility of working with HDF5 files. We decide to use a negative binomial model following the observation that droplet sequencing technologies do not induce zero inflation in the data. Thanks to these improvements and the possi- bility of massively parallelize the estimation process using PSOCK clusters, we are able to speed up the computations with the same or even better results than zinbwave. This type of parallelization can be used on multiple hardware setups, ranging from simple laptops to dedicated server clusters. This, paired with the ability to work with out-of-memory data, enables us to analyze datasets with milions of cells. 1.2.13 orf: Ordered Random Forests Gabriel Okasa, Research Assistant and PhD Candidate at the Swiss Institute for Empirical Economic Research, University of St. Gallen, Switzerland Track(s): R Machine Learning &amp; Models Abstract: The R package ‘orf’ is a software implementation of the Ordered Forest estimator as developed in Lechner and Okasa (2019). The Ordered Forest flexibly estimates the conditional class probabilities of models involving categorical outcomes with an inherent ordering structure, known as ordered choice models. Additionally to common machine learning algorithms, the Ordered Forest enables estimation of marginal effects together with statistical inference and thus provides comparable output as in standard econometric models. Accordingly, the ‘orf’ package provides generic R functions to estimate, predict, plot, print and summarize the estimation output of the Ordered Forest along with various options for specific forest-related tuning parameters. Finally, computational speed is ensured as the core forest algorithm relies on the fast C++ forest implementation from the ranger package (Wright and Ziegler 2017). 1.2.14 Biomarker discovery by the leveraging of omic and clinical data using biomarkeRbox. Giulio Ferrero, University of Turin Track(s): R Life Sciences Abstract: The diffusion of sequencing technologies rapidly increased the availability of omic data useful to identify novel molecular biomarkers of human pathological phenotypes. However, omic information remains far to be easily integrated with clinical parameters as well as with lifestyle, dietary information and other meta-data. To overcome this problem, we designed biomarkeRbox, an R pipeline for the joined integrative analyses of omic data (e.g. transcriptomic) with clinical and other meta-data. The pipeline includes different modules designed for: i) data quality control and preprocessing; ii) covariate analysis; iii) omic data analysis; and iv) the joined analysis of omic and covariate data to identify candidate biomarkers for the studied disease(s) using machine learning techniques. The pipeline can be either fully automatized with the generation of analysis reports with R markdown or customized using an ad-hoc Shiny interface. The pipeline has been recently tested in a case-control study in which microRNA profiles have been investigated as biomarkers for colorectal cancer subtypes. 1.2.15 An package for Bayesian analysis of structured time series models with Izhar Asael Alonzo Matamoros, Universidad Nacional Autónoma De Honduras Track(s): R Machine Learning &amp; Models, R Applications Abstract: is an R package for Bayesian analysis of time series models using Stan. The package offers a dynamic way to choose your model, define priors in a wide range of distributions, check model’s fit, and forecast with the m-steps ahead predictive distribution. The users can widely choose between implemented models such as . Every model constructor in varstan defines weakly informative priors, but prior specifications can be changed in a dynamic and flexible way, so the prior distributions reflect the parameter’s initial beliefs.\\ For model selection, the package offers the classical information criteria, such as AIC, AICc, BIC, DIC, Bayes factor, and more recent criteria such as Widely-applicable information criteria (), and the Bayesian leave one out cross-validation (). In addition, a Bayesian version for automatic order selection in seasonal ARIMA and dynamic regression models can be used as an initial step for the time series analysis.\\ 1.2.16 Bridging the gap between R and computer vision Lubomír Štěpánek, Biostatistician, Software Developer, Junior Lecturer, PhD Candidate at First Faculty of Medicine, Charles University &amp; Faculty of Biomedical Engineering, Czech Technical University in Prague (CZ) Track(s): R Applications, R Dataviz &amp; Shiny, R Life Sciences, R Machine Learning &amp; Models Abstract: R language is, without any doubt, a powerful tool well suitable for the vast majority of most analytical tasks arising from an everyday routine of a statistician, scientist, or data analyst. However, there are fields of data processing that R is not so much usable for, and, in addition, other languages or tools such as Python or Octave are preferred to handle them more natively. One of the fields is image processing and particularly facial image processing or computer vision, where there are barely original R packages currently available. There are, of course, some approaches how to handle missing libraries in R. In general, an API library could bridge the gap between R on the one hand, and the unique tool (usually built in a different programming language) on the other hand. In this work, we tried to rethink the problem and go even further such that we have developed a web-based shiny application providing facial image processing and especially automated facial landmarking. The facial landmarking is powered using C++ library dlib, dedicated to machine-learning based computer vision algorithms, but due to its origin, it is primarily available only for C++ speaking users. The connection between R, C++ dlib library, and well-known R package shiny could open R functionality and computing power to a broader audience using a comfortable and clickable way. 1.2.17 How R-hub can help you develop and maintain your R packages Maëlle Salmon, R-hub Track(s): R World Abstract: R-hub is a collection of services for all package developers, from newbies to seasoned maintainers. It is funded by the R Consortium as a top level project. This poster will showcase a roundup of R-hub services. On the tech part, R-hub’s flagship product is its package builder: Run R CMD check on 15 different platforms (operating systems, R versions…), from R! The package builder and its companion R package rhub can help prepare a CRAN submission, as well as debug a CRAN check failure by mimicking CRAN platforms. As of December 2019, it had been used nearly 85,000 times, by 2507 users to check 4418 unique packages. Other R-hub services include a package for finding R packages on CRAN (pkgsearch), badges for CRAN packages, and a mirror of CRAN source code. On the guidance part, R-hub has recently gained a docs website. Furthermore, R-hub also has an active blog illustrating R-hub services, as well as other topics relevant to package developers. Last but not least, the R-hub project has three communication channels: a gitter channel, a GitHub issue tracker, and a Twitter account. Come see this poster to learn more about R-hub and to tell us about your experience! 1.2.18 Power Supply health status monitoring dashboard Marco Calderisi, Kode srl, CTO Track(s): R Dataviz &amp; Shiny Abstract: The Primis project dashboard allows to perform an analysis on the health status of power supplies boards on two levels: (1) analysis of a specific board, to check its status and the presence of any anomalies, (2) analysis of multiple boards within a single Power Supply, to check if the set of boards reveals abnormal behavior and if some boards behave in a distinctly different way from the others. The analysis algorithms and the web application were created using the programming language R, and in particular the Shiny library. The application is therefore divided into two parts that reflect these different types of analysis, called respectively “Product View” (analysis and diagnostics of a specific board) and “Product Comparison” (comparison analysis between multiple boards of the same Power Supply). Both analyzes can be carried out on an arbitrary time interval, selectable through a special application menu. The analysis is carried out by means of: (1) univariate analysis, focusing on a specific parameter of one or more channels and displaying aggregate information regarding the status of the board in the entire observation period (2) multivariate analysis, that is the application of multivariate algorithms that allows to perform an overall analysis of the board, taking into account all the variables simultaneously. 1.2.19 First-year ICT students dropout predicting with R models Natalja Maksimova, Virumaa College of Tallinn University of Technology, lecturer Track(s): R Machine Learning &amp; Models Abstract: The aim of this study is to find how it is possible to predict first-year ICT students dropout in one Estonian college, Virumaa College of Tallinn University of Technology (TalTech) and possibly to engage methods to decrease dropout rate. We perform three approaches of machine learning using R tools: logistic regressions, decision trees and Naive Bayes to predict. The models are computed on the basis of the TalTech study information system data. As a result, we propose a methodical approach that may be realized in practice at other institutions. All applied methods yield high prediction with more than 85% accuracies. In the same time some influencing and non-influencing factors were found in predicting ICT students’ dropout. 1.2.20 Benchmark Percentage Disjoint Data Splitting in Cross Validation for Assessing the Skill of Machine Olalekan Joseph Akintande, University of Ibadan, Ph.D. Student Track(s): R Machine Learning &amp; Models Abstract: The controversies surrounding dataset splitting technique and folklore of what has been or what should be, remain an open debate. Several authors (bloggers, researchers, and data scientists) in the field of machine learning and similar research areas, have proposed various arbitrary percentage disjoint dataset splitting (DDS) options for validating the skill of machine learning algorithms and by extension the appropriate percentage DDS based on cross-validation techniques. In this work, we propose benchmarks for which the percentage DDS procedure should be based. These benchmarks are founded on various training sizes (m) and serve as the basis and justification for the choice of an appropriate percentage DDS for assessing the skill of ML algorithms and related fields, on the concept of cross-validation techniques. 1.2.21 Integrating professional software engineering practices in medical research software Patricia Ryser-Welch, Newcastle University, Population Health Science Institute, Research Associate, Track(s): R Applications Abstract: Health data sets are getting bigger, more complex, and are increasingly being linked up with other data sources. With this trend there is an increasing risk of patient identification and disclosure. Two different ways of mitigating this risk are to use a federated analysis approach or to use a data safe haven. DataSHIELD (www.datashield.ac.uk) is an established federated data analysis tool that is used in the medical sciences. This software has a variety of methods to reduce the risk of disclosure built in. Here we describe the steps we are taking to apply modern software engineering methodologies to DataSHIELD. The upcoming Medical Devices legislation requires that software has more rigourous testing done on it. While this legislation does not directly apply to software used for research, we think it is important the ideas behind this do filter down to research software. For us these principles include testing that functions work, as well as testing that they produce the correct answers. Using a static standard data set to test against (that is publicly available) is also an important aspect. This work is being done in a continuous integraion framework using Microsoft Azure. Additionally all our software is developed as open source. In addition to the protection DataSHIELD provides on its own we are also integrating it into our Trusted Research Environment as part of Connected Health Cities North East and North Cumbria. This will give an extra level of protection to data that may automatically flow from multiple data sources. Additionally, as analysis can be done in a federated way it means that that data does not need to leave its data controller’s environment. This opens up the possibility of analysis happening across trusts and regions. 1.2.22 A Three-Parameter Gompertz-Lindley Distribution: Its Properties And Applications Peter Koleoso, Department of Statistics, University of Ibadan, Oyo State, Nigeria. Track(s): R Machine Learning &amp; Models, R Life Sciences, R Applications Abstract: This research proposed a three-parameter probability distribution called Gompertz-Lindley distribution using Gompertz generalized (Gompertz-G) family of distributions. The distribution was proposed as an improvement on the Lindley distribution, for greater flexibility when modelling real life and survival data. The mathematical properties of the distribution such as moment, moment generating function, survival function and hazard function were derived. The parameters of the distribution were estimated using the method of maximum likelihood and the distribution was applied to model the strength of glass fibres. The maximum likelihood estimation and the modelling were carried out using R software. The proposed Gompertz-Lindley distribution performed best in modelling the strength of glass fibres (AIC = 62.8537), followed by Generalized Lindley distribution (AIC = 77.6237). The Lindley distribution had the least performance with the highest information criterion values. 1.2.23 Partitional clustering with extensions Tero Lähderanta, Research Unit of Mathematical Sciences, University of Oulu Track(s): R Machine Learning &amp; Models Abstract: Clustering is one of the most well-known machine learning methods. The most used algorithm for clustering is k-means, where the clusters are formed by minimizing the squared Euclidean distances between the data points and cluster centers. Very similar optimization problem can be found in location-allocation literature, which tackle the problem of placing facilities into a 2d region with demand points, while simultaneously minimizing the average distance from each point to its facility. Both frameworks are very topical problems today as the amount of spatial data is ever-growing. In both frameworks, the demand for different types of extensions in the optimization task is high due to the variety of the problems. The most common extensions are the inclusion of capacity limits for cluster sizes, outliers and different distance metrics and membership types. There already exists many extension approaches which focus on a single extension. However, in many scenarios multiple simultaneous extensions are needed. Our approach enables such simultaneous use of different extensions by combining constraints and penalties to a single objective function. Furthermore, we provide an easy-to-use R package rpack to formulate and solve problems from both frameworks with above-mentioned extensions. Secondly, we show examples of real-world scenarios where the package can be used. 1.2.24 Dealing with changing administrative boundaries: The case of Swiss municipalities Tobias Schieferdecker, Daily dealings with data at cynkra Track(s): R Applications Abstract: Switzerland’s municipalities are frequently merged or reorganized, in an attempt to reduce costs and increase efficiency. These mergers create a substantial problem for data analysis. Often it is desirable to study a municipality over time. But in order to create a time series for a region of interest, its borders should stay constant. Our goal is to provide R-functions that allow an easy and consistent handling of these mergers. We create a mapping table for municipality ID’s for a specified period of time, that allows us to track the mergers over time. We also provide weights, such as population as well as area of the municipalities, to facilitate the construction of weighted time series. Various other municipality mutations are also taken into account. We are creating two R packages: an infrastructure package that handles the task of keeping the data up to date; and a user package that contains the functions to deal with the mergers. 1.2.25 badDEA: An R package for measuring firms’ efficiency adjusted by undesirable outputs Yann Desjeux, INRAE, France Track(s): R Applications Abstract: Growing concerns on the detrimental effects of human production activities on the environment, e.g. air, soil and water pollution, have triggered the development of new performance indicators (including productivity and efficiency measures) accounting for such undesirable impacts. Firms can now be benchmarked not only in terms of economic performance, but also in terms of environmental performance linked to production. In the performance benchmarking literature, and more specifically the one on the non-parametric approach Data Envelopment Analysis (DEA), several methodologies have been developed to consider these impacts as undesirable (or bad) outputs. Related empirical applications in the literature, performed with various software, show that conclusions differ depending on the way these undesirable outputs are introduced. However none of these methodologies are routinely developed in R. In this context, we developed the badDEA package in order to provide a consistent and single framework where users (students, researchers, practitioners) can find the major methodologies proposed in the literature to compute efficiency measures that are adjusted by undesirable outputs. In this presentation, we will describe the aim, structure and options of the badDEA package, unfolding all the methodologies in their different variants and providing a promising tool for decision-making. "],
["regular-talk.html", "1.3 Regular talk", " 1.3 Regular talk 1.3.1 Design Patterns For Big Shiny Apps Alex Gold, Solutions Engineer, RStudio Track(s): R Dataviz &amp; Shiny, R Production Abstract: In about 20 minutes on the morning of January 27, 2020, one engineer launched over 500 individual cloud server instances for workshop attendees at RStudio::conf and managed them for the duration of the workshops — all from a Shiny app. The RStudio team manages a variety of production systems using Shiny apps including our workshop infrastructure and access to our sales demo server. The Shiny apps are robust enough for these mission-critical activities because of an important lesson from web engineering: separation of concerns between front-end user interaction logic and back-end business logic. This design pattern can be implemented in R by creating user interfaces in Shiny and managing interactions with other systems with Plumber APIs and R6 classes. This pattern allows for even complex Shiny apps to still be understandable and maintainable. Moreover, this pattern of designing and building large Shiny apps is broadly applicable to any app that has substantial interaction with outside systems. Session attendees will gain an understanding of this pattern, which can be useful for many large Shiny apps. 1.3.2 Using XGBoost, Plumber and Docker in production to power a new banking product André Rivenæs, Data Scientist, PwC Track(s): R Machine Learning &amp; Models, R Production Abstract: Buffer is a brand new and innovative banking product by one of the largest retail banks in Norway, Sparebanken Vest, and it is powered by R. In fact, the product’s decision engine is written entirely in R. We analyze whether a customer should get a loan and how much loan they should be allocated by analyzing large amounts of data from various sources. An essential part is analyzing the customer’s invoices using machine learning (XGBoost). In this talk, we will cover: How we use ML and Bayesian statistics to estimate the probability of an invoice being repaid. How we successfully put the decision engine in production, using e.g. Plumber, Docker, CircleCI and Kubernetes. What we have learned from using R in production at scale. 1.3.3 Astronomical source detection and background separation: a Bayesian nonparametric approach Andrea Sottosanti, University of Padova Track(s): R Machine Learning &amp; Models, R Applications Abstract: We propose an innovative approach based on Bayesian nonparametric methods to the signal extraction of astronomical sources in gamma-ray count maps under the presence of a strong background contamination. Our model simultaneously induces clustering on the photons using their spatial information and gives an estimate of the number of sources, while separating them from the irregular signal of the background component that extends over the entire map. From a statistical perspective, the signal of the sources is modeled using a Dirichlet Process mixture, that allows to discover and locate a possible infinite number of clusters, while the background component is completely reconstructed using a new flexible Bayesian nonparametric model based on b-spline basis functions. The resultant can be then thought of as a hierarchical mixture of nonparametric mixtures for flexible clustering of highly contaminated signals. We provide also a Markov chain Monte Carlo algorithm to infer on the posterior distribution of the model parameters, and a suitable post-processing algorithm to quantify the information coming from the detected clusters. Results on different datasets confirm the capacity of the model to discover and locate the sources in the analysed map, to quantify their intensities and to estimate and account for the presence of the background contamination. 1.3.4 Validation of visual inference methods using deep learning in R Anne Helby Petersen, University of Copenhagen Track(s): R Machine Learning &amp; Models Abstract: Visual inference is commonly used to assess whether a statistical model fulfills essential assumptions, fit well or converted properly. An example is the residual plot, which is used for identifying heteroscedasticity issues in normal linear regression models, and most statistics 101 classes train students in telling “healthy” and “problematic” plots apart. However, it is not really clear whether struggling students are actually posed with a solvable task, as the visual inference methods themselves are rarely validated. We propose a new tool for validating visual inference methods in R. The tool revolves around training convolutional neural networks (using the keras package) to tell apart simulated “healthy” and “problematic” plots. The obtained classification accuracy is then interpreted as the amount of discriminatory power available in the visual inference method and it is used to evaluate it: If the neural network cannot categorize the plots correctly, we doubt that humans will be able to. We showcase the tool on residual plots, where it is able to discriminate between problematic and healthy plots when there is an appropriate number of observations (e.g. n = 50), but less so when there are only few observations (e.g. n = 10). This implies that our proposed tool is not only able to positively show visual inference validity, but it is also useful for identifying lack thereof. 1.3.5 High dimensional sampling and volume computation Apostolos Chalkis, PhD in Computer Science Track(s): R Machine Learning &amp; Models Abstract: Sampling from multivariate distributions is a fundamental problem in statistics that plays important role in modern machine learning and data science. Many important problems such as convex optimization and multivariate integration can be efficiently solved via sampling. This talk presents the CRAN package volesti which offers to R community efficient C++ implementations of state-of-the-art algorithms for sampling and volume computation of convex sets. It scales up to hundred or thousand dimensions, depending the problem, providing the most efficient implementations for sampling and volume computation to date. Thus, volesti allows users to solve problems in dimensions and order of magnitude higher than before. We present the basic functionality of volesti and show how it can be used to provide approximate solutions to intractable problems in combinatorics, financial modeling, bioinformatics and engineering. We stand out two famous applications in finance. We show how volesti can be used to detect financial crises and evaluate portfolios performance in large stock markets with hundreds of assets, by giving real life examples using public data. 1.3.6 Fake News: AI on the battle ground Ayomide Shodipo, Senior Developer Advocate &amp; Media Developer Expert at Cloudinary Track(s): R Machine Learning &amp; Models, R Life Sciences, R Production, R World Abstract: Assumed products have been a longstanding and growing pain for companies around the globe. In addition to impacting company revenue, they damage brand reputation and customer confidence. Companies were asked to build a solution for a global electronics brand that can identify fake products by just taking one picture on a smartphone. In this session, we will look into the building blocks that make this AI solution work. We’ll find out that there is much more to it than just training a convolutional neural network. We look at challenges like how to manage and monitor the AI model and how to build and improve the model in a way that fits your DevOps production chain. Learn how we used Azure Functions, Cosmos DB and Docker to build a solid foundation. See how we used the Azure Machine Learning service to train the models. And find out how we used Azure DevOps to control, build and deploy this state-of-the-art solution. 1.3.7 Automation of File Monitoring in a Data Lake for Large Scale Systems Ceyda Sol, INGTECH NL , Data analyst Track(s): R Applications, R Machine Learning &amp; Models, R World Abstract: for Banks in regulatory reporting, the delayed and/or missing delivery of a file may lead to serious problems that can hinder the delivery towards law authorities and as a consequence a fine can be applied to the report providers. In this study, the objective is better monitoring and being able to trigger automated alerts to the targets and system of records (SoRs) and better distribution of the files to the loaders to reduce the waiting time in the loader queue. 1.3.8 From consulting to open-source and back Christoph Sax, R-enthusiast, economist @cynkra Track(s): R World Abstract: Open-source development is a great source of satisfaction and fulfillment, but someone has to pay the bills. A straightforward solution is to consult customers and help them to pick the right tools. As a small group of R enthusiasts, we try to align open source development by supporting our clients to accomplish their goals, contributing to the community along the way. It turns out that the benefits work in both ways: In addition to funding, consulting work allows us to test our tools and to improve their usability in a practical setting. At the same time, the involvement in open source development sharpens our analytical skills and serves as a first stop for new customers. Ideally, consulting projects lead to new developments, which in turn lead to new consulting projects. 1.3.9 How to apply R in a hospital environment on standard available hospital-wide data Deschepper Mieke, University hospital Ghent, staf member Strategic Policy cell, Ph.D. Track(s): R Life Sciences Abstract: Lots of data is registered within hospitals, for financial, clinical and administrative purposes. Today, this data is barely used. Due to not knowing the existence of the data, the possible applications and the skills to execute the analysis, … In this presentation we show how we can apply R on this data and what the possibilities are using standard available hospital-wide data on a low cost budget. 1. Reporting with R - using R and markdown as a tool for management reporting - Using R for data handling (ETL) - Shiny applications as alternative for dashboarding 2. Using R as a statistical tool - Performing regression models to gain insight in certain predictor 3. Using R a data science tool - Using R to perform Machine Learning analysis, e.g. Random Forests - Using R for the data wrangling and handle the high dimensional data 4. Requirements for all of the above 1.3.10 {polite}: web etiquette for R users Dmytro Perepolkin, Lund University Track(s): R World, R Applications Abstract: Data is everywhere, but it does not mean it is freely available. What are best practices and acceptable norms for accessing the data on the web? How does one know when it is OK to scrape the content of a website and how to do it in such a way that it does not create problems for data owner and/or other users? This talk with provide examples of using {polite} package for safe and responsible web scraping. The three pillars of {polite} are seeking permission, taking slowly and never asking twice. 1.3.11 Hydrological Modelling and R Emanuele Cordano, www.rendena100.eu Track(s): R Applications Abstract: Eco-hydrological and biophysical models are increasingly used in the contexts of hydrology, ecology, precision agriculture for better management of water resources and climate change impact studies at various scales: local, watershed or regional scale. However, to satisfy the researchers and stakeholders demand, user friendly interfaces are needed. The integration of such models in the powerful software environment of R greatly eases the application, input data preparation, output elaboration and visualization. In this work we present new developments for a R interface (R open-source package geotopbricks (https://CRAN.R-project.org/package=geotopbricks) and related) for the GEOtop hydrological distributed model (www.geotop.org - GNU General Public License v3.0). This package aims to be a link between the work of environmental engineers, who develop hydrological models, and the ones of data and applied scientists, who can extract information from the model results. Applications related to the simulation of water cycle dynamics (model calibration, mapping, data visualization) in some alpine basins and under scenarios of climate change and variability are shown. In particular, we will present an application to predict with the model winter snow conditions, which play a critical role in governing the spatial distribution of fauna in temperate ecosystems. 1.3.12 GeneTonic: enjoy RNA-seq data analysis, responsibly Federico Marini, Center for Thrombosis and Hemostasis (CTH) &amp; Institute of Medical Biostatistics, Epidemiology and Informatics (IMBEI) - University Medical Center Mainz Track(s): R Life Sciences Abstract: Interpreting the results from RNA-seq transcriptome experiments can be a complex task, where the essential information is distributed among different tabular and list formats - normalized expression values, results from differential expression analysis, and results from functional enrichment analyses. The identification of relevant functional patterns, as well as their contextualization in the data and results at hand, are not straightforward operations if these pieces of information are not combined together efficiently. Interactivity can play an essential role in simplifying the way how one accesses and digests RNA-seq data analysis in a more comprehensive way. I introduce GeneTonic (https://github.com/federicomarini/GeneTonic), an application developed in Shiny and based on many essential elements of the Bioconductor project, that aims to reduce the barrier to understanding such data better, and to efficiently combine the different components of the analytic workflow. For example, starting from bird’s eye perspective summaries (with interactive bipartite gene-geneset graphs, or enrichment maps), it is easy to generate a number of visualizations, where drill-down user actions enable further insight and deliver additional information (e.g., gene info boxes, geneset summary, and signature heatmaps). Complex datasets interpretation can be wrapped up into a single call to the GeneTonic main function, which also supports built-in RMarkdown reporting, to both conclude an exploration session, or also to generate in batch the output of the available functionality, delivering an essential foundation for computational reproducibility. 1.3.13 A simple and flexible inactivity/sleep detection R package Francesca Giorgolo, Kode s.r.l. - Data Scientist Track(s): R Life Sciences Abstract: With the widespread usage of wearable devices great amount of data became available and new fields of application arised, like health monitoring and activity detection. Our work focused on inactivity and sleep detection from continuous raw tri-axis accelerometer data, recorded using different accelerometers brands having sampling frequencies below and above 1Hz. The algorithm implemented is the SPT-window detection algorithm described in literature slighty modified to met the flexibility requirement we imposed ourselves. The R package developed provides functions to clean data, to identify inactivity/sleep windows and to visualize the results. The main function has a parameter to specify the measurement unit of the data, a threshold to distinguish low and high activity and also a parameter to handle non-wearing periods, where a non wear period is defined as a period of time where all the accelerometers are equal to zero. Other functions allow to separate overlapped accelerometer signals, i.e. when a device is replaced by another, and to visualize the obtained results. 1.3.14 progressr: An Inclusive, Unifying API for Progress Updates Henrik Bengtsson, UCSF, Assoc Prof, CS/Stats, R since 2000 Track(s): R Production, R Applications Abstract: The ‘progressr’ package provides a minimal, unifying API for scripts and packages to report progress from anywhere including when using parallel processing to anywhere. It is designed such that the developer can focus on what to report progress on without having to worry about how to present it. The end user has full control of how, where, and when to render these progress updates. Progress bars from popular progress packages are supported and more can be added. The ‘progressr’ is inclusive by design. Specifically, no assumptions are made how progress is reported, i.e. it does not have to be a progress bar in the terminal. Progress can also be reported as audio (e.g. unique begin and end sounds with intermediate non-intrusive step sounds), or via a local or online notification system. Another novelty is that progress updates are controlled and signaled via R’s condition framework. Because of this, there is no need for progress-specific arguments and progress can be reported from nearly everywhere in R, e.g. in classical for and while loops, within map-reduce APIs like the ‘lapply()’ family of functions, ‘purrr’, ‘plyr’, and ‘foreach’. It also works with parallel processing via the ‘future’ framework, e.g. ‘future.apply’, ‘furrr’, and ‘foreach’ with ‘doFuture’. The package is compatible with Shiny applications. 1.3.15 varycoef: Modeling Spatially Varying Coefficients Jakob Dambon, HSLU &amp; UZH, Switzerland Track(s): R Machine Learning &amp; Models Abstract: In regression models for spatial data, it is often assumed that the marginal effects of covariates on the response are constant over space. In practice, this assumption might often be questionable. Spatially varying coefficient (SVC) models are commonly used to account for spatial structure within the coefficients. With the R package varycoef, we provide the frame work to estimate Gaussian process-based SVC models. It is based on maximum likelihood estimation (MLE) and in contrast to existing model-based approaches, our method scales better to data where both the number of spatial points is large and the number of spatially varying covariates is moderately-sized, e.g., above ten. We compare our methodology to existing methods such as a Bayesian approach using the stochastic partial differential equation (SPDE) link, geographically weighted regression (GWR), and eigenvector spatial filtering (ESF) in both a simulation study and an application where the goal is to predict prices of real estate apartments in Switzerland. The results from both the simulation study and application show that our proposed approach results in increased predictive accuracy and more precise estimates. 1.3.16 FastAI in R: preserving wildlife with computer vision Jędrzej Świeżewski, Data Scientist at Appsilon Track(s): R Machine Learning &amp; Models Abstract: In this presentation, we will discuss using the latest techniques in computer vision as an important part of “AI for Good” efforts, namely, enhancing wildlife preservation. We will present how to make use of the latest technical advancements in an R setup even if they are originally implemented in Python. A topic rightfully receiving growing attention among Machine Learning researchers and practitioners is how to make good use of the power obtained with the advancement of the tools. One of the avenues in these efforts is assisting wildlife conservation by employing computer vision in making observations of wildlife much more effective. We will discuss several of such efforts during the talk. One of the very promising frameworks for computer vision developed recently is the Fast.ai wrapper of PyTorch, a Python framework used for computer vision among other things. While it incorporates the latest theoretical developments in the field (such as one cycle policy training) it provides an easy to use framework allowing a much wider audience to benefit from the tools, such as AI for Good initiatives run by people who are not formally trained in Machine Learning. During the presentation we will show how to make use of a model trained using the Python’s fastai library within an R workflow with the use of the reticulate package. We will focus on use cases concerning classifying species of African wildlife based on images from camera traps. 1.3.17 Shazam in R? Audio analysis using the ‘av’ package Jeroen Ooms, rOpenSci, UC Berkeley Track(s): R Applications Abstract: Shazam [1] is a popular smartphone app that can quickly identify a song or movie from a short audio recording. The basic recognition algorithm [2] effectively combines a few statistical methods to fingerprint an audio fragment based on density peaks in the time-frequency graph (spectrogram). These fingerprints are robust against noise and distortion, and can quickly be compared against a database of known songs. Similar approaches are used in audio signal classification to analyze anything from human speech to whale mating calls. This talk describes how we would implement something like this in R. We use the new rOpenSci package ‘av’ to read high quality audio/video files (mkv, mp3, aac, etc) into frequency data [3]. The av package makes it easy to cut, convert, and downsample audio, and customize FFT parameters, to prepare audio for analysis in R. We can visually inspect the frequency data by plotting the spectrogram, and finally try to calculate some of the spectrogram fingerprint statistics as described by the Shazam paper. [1] https://www.shazam.com [2] https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf [3] https://docs.ropensci.org/av/articles/articles/spectrograms.html 1.3.18 Powering Turing e-Atlas with R Layik Hama, Alan Turing Institute Track(s): R Applications, R Production, R Dataviz &amp; Shiny Abstract: Turing e-Atlas is a research project under the Urban Analytics research theme at Alan Turing Institute (ATI). The ATI is UK’s national institute for data science and Artificial Intelligence based at the British Library in London. The research is a grand vision for which we have been trying to take baby steps under the banner of an e-Atlas. And we believe R is positioned to play a foundation role in any scalable solution to analyse and visualize large scale datasets especially geospatial datasets. The application presented is built using RStudio’s Plumber package which relies on solid libraries to develop web applications. The front-end is made up of Uber’s various visualization packages using Facebook’s React JavaScript framework. 1.3.19 R at the service of plastic surgery: a web-based shiny application evaluating facial attractiveness Lubomír Štěpánek, Biostatistician, Software Developer, Junior Lecturer, PhD Candidate at First Faculty of Medicine, Charles University &amp; Faculty of Biomedical Engineering, Czech Technical University in Prague (CZ) Track(s): R Life Sciences, R Machine Learning &amp; Models, R Applications, R Dataviz &amp; Shiny Abstract: There are plenty of approaches on how to evaluate human facial attractiveness and how to compare facial images of patients taken before and after they underwent aesthetic facial plastic surgery. However, no one of them is complex enough; thus, any ongoing research or tool enabling to analyze image data and opening the computational power to a broader audience could help to make progress in the field. In this project, we developed a web-based shiny application providing facial image processing, both manual and automated landmarking, following facial geometry computations and machine-learning models. Besides others, it allows determining geometric facial features significantly increasing facial attractiveness after undergoing the given aesthetic facial plastic surgery. Facial image data of patients undergoing rhinoplasty, i. e. a correction of nose shape of size, were analyzed using the application, which confirmed the meaning and usability of the application. Machine-learning built-in methods helped to identify which facial predictors increase facial attractiveness. The shiny web framework enables to develop a complex web interface including HTML, CSS, and javascript front-end and R-based back-end bridging C++ library dlib, which performs image computations. Furthermore, the connected shinyjs package offers a user-server clickable interaction useful for the landmarking. 1.3.20 Manifoldgstat: an R package for spatial statistics of manifold data Luca Torriani, MOX, Department of Mathematics, Politecnico di Milano Track(s): R Machine Learning &amp; Models Abstract: The statistical analysis of data belonging to Riemannian manifolds is becoming increasingly important in many applications, such as shape analysis or diffusion tensor imaging. In many cases, the available data are georeferenced, making spatial dependence a non-negligible data characteristic. Modeling and accounting for it, typically, is not trivial, because of the non-linear geometry of the manifold. In this contribution, we present the Manifoldgstat R package, which provides a set of fast routines allowing to efficiently analyze sets of spatial Riemannian data, based on state-of-the-art statistical methodologies. The package stems from the need to create an efficient and reproducible environment allowing to run extensive simulation studies and bagging algorithms for spatial prediction of symmetric positive definite matrices. The package implements three main algorithms (Pigoli et al, 2016, Menafoglio et al, 2019, Sartori &amp; Torriani, 2019). The latter two are particularly computationally demanding, as they rely on Random Domain Decompositions of the geographical domain. To substantially improve performances, the package exploits dplyr and Rcpp to integrate R with C++ code, where template factories handle all run-time choices. In this communication, we shall revise the characteristics of the three methodologies considered, and the key-points of their implementation. 1.3.21 Voronoi Linkage for Spatially Misaligned Data Luís G. Silva e Silva, Food and Agriculture Organization - FAO - Data Scientist Track(s): R Dataviz &amp; Shiny, R World Abstract: In studies of elections, voting outcomes are point-referenced at voting stations while socioeconomic covariates are areal data available at census tracts. The misaligned spatial structure of these two data sources makes the regression analysis to identify socioeconomic factors that affect the voting outcomes a challenge. Here we propose a novel approach to link these two sources of spatial data through Voronoi tessellation. Our proposal is creating a Voronoi tessellation with respect to the point-referenced data, with this outset, the spatial points become a set of mutually exclusive polygons named Voronoi cells. The extraction of data from the census tracts is proportional to the intersection area of each census tract polygon and Voronoi cells. Consequently, we use 100% of the available information and preserve the polygons’ autocorrelation structure. When visualised through our Shiny App, the method provides a finer spatial resolution than municipalities and facilitates the identification of spatial structures at a most detailed level. The technique is applied for the 2018 Brazilian presidential election data. The tool provides deep access to Brazilian election results by enabling to create general maps, plots, and tables by states and cities. 1.3.22 Be proud of your code! Tools and patterns for making production-ready, clean R code Marcin Dubel, Software Engineer at Appsilon Data Science Track(s): R Production, R World Abstract: In this talk you’ll learn the tools and best practices for making clean, reproducible R code in a working environment ready to be shared and productionalised. Save your time for maintenance, adjusting and struggling with packages. R is a great tool for fast data analysis. It’s simplicity in setup combined with powerful features and community support makes it a perfect language for many subject matter experts e.g. in finance or bioinformatics. Yet often what started as a pet project or proof of concept begins to grow and expand, with additional collaborators working on it. It is then crucial that you have your project organised well, reusable, with an environment set, so that the code works every time and on any machine. Otherwise the solution won’t be used by anyone but you. By following a few patterns and with appropriate tools it won’t be overwhelming or disturbing and will highlight the true value of the code. Both Appsilon and I personally have taken part in many R projects for which the goal was to clean and organise the code as well as the project structure. We would like to share our experience, best practices and useful tools to share code shamelessly. During the presentation I will show: setting up the development environment with packrat, renv and docker, organising the project structure, the best practices in writing R code, automated with linter, sharing the code using git, organising workflow with drake, optimising the Shiny apps and data loading with plumber and database, preparing the tests and continuous integration circle CI. 1.3.23 DaMiRseq 2.0: from high dimensional data to cost-effective reliable prediction models Mattia Chiesa, Senior data scientist @ Centro Cardiologico Monzino IRCCS Track(s): R Life Sciences Abstract: High dimensional data generated by modern high-throughput platforms pose a great challenge in selecting a small number of informative variables, for biomarker discovery and classification. Machine learning is an appropriate approach to derive general knowledge from data, identifying highly discriminative features and building accurate prediction models. To this end, we developed the R/Bioconductor package DaMiRseq, which (i) helps researchers to filter and normalize high dimensional datasets, arising from RNA-Seq experiments, by removing noise and bias and (ii) exploits a custom machine learning workflow to select the minimum set of robust informative features able to discriminate classes. Here, we present the version 2.0 of the DaMiRseq package, an extension that provides a flexible and convenient framework for managing high dimensional data such as omics data, large-scale medical histories, or even social media and financial data. Specifically, DaMiRseq 2.0 implements new functions that allow training and testing of several different classifiers and selection of the most reliable one, in terms of classification performance and number of selected features. The resulting classification model can be further used for any prediction purpose. This framework will give users the ability to build an efficient prediction model that can be easily replicated in further related settings. 1.3.24 Interpretable and accessible Deep Learning for omics data with R and friends Moritz Hess, Research Associate, Institute of Medical Biometry and Statistics, Faculty of Medicine and Medical Center - University of Freiburg Track(s): R Life Sciences Abstract: Recently, generative Deep Learning approaches were shown to have a huge potential for e.g. retrieving compact, latent representations of high-dimensional omics data such as single-cell RNA-Seq data. However, there are no established methods to infer how these latent representations relate to the observed variables, i.e. the genes. For extracting interpretable patterns from gene expression data that indicate distinct sub-populations in the data, we here employ log-linear models, applied to the synthetic data and corresponding latent representations, sampled from generative deep models, which were trained with single-cell gene expression data. While omics data are routinely analyzed in R and powerful toolboxes, tailored to omics data are available, there are no established and truely accessible approaches for Deep Learning applications here. To close this gap, we here demonstrate how easily customizable Deep Learning frameworks, developed for the Julia programming language, can be leveraged in R, to perform accessible and interpretable Deep Learning with omics data. 1.3.25 Elevating shiny module with {tidymodules} Mustapha Larbaoui, Novartis, Associate Director, Scientific Computing &amp; Consulting Track(s): R Dataviz &amp; Shiny Abstract: Shiny App developers have warmly welcomed the concept of Shiny modules as a way to simplify the app development process through the introduction of reusable building blocks. Shiny modules are similar in spirit to the concept of functions in R, except each is implemented with paired ui and server codes along with their own namespace. The {tidymodules} R package introduces a novel structure that harmonizes module development based on R6 (https://r6.r-lib.org/), which is an implementation of encapsulated object-oriented programming for R, thus knowledge of R6 is a prerequisite for using {tidymodules} to develop Shiny modules. Some key features of this package are module encapsulation, reference semantics, central module store and an innovative framework for enabling and facilitating cross module – module communication. It does this through the creation of “ports”, both input and output, where users may pass data and information through pipe operators. Because the connections are strictly specified, the module network may be visualized which shows how data move from one module to another. We feel the {tidymodules} framework will simplify the module development process and will reduce the code complexity through programming concepts like inheritance. 1.3.26 APFr: Average Power Function and Bayes FDR for Robust Brain Networks Construction Nicolo’ Margaritella, University of Edinburgh Track(s): R Life Sciences Abstract: Brain functional connectivity is widely investigated in neuroscience. In recent years, the study of brain connectivity has been largely aided by graph theory. The link between time series recorded at multiple locations in the brain and a graph is usually an adjacency matrix. This converts a measure of the connectivity between two time series, typically a correlation coefficient, into a binary choice on whether the two brain locations are functionally connected or not. As a result, the choice of a threshold over the correlation coefficient is key. In the present work, we propose a multiple testing approach to the choice of a suitable threshold using the Bayes false discovery rate (FDR) and a new estimator of the statistical power called average power function (APF) to balance the two types of statistical error. We show that the proposed APF behaves well in case of independence of the tests and it is reliable under several dependence conditions. Moreover, we propose a robust method for threshold selection using the 5% and 95% percentiles of APF and FDR bootstrap distributions, respectively, to improve stability. In addition, we developed a R-package called APFr which performs APF and Bayes FDR robust estimation and provides simple examples to improve usability. The package has attracted more than 3200 downloads since its publication online (June 2019) at https://CRAN.R-project.org/package=APFr. 1.3.27 Flexible Meta-Analysis of Generalized Additive Models with metagam Øystein Sørensen, Associate Professor, University of Oslo Track(s): R Life Sciences, R Machine Learning &amp; Models Abstract: Analyzing biomedical data from multiple studies has great potential in terms of increasing statistical power, enabling detection of associations of smaller magnitude than would be possible analyzing each study separately. Restrictions due to privacy or proprietary data as well as more practical concerns can make it hard to share datasets, such that analyzing all data in a single mega-analysis might not be possible. Meta-analytic methods provide a way to overcome this issue, by combining aggregated quantities like model parameters or risk ratios. However, most meta-analytic tools have focused on parametric statistical models, and software for meta-analyzing semi-parametric models like generalized additive models (GAMs) have not been developed. The metagam package attempts to fill this gap: It provides functionality for removing individual participant data from GAM objects such that they can be analyzed in a common location; furthermore metagam enables meta-analysis of the resulting GAM objects, as well as various tools for visualization and statistical analysis. This talk will illustrate use of the metagam package for analysis of the relationship between sleep quality and brain structure using data from six European brain imaging cohorts. 1.3.28 Controlled R development with Docker Peter Schmid, R programmer at Mirai Solutions Track(s): R Production Abstract: When deploying productive solutions, it is essential to have full control over the code-base and environment to ensure reproducibility and stability of the setup. Additionally, guaranteeing full equivalence of the development setup to (alternative) target productive stages is a key aspect of well-managed release pipelines. In the case of R-based projects, this implies fixing and aligning the version of R as well as package and system dependencies. This is, however, often disregarded due to the absence of out-of-the-box solutions, especially in free open source projects. With this talk we illustrate an approach to manage a version-stable R development environment that, in the context of containerized solutions based on Docker and the Rocker project, allows to regulate your setup, along with laying out a path for trouble-free deployments and releases. It additionally enables the coexistence of multiple dockerized development flavors, to match various target production environments or projects. Since the approach does not rely on commercial tools, it is particularly apt for open source projects, as we showcase with the concrete example of OasisUI (https://github.com/OasisLMF/OasisUI): a web-based Shiny app providing a front-end interface to the Oasis LMF platform, an open source natural catastrophe loss modelling framework, freely available at https://github.com/OasisLMF. 1.3.29 CorrelAidX - Building R-focused Communities for Social Good on the Local Level Regina Siegers, CorrelAidX Coordination Track(s): R World Abstract: Data scientists with their valuable skills have enormous potential to contribute to the social good. This is also true for the R community - and R users seem to be especially motivated to use their skills for the social good, as the overwhelmingly positive reception of Julien Cornebise’s keynote “AI for Good” at useR2019 (Cornebise 2019) has shown. However, specific strategies for putting the abstract goal “use data science for the social good” into practice are often missing, especially in volunteering contexts like the R community, where resources are often limited. In our talk, we present formats that we have implemented on the local level to build R-focused, data-for-good communities across Europe. Originating from the German data4good network CorrelAid with its over 1600 members, we have established 9 local CorrelAidX groups in Germany, the Netherlands, and France. The specific formats build on a three-pillared concept of community building, namely group-bonding, social entrepreneurship and outreach. We present multiple examples that illustrate how our local chapters operate to put data science for good into practice - using the formats of data dialogues, local data4good projects, and CorrelAidX workshops. Lastly, we also outline possibilities to implement such formats in cooperation between CorrelAidX chapters and R community groups such as R user groups or RLadies chapters. 1.3.30 Interactive visualization of complex texts Renate Delucchi Danhier, Post-Doc, TU Dortmund Track(s): R Dataviz &amp; Shiny Abstract: Hundreds of speakers may describe the same circumstance - e.g. explaining a fixed route to a goal - without producing two identical texts. The enormous variability of language and the complexity involved in encoding meaning poses a real difficulty for linguists analyzing text databases. In order to aid linguists in identifying patterns to perform comparative research, we developed an interactive shiny app that enables quantitative analysis of text corpora without oversimplifying the structure of language. Route directions are an example of complex texts, in which speakers take cognitive decisions such as segmenting the route, selecting landmarks and organizing spatial concepts into sentences. In the data visualization, symbols and colors representing linguistic concepts are plotted into coordinates that relate the information to fixed points along the route. Six interconnected layers of meaning represent the multi-layered form-to-meaning mapping characteristic of language. The shiny app allows to select and deselect information on these different layers, offering a holistic linguistic analysis way beyond the complexity attempted within traditional linguistics. The result is a kind of visual language in itself that deconstructs the interconnected layers of meaning found in natural language. 1.3.31 BNPmix: an new package to estimate Bayesian nonparametric mixtures Riccardo Corradin, Università degli Studi Milano Bicocca Track(s): NA Abstract: Bayesian nonparametric mixtures are flexible models for density estimation and clustering analysis, where the mixture is assumed composed by an infinite number of components. Different strategies were proposed in the last two decades to deal with the estimation of these models, most resorting to Markov chain Monte Carlo (MCMC) methods. Along with the study of a new approach to estimate infinite mixtures, we implemented an efficient R package, named BNPmix. The package include three different MCMC strategies to deal with nonparametric mixtures, and allows for flexible estimation of the models by choosing the algorithm, and by tuning model/algorithm-specific parameters. Routines for post-processing the results are also available. In order to face the computational complexity of the problem, the main functions are written in C++ and interfaced with R through the Rcpp and RcppArmadillo packages. The BNPmix packages is integrated in the R environment for Bayesian analysis and graphical visualization. 1.3.32 CONNECTOR: a computational approach to study intratumor heterogeneity. Simone Pernice, Ph.D student at Department of Computer Science of the University of Turin Track(s): R Life Sciences Abstract: Literature is characterized by a broad class of mathematical models which can be used for fitting cancer growth time series, but with no a global consensus or biological evidence that can drive the choice of the correct model. The conventional perception is that the mechanistic models enable the biological understanding of the systems under study. However, there is no way that these models can capture the variability characterizing the cancer progression, especially because of the irregularity and sparsity of the available data. For this reason, we propose CONNECTOR, an R package built on the model-based approach for clustering functional data. Such method is based on the clustering and fitting of the data through a combination of cubic natural splines basis with coefficients treated as random variables. Our approach is particularly effective when the observations are sparse and irregularly spaced, as growth curves usually are. CONNECTOR provides a tool set to easily guide through the parameters selection, i.e., (i) the dimension of the spline basis, (ii) the dimension of the mean space and (iii) the number of clusters to fit, to be properly chosen before fitting. The effectiveness of CONNECTOR is evaluated on growth curves of Patient Derived Xenografts (PDXs) of ovarian cancer. Genomic analyses of PDXs allowed correlating fitted and clustered PDX growth curves to cell population distribution. 1.3.33 gWQS: An R Package for Linear and Generalized Weighted Quantile Sum (WQS) Regression Stefano Renzetti, PhD Student at Università degli Studi di Milano Track(s): R Machine Learning &amp; Models, R Life Sciences Abstract: Weighted Quantile Sum (WQS) regression is a statistical model for multivariate regression in high-dimensional datasets commonly encountered in environmental exposures. The model constructs a weighted index estimating the mixture effect associated with all predictor variables on an outcome. The package gWQS extends WQS regression to applications with continuous, categorical and count outcomes. We provide four examples to illustrate the usage of the package. 1.3.34 R/LinkedCharts: A novel approach for simple but powerful interactive data analysis Svetlana Ovchinnikova, Doctoral student, Zentrum für Molekulare Biologie der Universität Heidelberg Track(s): R Dataviz &amp; Shiny Abstract: In exploratory data analysis, one usually jumps back and forth between visualizations that provide overview of the whole data and others that dive into details. In data quality assessment, for example, it might be very helpful to have one chart showing a summary statistic for all samples, and clicking on one of the data points would display details on this sample in a second plot. Setting up such interactively linked charts is usually cumbersome and time-consuming to use them in ad hoc analysis. We present R/LinkedChart, a framework that renders this tasks radically simple: Producing linked charts is as quickly done as is producing conventional static plots in R, requiring a data scientist to write only very few lines of simple R code to obtain complex and general visualization. We expect that the convinience of our new tool will enable data scientists and bioinformaticians to perform much deeper and more thorough EDA with much less effort. Furthermore, R/LinkedChart apps, typically first written as quick-and-dirty hacks, can also be polished to provide interactive data access in publication quality, thus contributing to open science. 1.3.35 Transparent Journalism Through the Power of R Tatjana Kecojevic, SisterAnalyst.org; founder and director Track(s): R World Abstract: This study examines the often-tricky process of delivering data literacy programmes to professionals with most to gain from a deeper understanding of data analysis. As such, the author discusses the process of building and delivering training strategies to journalists in regions where press freedom is constrained by numerous factors, not least of all institutionalised corruption. Reporting stories that are supplemented with transparent procedural systems are less likely to be contradicted and challenged by vested interest actors. Journalists are able to present findings supported by charts and info graphics, but these are open to translation. Therefore, most importantly, the data and code of the applied analytical methodology should also be available for scrutiny and is less likely to be subverted or prohibited. As part of creating an accessible programme geared to acquiring skills necessary for data journalism, the author takes a step-by-step approach to discussing the actualities of building online platforms for training purposes. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods in a transparent and reproducible way is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand. The resulting ‘R toolbox for journalists’ is an accessible open-source resource. It can also be adapted to accommodate the need to provide a deeper understanding of the potential for data proficiency to other professions. The accessibility of R allows for users to build support communities, which in the case of journalists is essential for information gathering. Establishing and implementing transparent channels of communication is the key to scrupulous journalism and is why R is so applicable to this objective. 1.3.36 Deduplicating real estate ads using Naive Bayes record linkage Thomas Maier, Datahouse AG Track(s): R Applications Abstract: We demonstrate in this talk, how we used a containerized R and PostgreSQL data pipeline to deduplicate 60 million real estate ads from Germany and Switzerland using a multi-step Naive Bayes record linkage approach. Real estate platforms publish millions of rental flat and condominium ads yearly. A given region or country of interest is normally covered by various competing platforms, leading to multiple published ads for a single real world object. Because quantifying and modeling the real estate market requires unbiased input data, our aim was to deduplicate real estate ads using Naive Bayes record linkage. We used commercially available German and Swiss real estate ad data from 2012 to 2019 consisting of approximately 60 million individual records. After multiple data cleaning and preparation steps we employed a Naive Bayes weighting of 12-14 variables to calculate similarity scores between ads and determined a linkage threshold based on expert judgment. The deduplication pipeline consisted of three steps: linking ads based on identity comparisons, linking similar ads within small regional areas (municipalities) and linking similar ads within large regional areas (cantons, states). The pipeline was deployed as a containerized setup with in-memory calculations in R and out-of-memory calculations and data storage in PostgreSQL. Deduplication linked the around 60 million ads to around 14 million object groups (Germany: 10 millions, Switzerland: 4 millions). The distribution of similarity scores showed high separation power and the resulting object groups displayed high homogeneity in geographic location and price distribution. Furthermore, yearly results corresponded well with published relocation rates. Using Naive Bayes record linkage to deduplicate real estate ads resulted in a sensible grouping of ads into object groups (rental flats, condominiums). We were able to combine similarities across different variables into a single similarity score. An advantage of the Naive Bayes approach is the high interpretability of the influence of individual variables. However, by manually determining the linkage threshold our results are heavily influenced by possible expert biases. The containerized R and PostgreSQL setup proved it’s portability and scaling capabilities. The same approach could easily be transferred to other domains requiring deduplication of multivariate data sets. 1.3.37 Global Poverty Monitoring at scale using R Tony Fujs, Data Scientist, World Bank Track(s): R Production, R Applications Abstract: The World Bank has been trying to assess and monitor the extent of extreme poverty in the world since 1979, and is currently responsible for reporting on the first Sustainable Development Goal, to end extreme poverty by 2030. The World Bank is using R to modernize its poverty data infrastructure, streamline the production of poverty statistics, and make this production process more transparent and less error prone. In this presentation, we will describe: • How we leveraged R to address some of the technical and methodological challenges of producing Global Poverty estimates, and streamline the production of poverty statistics • How we integrated best software development practices in the creation of our R packages, and worked closely with our IT team to deploy our R packages in production • How we are promoting transparency by publishing our tools as open-source, and fostering closer collaboration with the research and open-source community • We will also share some of the challenges encountered along the way 1.3.38 Mixed interactive debugging of R and native code with FastR and Vistual Studio Code Zbyněk Šlajchrt, Oracle Labs, Ph.D. Track(s): R World Abstract: Interactive debuggers are one of the most useful tools to aid software development. The two most used approaches for interactive debugging in the R ecosystem, the built-in debugger and R Studio, do not support interactive debugging of both R and C code of R packages at the same time and in one tool. FastR is an open source alternative R implementation that, apart from being compatible with GNU-R, puts emphasis on the performance of R execution and tooling support. FastR is part of the multilingual virtual machine (GraalVM) that, among other things, provides language agnostic support for interactive debugging. One of the other projects built on top of GraalVM is a C/C++ interpreter. FastR can be configured to run the native code of selected R packages using this C/C++ interpreter, which should yield the same behavior, but since both languages are now running in one system, it opens up many exciting possibilities including seamless cross-language debugging. In the talk, we will demonstrate how to configure Visual Studio Code and FastR for cross-language interactive debugging and how to debug a sample R package with native code. "],
["shiny-demo.html", "1.4 Shiny demo", " 1.4 Shiny demo 1.4.1 Visualising and Modelling Bike Sharing Mobility usage in the city of Milan Agostino Torti, PhD student - Politecnico di Milano Track(s): R Dataviz &amp; Shiny Abstract: A major trend in modern science is the collection of datasets which are not only “big” but also “complex”. This is particularly true in all sharing mobility systems where data are continuously collected at all time and they are characterised by a high number of features. To extract useful information contained in this huge mass of data, the development of novel statistical techniques and innovative visualization methods are requested. In this context, we focused on BikeMi, the main bike sharing system (BSS) in the city of Milan in Italy, and we implemented an R Shiny App to analyse and study people’s mobility behaviour across the city. Through the app, it is possible to dynamically select different parameters which allow to visualise the bike sharing flows among the districts of the city according to the hour of the day, the day of the week and the weather conditions. Moreover, a predictive model is implemented in the dashboard allowing to observe the future behaviour or the BSS. By doing this, we would like to both visualize the statistically significant spatio-temporal patterns of the users and to model each possible bike flow in the BikeMi network. 1.4.2 Media Shiny: Marketing Mix Models Builder Andrea Melloncelli, VanLog Track(s): R Dataviz &amp; Shiny, R Machine Learning &amp; Models, R Production Abstract: Marketing Mix Models are used to understand the effects of advertising campaigns. Building such models is challenging: first of all, it requires control of the external effects, such as seasonalities, competitor activities etc. Secondly, it requires to model the decay effect of communication (adstock effect: I do my advertising today, and in two weeks it still has some effect). The MediaShiny application allows to build Marketing Mix Models interactively: all steps of MMM, such as selecting, transforming and exploring features (time series), adstock control, model building, evaluation and forecasting can be done interactively. As a result the model explains the impact on sales of each media channel (tv, digital, press etc), controlling the external effects. An extra module allows the media budget optimization, using historical data to understand if the level of advertising has no impact or is too high (saturation). MediaShiny is a Package App developed with Golem and modularized with Shiny modules. Automatically built and provisioned as a Docker image running in a Shiny Proxy instance. Best User Experience is provided with Drag and Drop and navigation guided by action buttons. 1.4.3 ESPRES: A shiny web tool to support River Basin Management planning in European Watersheds Angel Udias, European Commission, Joint Research Centre (JRC), Ispra, Italy Track(s): R Life Sciences Abstract: Integrated river basin management must meet environmental targets while preserving the economic activities of its communities. Stakeholder decisions need to consider conflicting trade-offs between legislative environmental targets and economic activities, while maintaining a basis of transparency and accountability. ESPRES is a shiny web-based Decision Support Tool (DST) that can be used by stakeholders to explore management options in European water bodies. The management options considered in ESPRES are related with the pressures (water use and nutrient application) reduction. The shiny web interface provides a point of access to perform analyses of efficient pressure reduction strategies reflecting their perception of costs/effort, political difficulty, and social acceptability of the available solutions. Stakeholders express preferences and perceived difficulties in addressing each environmental pressure by assigning relative weights. The tool include a MOO engine to identified Pareto efficient strategies in terms of maximize the quality in the basin minimizing the total effort for reducing the pressures An online version of ESPRES is currently available (www.espres.eu) for four European basins of the Globaqua project, namely the Adige, the Ebro, the Evrotas, and the Sava, to addresses water abstraction and nutrient pollution pressures. 1.4.4 How green is your portfolio ? Tracking C02 footprint in the insurance sector Antoine Logean, Swiss Re, Casualty R&amp;D Department, Senior Data Scientist Track(s): R Production, R Applications, R Dataviz &amp; Shiny Abstract: Climate change is already having far-reaching global effects. They materialize, among other things, as a growing financial burden for governments and societies. Consequently, regulators are responding with new requirements to promote sustainability efforts. In addition, investment strategies of small and large investors are increasingly designed to better incorporate sustainability considerations, and to contribute to a decreased global carbon footprint. In this work an R application has been developed to facilitate establishing where high carbon footprint is a risk, whether it be in certain industrial segments or amongst specific companies. We utilize record linkage to match internal lists of high-risk corporations within existing insurance portfolio. Our tool is capable to pinpoint relevant characteristics of high-risk entities and it provides in-depth statistical analyses and data visualizations to end users. In our presentation, we also discuss preliminary quantitative comparisons of different industrial sector contributions. 1.4.5 Decision support for maritime spatial planning Dario Masante, GIS and data specialist Track(s): R Dataviz &amp; Shiny, R Applications Abstract: The VAPEM Shiny app provides an interface for managers and decision makers in maritime activities, to help understanding different management choices within the maritime spatial planning framework. In this domain, the planning of activities has proven complex, due primarily to the variety of stakeholders with contrasting interests to address, and the ecosystem’s limited capacity to provide services at sustainable levels, like fisheries. The VAPEM tool allows the easy exploration of a model that links Natural Capital and maritime activities, specifically by means of probabilistic graphical models (Bayesian networks) combined with interactive mapping. The user can set input parameters and query the model to get information on the spatial distribution and intensity of a certain maritime activity and its dependency on the ecosystem components, such as benthic habitats. From an R/Shiny perspective, this is an interesting example of smooth integration of modelling and user-oriented application, addressing different levels of expertise in the domain. Despite its focus on Basque Country and the use of Bayesian networks, the tool is transferable to any geographical domains and, with the due changes in parametrization, to different modelling techniques. 1.4.6 Automated, receptive and interactive: a classroom-based data generation exercise using Shiny Dean Langan, Senior Teaching Fellow (University College London, UK) Track(s): R Applications, R Dataviz &amp; Shiny Abstract: Students find it easier to engage with statistics training when presented with examples from a familiar subject area. However, when teaching students of varying professional backgrounds, finding relatable examples can be especially challenging. Classroom based data generation exercises offer a solution as all students are involved in the process from data collection through to the choice and use of appropriate analyses. One such exercise that forms an integral part of an introductory statistics course is based on beer mat (coaster) flipping, a popular UK pub game. We have recently moved the data collection process online so that students can enter data via their smartphones and developed a web application using the R package shiny in R. This application allows students to explore the results interactively and independently. The application comes to life with visual demonstrations of core statistical concepts such as the central limit theorem and bootstrapping. This technology further engages students and the ensuing discussion comparing outputs and interpretation is a welcome addition to classroom interactivity. We present details of this exercise, focusing on use of the web application, example outputs, and student feedback. 1.4.7 tsviz: a data-scientist-friendly addin for RStudio Emanuele Fabbiani, Chief Data Scientist at xtream, PhD student in Machine Learning Track(s): R Dataviz &amp; Shiny Abstract: In recent years, charting libraries have evolved following two main directions. First, they provided users with as many features as possible and second, they added high-level APIs to easily create the most frequent visualizations. RStudio, with its addins, offers the opportunity to further ease the creation of common plots. Born as an internal project in xtream, tsviz is an open-source Shiny-based addin which contains powerful tools to perform explorative analysis of multivariate time series. Its usage is dead simple. Once launched, it scans the global environment for suitable variables. You chose one, and several plots of the time series are shown. Line charts, scatter plots, autocorrelogram, periodogram are only a few examples. Interactivity is achieved by the miniUI framework and the adoption of Plotly charts. Its wide adoption among our customers and the overall positive feedback we received demonstrate how addins, usually thought as shortcuts for developers, may provide effective support to data scientists in performing their routine tasks. 1.4.8 Mobility scan Josue Aduna, Behavioural and data scientist at Livemobility Track(s): R Dataviz &amp; Shiny Abstract: This is a Shiny application designed and developed to foster sustainable mobility behavior under a specific initiative that I currently work in: Livemobility (see https://www.livemobility.com/). Broadly speaking, Livemobility is a platform that rewards people for sustainable commuting behavior and helps companies to save money, avoid environmental pollution, improve public health and save travel time. This is achieved through a digital ecosystem that analyses mobility behavior and generates personalized insights to improve mobility efficiency. The Shiny application makes use of web interactive settings together with Google Maps APIs to provide relevant indicators of impact, generate geographic scans and create mobility profiles. 1.4.9 Developing Shiny applications to facilitate precision agriculture workflows Lorenzo Busetto, Institute on Remote Sensing of Enviroment - National Research Council of Italy (CNR-IREA) Track(s): R Applications, R Dataviz &amp; Shiny Abstract: Precision Agriculture applications rely on geospatial datasets from heterogeneous sources such as crop maps, information about fertilization/phytosanitary treatments, satellite and meteo data, to optimize agricultural practices from an economic and environmental standpoint. Software instruments allowing to easily record, manage and process such datasets are therefore of paramount importance to facilitate, standardize and speed-up the steps required to implement specific workflows. Although required functionalities are available in open source/commercial software, technicians are often required to use different software tools. This affects the time and effort required to replicate a specific workflow on different areas and crop seasons. In this contribution we present our experience in developing two Shiny-based prototypes specifically tailored to the needs of operators of a agro-consulting firm providing precision agriculture services. The first prototype is mainly aimed at providing a simplified, standardized and scalable way to insert, record and query information about agricultural practices, such as crop type/variety, fertilisation and phytosanitary treatments and yield. The second is instead dedicated to facilitating access to satellite imagery data, and applying dedicated processing algorithms for identification of homegenous Management Unit Zones. 1.4.10 “GUInterp”: a Shiny GUI to support spatial interpolation Luigi Ranghetti, Institute for Remote Sensing of Environment, Consiglio Nazionale delle Ricerche (IREA-CNR) Track(s): R Dataviz &amp; Shiny Abstract: In this demo we present “GUInterp”, a Shiny interface written to facilitate the operations required to interpolate point data. A typical spatial interpolation workflow includes common steps: loading point data, filtering them to exclude undesired outlier values, setting the interpolation method and parameters, defining an output raster grid and processing data. Interpolation can be conducted in R using dedicated packages; nevertheless, the availability of an interactive interface could be useful to provide additional control during steps requiring user intervention and to facilitate users with low or no programming skills. “GUInterp” was written for this purpose. The user can import input point data, optionally loading a polygon dataset of borders used to constrain the extent of the interpolated outputs. A set of selectors allows filtering input points based on the distribution of the variable to interpolate (which is shown with a reactive histogram) or the spatial position of points (visible on a map). The interpolation can be performed with IDW or Ordinary Kriging methods: in the latter case, the semivariogram can be interactively defined and optimised using a dedicated interface. Further settings can be exploited to tune computation requirements (RAM usage, amount of time) on the basis of available hardware or user needs. “GUInterp” is released as R package under the GNU GPL-3 license. 1.4.11 Scoring the Implicit Association Test has never been easier: DscoreApp Ottavia M. Epifania, University of Padova (IT) Track(s): R Dataviz &amp; Shiny Abstract: Throughout the past decades, the interest in the implicit investigation of attitudes and preferences has been constantly growing among social scientists, and the Implicit Association Test (IAT) is one of the most common measures used for this aim. The so-called “IAT effect” (i.e., the difference in respondents’ performance between two contrasting categorization tasks) is usually expressed by the D-score. Despite that several options exist for computing the D-score, including R packages and SPSS syntaxes, none of them provides either an easy to use interface or a means for immediately visualizing the results. A Shiny Web application (DscoreApp) was developed to provide IAT users with an easy to use and powerful tool for the computation of the D-score. DscoreApp allows users to upload their IAT data, decide which specific D-score algorithm to compute, and immediately see the results in easy to read and interactive graphs. At the end of the computation, users can download a data frame containing the computed D-score and other information on respondents’ performance, such as the proportion of correct responses or the number of trials exceeding a time threshold. Graphical representations can be downloaded as well. Besides providing an easy to use and open source tool for computing the D-score, DscoreApp allows for grasping an immediate overview of the results, and to visually inspect them. 1.4.12 Interactive project management tool using shiny Parvaneh Shafiei, Data scientist Track(s): R Dataviz &amp; Shiny, R Production Abstract: This tools is developing based on shiny enables project managers to create multiple projects and: - Update status of project ( in progress, to do, etc.), - Define tasks &amp; prioritize them - Assign tasks to users - See multiple visutalization related to projects &amp; tasks The tool is multi-sign dashboard &amp; has a dedicated dashbaord for each user for giving them various accessibilities. Users are able to: - Visualize project timeline - Create &amp; follow dedicated tasks &amp; update their status - Create visualizations &amp; reporting 1.4.13 rTRhexNG: Hexagon sticker app for rTRNG Riccardo Porreca, R Enthusiast at Mirai Solutions Track(s): R Dataviz &amp; Shiny Abstract: Hexagon stickers have become a popular way to make software tools, and R packages in particular, visually recognizable and stand out as landmarks in an ever-growing ecosystem. In general, good hexagon logos are not only visually appealing but also convey the key aspects of a package with their graphical design. In this talk, we will showcase rTRhexNG (https://github.com/miraisolutions/rTRhexNG#readme), a Shiny app built for creating the hexagon sticker of the rTRNG (https://github.com/miraisolutions/rTRNG#readme) package. The core idea behind the logo was to have an appealing design that would at the same time illustrate the key features of the package: jump and split operations on (pseudo-)random sequences. Leveraging on the simple yet powerful SVG image format, R was used to automate the creation and location of several visual elements representing random sequences, and a Shiny app was built on top to quickly assess different designs in an interactive way. We demonstrate the Shiny app in action to concretely explain what jump and split mean in rTRNG, and show how the sticker design naturally emerges from their visual representation. The power of this interactive yet automated approach was invaluable to fine-tune the final look of the sticker, also allowing to easily explore alternative polygon or circle designs the implementation naturally extends to. "]
]
