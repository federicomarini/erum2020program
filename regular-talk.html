<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 Regular talk | eRum2020 Program</title>
  <meta name="description" content="1.3 Regular talk | eRum2020 Program" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 Regular talk | eRum2020 Program" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 Regular talk | eRum2020 Program" />
  
  
  

<meta name="author" content="eRum2020 organizing Committee" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="poster.html"/>
<link rel="next" href="shiny-demo.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> eRum2020 Program Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="lightning-talk.html"><a href="lightning-talk.html"><i class="fa fa-check"></i><b>1.1</b> Lightning talk</a><ul>
<li class="chapter" data-level="1.1.1" data-path="lightning-talk.html"><a href="lightning-talk.html#an-enriched-disease-risk-assessment-model-based-on-historical-blood-donors-records"><i class="fa fa-check"></i><b>1.1.1</b> An enriched disease risk assessment model based on historical blood donors records</a></li>
<li class="chapter" data-level="1.1.2" data-path="lightning-talk.html"><a href="lightning-talk.html#next-generation-supply-chain-planning-with-r-a-case-study"><i class="fa fa-check"></i><b>1.1.2</b> Next Generation Supply Chain Planning With R: A Case Study</a></li>
<li class="chapter" data-level="1.1.3" data-path="lightning-talk.html"><a href="lightning-talk.html#rdwd-r-interface-to-german-weather-service-data"><i class="fa fa-check"></i><b>1.1.3</b> rdwd: R interface to German Weather Service data</a></li>
<li class="chapter" data-level="1.1.4" data-path="lightning-talk.html"><a href="lightning-talk.html#tv-show-data-frames-in-the-browser"><i class="fa fa-check"></i><b>1.1.4</b> tv: Show Data Frames in the Browser</a></li>
<li class="chapter" data-level="1.1.5" data-path="lightning-talk.html"><a href="lightning-talk.html#predicting-the-euro-2020-results-using-tournament-rank-probabilities-scores-from-the-soccer-package"><i class="fa fa-check"></i><b>1.1.5</b> Predicting the Euro 2020 results using tournament rank probabilities scores from the socceR package</a></li>
<li class="chapter" data-level="1.1.6" data-path="lightning-talk.html"><a href="lightning-talk.html#r-in-medical-research-users-and-stakeholders"><i class="fa fa-check"></i><b>1.1.6</b> R in Medical Research: UseRs and StakeholdeRs</a></li>
<li class="chapter" data-level="1.1.7" data-path="lightning-talk.html"><a href="lightning-talk.html#ultra-fast-penalized-regressions-with-r-package-bigstatsr"><i class="fa fa-check"></i><b>1.1.7</b> Ultra fast penalized regressions with R package {bigstatsr}</a></li>
<li class="chapter" data-level="1.1.8" data-path="lightning-talk.html"><a href="lightning-talk.html#supporting-twitter-analytics-application-with-graph-databases-and-the-arangodb-package"><i class="fa fa-check"></i><b>1.1.8</b> Supporting Twitter analytics application with graph-databases and the aRangodb package</a></li>
<li class="chapter" data-level="1.1.9" data-path="lightning-talk.html"><a href="lightning-talk.html#deep-learning-and-time-series-approaches-for-improvement-of-vehicle-distribution-process"><i class="fa fa-check"></i><b>1.1.9</b> Deep learning and time series approaches for improvement of vehicle distribution process</a></li>
<li class="chapter" data-level="1.1.10" data-path="lightning-talk.html"><a href="lightning-talk.html#what-are-the-potato-eaters-eating"><i class="fa fa-check"></i><b>1.1.10</b> What are the potato eaters eating</a></li>
<li class="chapter" data-level="1.1.11" data-path="lightning-talk.html"><a href="lightning-talk.html#dm-working-with-relational-data-models-in-r"><i class="fa fa-check"></i><b>1.1.11</b> dm: working with relational data models in R</a></li>
<li class="chapter" data-level="1.1.12" data-path="lightning-talk.html"><a href="lightning-talk.html#explaining-black-box-models-with-xspliner-to-make-deliberate-business-decisions"><i class="fa fa-check"></i><b>1.1.12</b> Explaining black-box models with xspliner to make deliberate business decisions</a></li>
<li class="chapter" data-level="1.1.13" data-path="lightning-talk.html"><a href="lightning-talk.html#using-open-access-data-to-derive-genome-composition-of-emerging-viruses"><i class="fa fa-check"></i><b>1.1.13</b> Using open-access data to derive genome composition of emerging viruses</a></li>
<li class="chapter" data-level="1.1.14" data-path="lightning-talk.html"><a href="lightning-talk.html#a-principal-component-analysis-based-method-to-detect-biomarker-captation-from-vibrational-spectra"><i class="fa fa-check"></i><b>1.1.14</b> A principal component analysis based method to detect biomarker captation from vibrational spectra</a></li>
<li class="chapter" data-level="1.1.15" data-path="lightning-talk.html"><a href="lightning-talk.html#emojis-show-your-emotions"><i class="fa fa-check"></i><b>1.1.15</b> Emojis; show your emotions! 😀</a></li>
<li class="chapter" data-level="1.1.16" data-path="lightning-talk.html"><a href="lightning-talk.html#an-innovative-way-to-support-your-sales-force"><i class="fa fa-check"></i><b>1.1.16</b> An innovative way to support your sales force</a></li>
<li class="chapter" data-level="1.1.17" data-path="lightning-talk.html"><a href="lightning-talk.html#ptmixed-an-r-package-for-flexible-modelling-of-longitudinal-overdispersed-count-data"><i class="fa fa-check"></i><b>1.1.17</b> ptmixed: an R package for flexible modelling of longitudinal overdispersed count data</a></li>
<li class="chapter" data-level="1.1.18" data-path="lightning-talk.html"><a href="lightning-talk.html#one-way-non-normal-anova-in-reliability-analysis-using-with-doex"><i class="fa fa-check"></i><b>1.1.18</b> One-way non-normal ANOVA in reliability analysis using with doex</a></li>
<li class="chapter" data-level="1.1.19" data-path="lightning-talk.html"><a href="lightning-talk.html#towards-more-structured-data-quality-assessment-in-the-process-mining-field-the-daqapo-package"><i class="fa fa-check"></i><b>1.1.19</b> Towards more structured data quality assessment in the process mining field: the DaQAPO package</a></li>
<li class="chapter" data-level="1.1.20" data-path="lightning-talk.html"><a href="lightning-talk.html#analyzing-preference-data-with-the-bayesmallows-package"><i class="fa fa-check"></i><b>1.1.20</b> Analyzing Preference Data with the BayesMallows Package</a></li>
<li class="chapter" data-level="1.1.21" data-path="lightning-talk.html"><a href="lightning-talk.html#supporting-r-in-the-binder-community"><i class="fa fa-check"></i><b>1.1.21</b> Supporting R in the Binder Community</a></li>
<li class="chapter" data-level="1.1.22" data-path="lightning-talk.html"><a href="lightning-talk.html#flexible-deep-learning-via-the-juliaconnector"><i class="fa fa-check"></i><b>1.1.22</b> Flexible deep learning via the JuliaConnectoR</a></li>
<li class="chapter" data-level="1.1.23" data-path="lightning-talk.html"><a href="lightning-talk.html#time-series-missing-data-visualizations"><i class="fa fa-check"></i><b>1.1.23</b> Time Series Missing Data Visualizations</a></li>
<li class="chapter" data-level="1.1.24" data-path="lightning-talk.html"><a href="lightning-talk.html#effectclass-an-r-package-to-interpret-effects-and-visualise-uncertainty"><i class="fa fa-check"></i><b>1.1.24</b> effectclass: an R package to interpret effects and visualise uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="poster.html"><a href="poster.html"><i class="fa fa-check"></i><b>1.2</b> Poster</a><ul>
<li class="chapter" data-level="1.2.1" data-path="poster.html"><a href="poster.html#modified-likelihood-ratio-model-for-handwriting-recognition-in-forensic-science."><i class="fa fa-check"></i><b>1.2.1</b> Modified likelihood ratio model for handwriting recognition in forensic science.</a></li>
<li class="chapter" data-level="1.2.2" data-path="poster.html"><a href="poster.html#the-r-package-flexreg-regression-mixture-models-for-bounded-responses"><i class="fa fa-check"></i><b>1.2.2</b> The R-package ‘FlexReg’: regression mixture models for bounded responses</a></li>
<li class="chapter" data-level="1.2.3" data-path="poster.html"><a href="poster.html#using-r-for-analysis-of-microscopic-images-of-streptomyces-growth-and-chromosome-distribution"><i class="fa fa-check"></i><b>1.2.3</b> Using R for analysis of microscopic images of Streptomyces growth and chromosome distribution</a></li>
<li class="chapter" data-level="1.2.4" data-path="poster.html"><a href="poster.html#a-flexible-dashboard-for-monitoring-platform-trials"><i class="fa fa-check"></i><b>1.2.4</b> A flexible dashboard for monitoring platform trials</a></li>
<li class="chapter" data-level="1.2.5" data-path="poster.html"><a href="poster.html#prda-package-enhancing-statistical-inference-via-prospective-and-retrospective-design-analysis."><i class="fa fa-check"></i><b>1.2.5</b> PRDA package: Enhancing Statistical Inference via Prospective and Retrospective Design Analysis.</a></li>
<li class="chapter" data-level="1.2.6" data-path="poster.html"><a href="poster.html#corpfinder--a-new-application-to-identify-large-corporate-risks"><i class="fa fa-check"></i><b>1.2.6</b> CorpFinder- a new application to identify Large Corporate Risks</a></li>
<li class="chapter" data-level="1.2.7" data-path="poster.html"><a href="poster.html#automate-flexdashboard-with-github"><i class="fa fa-check"></i><b>1.2.7</b> Automate flexdashboard with GitHub</a></li>
<li class="chapter" data-level="1.2.8" data-path="poster.html"><a href="poster.html#it-process-optimization-in-data-lake-via-r"><i class="fa fa-check"></i><b>1.2.8</b> IT Process Optimization in Data Lake via R</a></li>
<li class="chapter" data-level="1.2.9" data-path="poster.html"><a href="poster.html#level-up-your-tables-with-tablehtml-in-r"><i class="fa fa-check"></i><b>1.2.9</b> Level up your tables with tableHTML in R</a></li>
<li class="chapter" data-level="1.2.10" data-path="poster.html"><a href="poster.html#guidance-for-teaching-r-to-non-programmers"><i class="fa fa-check"></i><b>1.2.10</b> Guidance for teaching R to non-programmers</a></li>
<li class="chapter" data-level="1.2.11" data-path="poster.html"><a href="poster.html#transparent-presentation-of-uncertain-lotteries-using-deals"><i class="fa fa-check"></i><b>1.2.11</b> Transparent presentation of uncertain lotteries using {deals}</a></li>
<li class="chapter" data-level="1.2.12" data-path="poster.html"><a href="poster.html#newwave-a-scalable-r-package-for-the-dimensionality-reduction-of-single-cell-rna-seq"><i class="fa fa-check"></i><b>1.2.12</b> NewWave: a scalable R package for the dimensionality reduction of single-cell RNA-seq</a></li>
<li class="chapter" data-level="1.2.13" data-path="poster.html"><a href="poster.html#orf-ordered-random-forests"><i class="fa fa-check"></i><b>1.2.13</b> orf: Ordered Random Forests</a></li>
<li class="chapter" data-level="1.2.14" data-path="poster.html"><a href="poster.html#biomarker-discovery-by-the-leveraging-of-omic-and-clinical-data-using-biomarkerbox."><i class="fa fa-check"></i><b>1.2.14</b> Biomarker discovery by the leveraging of omic and clinical data using biomarkeRbox.</a></li>
<li class="chapter" data-level="1.2.15" data-path="poster.html"><a href="poster.html#an-package-for-bayesian-analysis-of-structured-time-series-models-with"><i class="fa fa-check"></i><b>1.2.15</b> An  package for Bayesian analysis of structured time series models with </a></li>
<li class="chapter" data-level="1.2.16" data-path="poster.html"><a href="poster.html#bridging-the-gap-between-r-and-computer-vision"><i class="fa fa-check"></i><b>1.2.16</b> Bridging the gap between R and computer vision</a></li>
<li class="chapter" data-level="1.2.17" data-path="poster.html"><a href="poster.html#how-r-hub-can-help-you-develop-and-maintain-your-r-packages"><i class="fa fa-check"></i><b>1.2.17</b> How R-hub can help you develop and maintain your R packages</a></li>
<li class="chapter" data-level="1.2.18" data-path="poster.html"><a href="poster.html#power-supply-health-status-monitoring-dashboard"><i class="fa fa-check"></i><b>1.2.18</b> Power Supply health status monitoring dashboard</a></li>
<li class="chapter" data-level="1.2.19" data-path="poster.html"><a href="poster.html#first-year-ict-students-dropout-predicting-with-r-models"><i class="fa fa-check"></i><b>1.2.19</b> First-year ICT students dropout predicting with R models</a></li>
<li class="chapter" data-level="1.2.20" data-path="poster.html"><a href="poster.html#benchmark-percentage-disjoint-data-splitting-in-cross-validation-for-assessing-the-skill-of-machine"><i class="fa fa-check"></i><b>1.2.20</b> Benchmark Percentage Disjoint Data Splitting in Cross Validation for Assessing the Skill of Machine</a></li>
<li class="chapter" data-level="1.2.21" data-path="poster.html"><a href="poster.html#integrating-professional-software-engineering-practices-in-medical-research-software"><i class="fa fa-check"></i><b>1.2.21</b> Integrating professional software engineering practices in medical research software</a></li>
<li class="chapter" data-level="1.2.22" data-path="poster.html"><a href="poster.html#a-three-parameter-gompertz-lindley-distribution-its-properties-and-applications"><i class="fa fa-check"></i><b>1.2.22</b> A Three-Parameter Gompertz-Lindley Distribution: Its Properties And Applications</a></li>
<li class="chapter" data-level="1.2.23" data-path="poster.html"><a href="poster.html#partitional-clustering-with-extensions"><i class="fa fa-check"></i><b>1.2.23</b> Partitional clustering with extensions</a></li>
<li class="chapter" data-level="1.2.24" data-path="poster.html"><a href="poster.html#dealing-with-changing-administrative-boundaries-the-case-of-swiss-municipalities"><i class="fa fa-check"></i><b>1.2.24</b> Dealing with changing administrative boundaries: The case of Swiss municipalities</a></li>
<li class="chapter" data-level="1.2.25" data-path="poster.html"><a href="poster.html#baddea-an-r-package-for-measuring-firms-efficiency-adjusted-by-undesirable-outputs"><i class="fa fa-check"></i><b>1.2.25</b> badDEA: An R package for measuring firms’ efficiency adjusted by undesirable outputs</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="regular-talk.html"><a href="regular-talk.html"><i class="fa fa-check"></i><b>1.3</b> Regular talk</a><ul>
<li class="chapter" data-level="1.3.1" data-path="regular-talk.html"><a href="regular-talk.html#design-patterns-for-big-shiny-apps"><i class="fa fa-check"></i><b>1.3.1</b> Design Patterns For Big Shiny Apps</a></li>
<li class="chapter" data-level="1.3.2" data-path="regular-talk.html"><a href="regular-talk.html#using-xgboost-plumber-and-docker-in-production-to-power-a-new-banking-product"><i class="fa fa-check"></i><b>1.3.2</b> Using XGBoost, Plumber and Docker in production to power a new banking product</a></li>
<li class="chapter" data-level="1.3.3" data-path="regular-talk.html"><a href="regular-talk.html#astronomical-source-detection-and-background-separation-a-bayesian-nonparametric-approach"><i class="fa fa-check"></i><b>1.3.3</b> Astronomical source detection and background separation: a Bayesian nonparametric approach</a></li>
<li class="chapter" data-level="1.3.4" data-path="regular-talk.html"><a href="regular-talk.html#validation-of-visual-inference-methods-using-deep-learning-in-r"><i class="fa fa-check"></i><b>1.3.4</b> Validation of visual inference methods using deep learning in R</a></li>
<li class="chapter" data-level="1.3.5" data-path="regular-talk.html"><a href="regular-talk.html#high-dimensional-sampling-and-volume-computation"><i class="fa fa-check"></i><b>1.3.5</b> High dimensional sampling and volume computation</a></li>
<li class="chapter" data-level="1.3.6" data-path="regular-talk.html"><a href="regular-talk.html#fake-news-ai-on-the-battle-ground"><i class="fa fa-check"></i><b>1.3.6</b> Fake News: AI on the battle ground</a></li>
<li class="chapter" data-level="1.3.7" data-path="regular-talk.html"><a href="regular-talk.html#automation-of-file-monitoring-in-a-data-lake-for-large-scale-systems"><i class="fa fa-check"></i><b>1.3.7</b> Automation of File Monitoring in a Data Lake for Large Scale Systems</a></li>
<li class="chapter" data-level="1.3.8" data-path="regular-talk.html"><a href="regular-talk.html#from-consulting-to-open-source-and-back"><i class="fa fa-check"></i><b>1.3.8</b> From consulting to open-source and back</a></li>
<li class="chapter" data-level="1.3.9" data-path="regular-talk.html"><a href="regular-talk.html#how-to-apply-r-in-a-hospital-environment-on-standard-available-hospital-wide-data"><i class="fa fa-check"></i><b>1.3.9</b> How to apply R in a hospital environment on standard available hospital-wide data</a></li>
<li class="chapter" data-level="1.3.10" data-path="regular-talk.html"><a href="regular-talk.html#polite-web-etiquette-for-r-users"><i class="fa fa-check"></i><b>1.3.10</b> {polite}: web etiquette for R users</a></li>
<li class="chapter" data-level="1.3.11" data-path="regular-talk.html"><a href="regular-talk.html#hydrological-modelling-and-r"><i class="fa fa-check"></i><b>1.3.11</b> Hydrological Modelling and R</a></li>
<li class="chapter" data-level="1.3.12" data-path="regular-talk.html"><a href="regular-talk.html#genetonic-enjoy-rna-seq-data-analysis-responsibly"><i class="fa fa-check"></i><b>1.3.12</b> GeneTonic: enjoy RNA-seq data analysis, responsibly</a></li>
<li class="chapter" data-level="1.3.13" data-path="regular-talk.html"><a href="regular-talk.html#a-simple-and-flexible-inactivitysleep-detection-r-package"><i class="fa fa-check"></i><b>1.3.13</b> A simple and flexible inactivity/sleep detection R package</a></li>
<li class="chapter" data-level="1.3.14" data-path="regular-talk.html"><a href="regular-talk.html#progressr-an-inclusive-unifying-api-for-progress-updates"><i class="fa fa-check"></i><b>1.3.14</b> progressr: An Inclusive, Unifying API for Progress Updates</a></li>
<li class="chapter" data-level="1.3.15" data-path="regular-talk.html"><a href="regular-talk.html#varycoef-modeling-spatially-varying-coefficients"><i class="fa fa-check"></i><b>1.3.15</b> varycoef: Modeling Spatially Varying Coefficients</a></li>
<li class="chapter" data-level="1.3.16" data-path="regular-talk.html"><a href="regular-talk.html#fastai-in-r-preserving-wildlife-with-computer-vision"><i class="fa fa-check"></i><b>1.3.16</b> FastAI in R: preserving wildlife with computer vision</a></li>
<li class="chapter" data-level="1.3.17" data-path="regular-talk.html"><a href="regular-talk.html#shazam-in-r-audio-analysis-using-the-av-package"><i class="fa fa-check"></i><b>1.3.17</b> Shazam in R? Audio analysis using the ‘av’ package</a></li>
<li class="chapter" data-level="1.3.18" data-path="regular-talk.html"><a href="regular-talk.html#powering-turing-e-atlas-with-r"><i class="fa fa-check"></i><b>1.3.18</b> Powering Turing e-Atlas with R</a></li>
<li class="chapter" data-level="1.3.19" data-path="regular-talk.html"><a href="regular-talk.html#r-at-the-service-of-plastic-surgery-a-web-based-shiny-application-evaluating-facial-attractiveness"><i class="fa fa-check"></i><b>1.3.19</b> R at the service of plastic surgery: a web-based shiny application evaluating facial attractiveness</a></li>
<li class="chapter" data-level="1.3.20" data-path="regular-talk.html"><a href="regular-talk.html#manifoldgstat-an-r-package-for-spatial-statistics-of-manifold-data"><i class="fa fa-check"></i><b>1.3.20</b> Manifoldgstat: an R package for spatial statistics of manifold data</a></li>
<li class="chapter" data-level="1.3.21" data-path="regular-talk.html"><a href="regular-talk.html#voronoi-linkage-for-spatially-misaligned-data"><i class="fa fa-check"></i><b>1.3.21</b> Voronoi Linkage for Spatially Misaligned Data</a></li>
<li class="chapter" data-level="1.3.22" data-path="regular-talk.html"><a href="regular-talk.html#be-proud-of-your-code-tools-and-patterns-for-making-production-ready-clean-r-code"><i class="fa fa-check"></i><b>1.3.22</b> Be proud of your code! Tools and patterns for making production-ready, clean R code</a></li>
<li class="chapter" data-level="1.3.23" data-path="regular-talk.html"><a href="regular-talk.html#damirseq-2.0-from-high-dimensional-data-to-cost-effective-reliable-prediction-models"><i class="fa fa-check"></i><b>1.3.23</b> DaMiRseq 2.0: from high dimensional data to cost-effective reliable prediction models</a></li>
<li class="chapter" data-level="1.3.24" data-path="regular-talk.html"><a href="regular-talk.html#interpretable-and-accessible-deep-learning-for-omics-data-with-r-and-friends"><i class="fa fa-check"></i><b>1.3.24</b> Interpretable and accessible Deep Learning for omics data with R and friends</a></li>
<li class="chapter" data-level="1.3.25" data-path="regular-talk.html"><a href="regular-talk.html#elevating-shiny-module-with-tidymodules"><i class="fa fa-check"></i><b>1.3.25</b> Elevating shiny module with {tidymodules}</a></li>
<li class="chapter" data-level="1.3.26" data-path="regular-talk.html"><a href="regular-talk.html#apfr-average-power-function-and-bayes-fdr-for-robust-brain-networks-construction"><i class="fa fa-check"></i><b>1.3.26</b> APFr: Average Power Function and Bayes FDR for Robust Brain Networks Construction</a></li>
<li class="chapter" data-level="1.3.27" data-path="regular-talk.html"><a href="regular-talk.html#flexible-meta-analysis-of-generalized-additive-models-with-metagam"><i class="fa fa-check"></i><b>1.3.27</b> Flexible Meta-Analysis of Generalized Additive Models with metagam</a></li>
<li class="chapter" data-level="1.3.28" data-path="regular-talk.html"><a href="regular-talk.html#controlled-r-development-with-docker"><i class="fa fa-check"></i><b>1.3.28</b> Controlled R development with Docker</a></li>
<li class="chapter" data-level="1.3.29" data-path="regular-talk.html"><a href="regular-talk.html#correlaidx---building-r-focused-communities-for-social-good-on-the-local-level"><i class="fa fa-check"></i><b>1.3.29</b> CorrelAidX - Building R-focused Communities for Social Good on the Local Level</a></li>
<li class="chapter" data-level="1.3.30" data-path="regular-talk.html"><a href="regular-talk.html#interactive-visualization-of-complex-texts"><i class="fa fa-check"></i><b>1.3.30</b> Interactive visualization of complex texts</a></li>
<li class="chapter" data-level="1.3.31" data-path="regular-talk.html"><a href="regular-talk.html#bnpmix-an-new-package-to-estimate-bayesian-nonparametric-mixtures"><i class="fa fa-check"></i><b>1.3.31</b> BNPmix: an new package to estimate Bayesian nonparametric mixtures</a></li>
<li class="chapter" data-level="1.3.32" data-path="regular-talk.html"><a href="regular-talk.html#connector-a-computational-approach-to-study-intratumor-heterogeneity."><i class="fa fa-check"></i><b>1.3.32</b> CONNECTOR: a computational approach to study intratumor heterogeneity.</a></li>
<li class="chapter" data-level="1.3.33" data-path="regular-talk.html"><a href="regular-talk.html#gwqs-an-r-package-for-linear-and-generalized-weighted-quantile-sum-wqs-regression"><i class="fa fa-check"></i><b>1.3.33</b> gWQS: An R Package for Linear and Generalized Weighted Quantile Sum (WQS) Regression</a></li>
<li class="chapter" data-level="1.3.34" data-path="regular-talk.html"><a href="regular-talk.html#rlinkedcharts-a-novel-approach-for-simple-but-powerful-interactive-data-analysis"><i class="fa fa-check"></i><b>1.3.34</b> R/LinkedCharts: A novel approach for simple but powerful interactive data analysis</a></li>
<li class="chapter" data-level="1.3.35" data-path="regular-talk.html"><a href="regular-talk.html#transparent-journalism-through-the-power-of-r"><i class="fa fa-check"></i><b>1.3.35</b> Transparent Journalism Through the Power of R</a></li>
<li class="chapter" data-level="1.3.36" data-path="regular-talk.html"><a href="regular-talk.html#deduplicating-real-estate-ads-using-naive-bayes-record-linkage"><i class="fa fa-check"></i><b>1.3.36</b> Deduplicating real estate ads using Naive Bayes record linkage</a></li>
<li class="chapter" data-level="1.3.37" data-path="regular-talk.html"><a href="regular-talk.html#global-poverty-monitoring-at-scale-using-r"><i class="fa fa-check"></i><b>1.3.37</b> Global Poverty Monitoring at scale using R</a></li>
<li class="chapter" data-level="1.3.38" data-path="regular-talk.html"><a href="regular-talk.html#mixed-interactive-debugging-of-r-and-native-code-with-fastr-and-vistual-studio-code"><i class="fa fa-check"></i><b>1.3.38</b> Mixed interactive debugging of R and native code with FastR and Vistual Studio Code</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="shiny-demo.html"><a href="shiny-demo.html"><i class="fa fa-check"></i><b>1.4</b> Shiny demo</a><ul>
<li class="chapter" data-level="1.4.1" data-path="shiny-demo.html"><a href="shiny-demo.html#visualising-and-modelling-bike-sharing-mobility-usage-in-the-city-of-milan"><i class="fa fa-check"></i><b>1.4.1</b> Visualising and Modelling Bike Sharing Mobility usage in the city of Milan</a></li>
<li class="chapter" data-level="1.4.2" data-path="shiny-demo.html"><a href="shiny-demo.html#media-shiny-marketing-mix-models-builder"><i class="fa fa-check"></i><b>1.4.2</b> Media Shiny: Marketing Mix Models Builder</a></li>
<li class="chapter" data-level="1.4.3" data-path="shiny-demo.html"><a href="shiny-demo.html#espres-a-shiny-web-tool-to-support-river-basin-management-planning-in-european-watersheds"><i class="fa fa-check"></i><b>1.4.3</b> ESPRES: A shiny web tool to support River Basin Management planning in European Watersheds</a></li>
<li class="chapter" data-level="1.4.4" data-path="shiny-demo.html"><a href="shiny-demo.html#how-green-is-your-portfolio-tracking-c02-footprint-in-the-insurance-sector"><i class="fa fa-check"></i><b>1.4.4</b> How green is your portfolio ? Tracking C02 footprint in the insurance sector</a></li>
<li class="chapter" data-level="1.4.5" data-path="shiny-demo.html"><a href="shiny-demo.html#decision-support-for-maritime-spatial-planning"><i class="fa fa-check"></i><b>1.4.5</b> Decision support for maritime spatial planning</a></li>
<li class="chapter" data-level="1.4.6" data-path="shiny-demo.html"><a href="shiny-demo.html#automated-receptive-and-interactive-a-classroom-based-data-generation-exercise-using-shiny"><i class="fa fa-check"></i><b>1.4.6</b> Automated, receptive and interactive: a classroom-based data generation exercise using Shiny</a></li>
<li class="chapter" data-level="1.4.7" data-path="shiny-demo.html"><a href="shiny-demo.html#tsviz-a-data-scientist-friendly-addin-for-rstudio"><i class="fa fa-check"></i><b>1.4.7</b> tsviz: a data-scientist-friendly addin for RStudio</a></li>
<li class="chapter" data-level="1.4.8" data-path="shiny-demo.html"><a href="shiny-demo.html#mobility-scan"><i class="fa fa-check"></i><b>1.4.8</b> Mobility scan</a></li>
<li class="chapter" data-level="1.4.9" data-path="shiny-demo.html"><a href="shiny-demo.html#developing-shiny-applications-to-facilitate-precision-agriculture-workflows"><i class="fa fa-check"></i><b>1.4.9</b> Developing Shiny applications to facilitate precision agriculture workflows</a></li>
<li class="chapter" data-level="1.4.10" data-path="shiny-demo.html"><a href="shiny-demo.html#guinterp-a-shiny-gui-to-support-spatial-interpolation"><i class="fa fa-check"></i><b>1.4.10</b> “GUInterp”: a Shiny GUI to support spatial interpolation</a></li>
<li class="chapter" data-level="1.4.11" data-path="shiny-demo.html"><a href="shiny-demo.html#scoring-the-implicit-association-test-has-never-been-easier-dscoreapp"><i class="fa fa-check"></i><b>1.4.11</b> Scoring the Implicit Association Test has never been easier: DscoreApp</a></li>
<li class="chapter" data-level="1.4.12" data-path="shiny-demo.html"><a href="shiny-demo.html#interactive-project-management-tool-using-shiny"><i class="fa fa-check"></i><b>1.4.12</b> Interactive project management tool using shiny</a></li>
<li class="chapter" data-level="1.4.13" data-path="shiny-demo.html"><a href="shiny-demo.html#rtrhexng-hexagon-sticker-app-for-rtrng"><i class="fa fa-check"></i><b>1.4.13</b> rTRhexNG: Hexagon sticker app for rTRNG</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">eRum2020 Program</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regular-talk" class="section level2">
<h2><span class="header-section-number">1.3</span> Regular talk</h2>
<div id="design-patterns-for-big-shiny-apps" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Design Patterns For Big Shiny Apps</h3>
<p><em>Alex Gold, Solutions Engineer, RStudio</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny, R Production</p>
<p><strong>Abstract:</strong></p>
<p>In about 20 minutes on the morning of January 27, 2020, one engineer launched over 500 individual cloud server instances for workshop attendees at RStudio::conf and managed them for the duration of the workshops — all from a Shiny app. The RStudio team manages a variety of production systems using Shiny apps including our workshop infrastructure and access to our sales demo server.</p>
<p>The Shiny apps are robust enough for these mission-critical activities because of an important lesson from web engineering: separation of concerns between front-end user interaction logic and back-end business logic. This design pattern can be implemented in R by creating user interfaces in Shiny and managing interactions with other systems with Plumber APIs and R6 classes.</p>
<p>This pattern allows for even complex Shiny apps to still be understandable and maintainable. Moreover, this pattern of designing and building large Shiny apps is broadly applicable to any app that has substantial interaction with outside systems. Session attendees will gain an understanding of this pattern, which can be useful for many large Shiny apps.</p>
</div>
<div id="using-xgboost-plumber-and-docker-in-production-to-power-a-new-banking-product" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Using XGBoost, Plumber and Docker in production to power a new banking product</h3>
<p><em>André Rivenæs, Data Scientist, PwC</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Production</p>
<p><strong>Abstract:</strong></p>
<p>Buffer is a brand new and innovative banking product by one of the largest retail banks in Norway, Sparebanken Vest, and it is powered by R.</p>
<p>In fact, the product’s decision engine is written entirely in R. We analyze whether a customer should get a loan and how much loan they should be allocated by analyzing large amounts of data from various sources. An essential part is analyzing the customer’s invoices using machine learning (XGBoost).</p>
<p>In this talk, we will cover:</p>
<ul>
<li>How we use ML and Bayesian statistics to estimate the probability of an invoice being repaid.</li>
<li>How we successfully put the decision engine in production, using e.g. Plumber, Docker, CircleCI and Kubernetes.</li>
<li>What we have learned from using R in production at scale.</li>
</ul>
</div>
<div id="astronomical-source-detection-and-background-separation-a-bayesian-nonparametric-approach" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Astronomical source detection and background separation: a Bayesian nonparametric approach</h3>
<p><em>Andrea Sottosanti, University of Padova</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Applications</p>
<p><strong>Abstract:</strong></p>
<p>We propose an innovative approach based on Bayesian nonparametric methods to the signal extraction of astronomical sources in gamma-ray count maps under the presence of a strong background contamination. Our model simultaneously induces clustering on the photons using their spatial information and gives an estimate of the number of sources, while separating them from the irregular signal of the background component that extends over the entire map. From a statistical perspective, the signal of the sources is modeled using a Dirichlet Process mixture, that allows to discover and locate a possible infinite number of clusters, while the background component is completely reconstructed using a new flexible Bayesian nonparametric model based on b-spline basis functions. The resultant can be then thought of as a hierarchical mixture of nonparametric mixtures for flexible clustering of highly contaminated signals. We provide also a Markov chain Monte Carlo algorithm to infer on the posterior distribution of the model parameters, and a suitable post-processing algorithm to quantify the information coming from the detected clusters. Results on different datasets confirm the capacity of the model to discover and locate the sources in the analysed map, to quantify their intensities and to estimate and account for the presence of the background contamination.</p>
</div>
<div id="validation-of-visual-inference-methods-using-deep-learning-in-r" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Validation of visual inference methods using deep learning in R</h3>
<p><em>Anne Helby Petersen, University of Copenhagen</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Visual inference is commonly used to assess whether a statistical model fulfills essential assumptions, fit well or converted properly. An example is the residual plot, which is used for identifying heteroscedasticity issues in normal linear regression models, and most statistics 101 classes train students in telling “healthy” and “problematic” plots apart. However, it is not really clear whether struggling students are actually posed with a solvable task, as the visual inference methods themselves are rarely validated.</p>
<p>We propose a new tool for validating visual inference methods in R. The tool revolves around training convolutional neural networks (using the keras package) to tell apart simulated “healthy” and “problematic” plots. The obtained classification accuracy is then interpreted as the amount of discriminatory power available in the visual inference method and it is used to evaluate it: If the neural network cannot categorize the plots correctly, we doubt that humans will be able to.</p>
<p>We showcase the tool on residual plots, where it is able to discriminate between problematic and healthy plots when there is an appropriate number of observations (e.g. n = 50), but less so when there are only few observations (e.g. n = 10). This implies that our proposed tool is not only able to positively show visual inference validity, but it is also useful for identifying lack thereof.</p>
</div>
<div id="high-dimensional-sampling-and-volume-computation" class="section level3">
<h3><span class="header-section-number">1.3.5</span> High dimensional sampling and volume computation</h3>
<p><em>Apostolos Chalkis, PhD in Computer Science</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Sampling from multivariate distributions is a fundamental problem in statistics that plays important role in modern machine learning and data science. Many important problems such as convex optimization and multivariate integration can be efficiently solved via sampling. This talk presents the CRAN package volesti which offers to R community efficient C++ implementations of state-of-the-art algorithms for sampling and volume computation of convex sets. It scales up to hundred or thousand dimensions, depending the problem, providing the most efficient implementations for sampling and volume computation to date. Thus, volesti allows users to solve problems in dimensions and order of magnitude higher than before. We present the basic functionality of volesti and show how it can be used to provide approximate solutions to intractable problems in combinatorics, financial modeling, bioinformatics and engineering. We stand out two famous applications in finance. We show how volesti can be used to detect financial crises and evaluate portfolios performance in large stock markets with hundreds of assets, by giving real life examples using public data.</p>
</div>
<div id="fake-news-ai-on-the-battle-ground" class="section level3">
<h3><span class="header-section-number">1.3.6</span> Fake News: AI on the battle ground</h3>
<p><em>Ayomide Shodipo, Senior Developer Advocate &amp; Media Developer Expert at Cloudinary</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Life Sciences, R Production, R World</p>
<p><strong>Abstract:</strong></p>
<p>Assumed products have been a longstanding and growing pain for companies around the globe. In addition to impacting company revenue, they damage brand reputation and customer confidence. Companies were asked to build a solution for a global electronics brand that can identify fake products by just taking one picture on a smartphone.</p>
<p>In this session, we will look into the building blocks that make this AI solution work. We’ll find out that there is much more to it than just training a convolutional neural network.</p>
<p>We look at challenges like how to manage and monitor the AI model and how to build and improve the model in a way that fits your DevOps production chain.</p>
<p>Learn how we used Azure Functions, Cosmos DB and Docker to build a solid foundation. See how we used the Azure Machine Learning service to train the models. And find out how we used Azure DevOps to control, build and deploy this state-of-the-art solution.</p>
</div>
<div id="automation-of-file-monitoring-in-a-data-lake-for-large-scale-systems" class="section level3">
<h3><span class="header-section-number">1.3.7</span> Automation of File Monitoring in a Data Lake for Large Scale Systems</h3>
<p><em>Ceyda Sol, INGTECH NL , Data analyst</em></p>
<p><strong>Track(s):</strong> R Applications, R Machine Learning &amp; Models, R World</p>
<p><strong>Abstract:</strong></p>
<p>for Banks in regulatory reporting, the delayed and/or missing delivery of a file may lead to serious problems that can hinder the delivery towards law authorities and as a consequence a fine can be applied to the report providers. In this study, the objective is better monitoring and being able to trigger automated alerts to the targets and system of records (SoRs) and better distribution of the files to the loaders to reduce the waiting time in the loader queue.</p>
</div>
<div id="from-consulting-to-open-source-and-back" class="section level3">
<h3><span class="header-section-number">1.3.8</span> From consulting to open-source and back</h3>
<p><em>Christoph Sax, R-enthusiast, economist <span class="citation">@cynkra</span></em></p>
<p><strong>Track(s):</strong> R World</p>
<p><strong>Abstract:</strong></p>
<p>Open-source development is a great source of satisfaction and fulfillment, but someone has to pay the bills. A straightforward solution is to consult customers and help them to pick the right tools. As a small group of R enthusiasts, we try to align open source development by supporting our clients to accomplish their goals, contributing to the community along the way. It turns out that the benefits work in both ways: In addition to funding, consulting work allows us to test our tools and to improve their usability in a practical setting. At the same time, the involvement in open source development sharpens our analytical skills and serves as a first stop for new customers. Ideally, consulting projects lead to new developments, which in turn lead to new consulting projects.</p>
</div>
<div id="how-to-apply-r-in-a-hospital-environment-on-standard-available-hospital-wide-data" class="section level3">
<h3><span class="header-section-number">1.3.9</span> How to apply R in a hospital environment on standard available hospital-wide data</h3>
<p><em>Deschepper Mieke, University hospital Ghent, staf member Strategic Policy cell, Ph.D.</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Lots of data is registered within hospitals, for financial, clinical and administrative purposes. Today, this data is barely used. Due to not knowing the existence of the data, the possible applications and the skills to execute the analysis, …
In this presentation we show how we can apply R on this data and what the possibilities are using standard available hospital-wide data on a low cost budget.
1. Reporting with R
- using R and markdown as a tool for management reporting
- Using R for data handling (ETL)
- Shiny applications as alternative for dashboarding
2. Using R as a statistical tool
- Performing regression models to gain insight in certain predictor
3. Using R a data science tool
- Using R to perform Machine Learning analysis, e.g. Random Forests
- Using R for the data wrangling and handle the high dimensional data
4. Requirements for all of the above</p>
</div>
<div id="polite-web-etiquette-for-r-users" class="section level3">
<h3><span class="header-section-number">1.3.10</span> {polite}: web etiquette for R users</h3>
<p><em>Dmytro Perepolkin, Lund University</em></p>
<p><strong>Track(s):</strong> R World, R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Data is everywhere, but it does not mean it is freely available. What are best practices and acceptable norms for accessing the data on the web? How does one know when it is OK to scrape the content of a website and how to do it in such a way that it does not create problems for data owner and/or other users? This talk with provide examples of using {polite} package for safe and responsible web scraping. The three pillars of {polite} are seeking permission, taking slowly and never asking twice.</p>
</div>
<div id="hydrological-modelling-and-r" class="section level3">
<h3><span class="header-section-number">1.3.11</span> Hydrological Modelling and R</h3>
<p><em>Emanuele Cordano, www.rendena100.eu</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Eco-hydrological and biophysical models are increasingly used in the contexts of hydrology, ecology, precision agriculture for better management of water resources and climate change impact studies at various scales: local, watershed or regional scale. However, to satisfy the researchers and stakeholders demand, user friendly interfaces are needed. The integration of such models in the powerful software environment of R greatly eases the application, input data preparation, output elaboration and visualization. In this work we present new developments for a R interface (R open-source package <strong>geotopbricks</strong> (<a href="https://CRAN.R-project.org/package=geotopbricks" class="uri">https://CRAN.R-project.org/package=geotopbricks</a>) and related) for the GEOtop hydrological distributed model (www.geotop.org - GNU General Public License v3.0). This package aims to be a link between the work of environmental engineers, who develop hydrological models, and the ones of data and applied scientists, who can extract information from the model results. Applications related to the simulation of water cycle dynamics (model calibration, mapping, data visualization) in some alpine basins and under scenarios of climate change and variability are shown. In particular, we will present an application to predict with the model winter snow conditions, which play a critical role in governing the spatial distribution of fauna in temperate ecosystems.</p>
</div>
<div id="genetonic-enjoy-rna-seq-data-analysis-responsibly" class="section level3">
<h3><span class="header-section-number">1.3.12</span> GeneTonic: enjoy RNA-seq data analysis, responsibly</h3>
<p><em>Federico Marini, Center for Thrombosis and Hemostasis (CTH) &amp; Institute of Medical Biostatistics, Epidemiology and Informatics (IMBEI) - University Medical Center Mainz</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Interpreting the results from RNA-seq transcriptome experiments can be a complex task, where the essential information is distributed among different tabular and list formats - normalized expression values, results from differential expression analysis, and results from functional enrichment analyses.</p>
<p>The identification of relevant functional patterns, as well as their contextualization in the data and results at hand, are not straightforward operations if these pieces of information are not combined together efficiently.</p>
<p>Interactivity can play an essential role in simplifying the way how one accesses and digests RNA-seq data analysis in a more comprehensive way.</p>
<p>I introduce <code>GeneTonic</code> (<a href="https://github.com/federicomarini/GeneTonic" class="uri">https://github.com/federicomarini/GeneTonic</a>), an application developed in Shiny and based on many essential elements of the Bioconductor project, that aims to reduce the barrier to understanding such data better, and to efficiently combine the different components of the analytic workflow.</p>
<p>For example, starting from bird’s eye perspective summaries (with interactive bipartite gene-geneset graphs, or enrichment maps), it is easy to generate a number of visualizations, where drill-down user actions enable further insight and deliver additional information (e.g., gene info boxes, geneset summary, and signature heatmaps).</p>
<p>Complex datasets interpretation can be wrapped up into a single call to the GeneTonic main function, which also supports built-in RMarkdown reporting, to both conclude an exploration session, or also to generate in batch the output of the available functionality, delivering an essential foundation for computational reproducibility.</p>
</div>
<div id="a-simple-and-flexible-inactivitysleep-detection-r-package" class="section level3">
<h3><span class="header-section-number">1.3.13</span> A simple and flexible inactivity/sleep detection R package</h3>
<p><em>Francesca Giorgolo, Kode s.r.l. - Data Scientist</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>With the widespread usage of wearable devices great amount of data became available and new fields of application arised, like health monitoring and activity detection.
Our work focused on inactivity and sleep detection from continuous raw tri-axis accelerometer data, recorded using different accelerometers brands having sampling frequencies below and above 1Hz.
The algorithm implemented is the SPT-window detection algorithm described in literature slighty modified to met the flexibility requirement we imposed ourselves.
The R package developed provides functions to clean data, to identify inactivity/sleep windows and to visualize the results.
The main function has a parameter to specify the measurement unit of the data, a threshold to distinguish low and high activity and also a parameter to handle non-wearing periods, where a non wear period is defined as a period of time where all the accelerometers are equal to zero. Other functions allow to separate overlapped accelerometer signals, i.e. when a device is replaced by another, and to visualize the obtained results.</p>
</div>
<div id="progressr-an-inclusive-unifying-api-for-progress-updates" class="section level3">
<h3><span class="header-section-number">1.3.14</span> progressr: An Inclusive, Unifying API for Progress Updates</h3>
<p><em>Henrik Bengtsson, UCSF, Assoc Prof, CS/Stats, R since 2000</em></p>
<p><strong>Track(s):</strong> R Production, R Applications</p>
<p><strong>Abstract:</strong></p>
<p>The ‘progressr’ package provides a minimal, unifying API for scripts and packages to report progress from anywhere including when using parallel processing to anywhere.</p>
<p>It is designed such that the developer can focus on what to report progress on without having to worry about how to present it. The end user has full control of how, where, and when to render these progress updates. Progress bars from popular progress packages are supported and more can be added.</p>
<p>The ‘progressr’ is inclusive by design. Specifically, no assumptions are made how progress is reported, i.e. it does not have to be a progress bar in the terminal. Progress can also be reported as audio (e.g. unique begin and end sounds with intermediate non-intrusive step sounds), or via a local or online notification system.</p>
<p>Another novelty is that progress updates are controlled and signaled via R’s condition framework. Because of this, there is no need for progress-specific arguments and progress can be reported from nearly everywhere in R, e.g. in classical for and while loops, within map-reduce APIs like the ‘lapply()’ family of functions, ‘purrr’, ‘plyr’, and ‘foreach’. It also works with parallel processing via the ‘future’ framework, e.g. ‘future.apply’, ‘furrr’, and ‘foreach’ with ‘doFuture’. The package is compatible with Shiny applications.</p>
</div>
<div id="varycoef-modeling-spatially-varying-coefficients" class="section level3">
<h3><span class="header-section-number">1.3.15</span> varycoef: Modeling Spatially Varying Coefficients</h3>
<p><em>Jakob Dambon, HSLU &amp; UZH, Switzerland</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>In regression models for spatial data, it is often assumed that the marginal effects of covariates on the response are constant over space. In practice, this assumption might often be questionable. Spatially varying coefficient (SVC) models are commonly used to account for spatial structure within the coefficients.
With the R package varycoef, we provide the frame work to estimate Gaussian process-based SVC models. It is based on maximum likelihood estimation (MLE) and in contrast to existing model-based approaches, our method scales better to data where both the number of spatial points is large and the number of spatially varying covariates is moderately-sized, e.g., above ten.
We compare our methodology to existing methods such as a Bayesian approach using the stochastic partial differential equation (SPDE) link, geographically weighted regression (GWR), and eigenvector spatial filtering (ESF) in both a simulation study and an application where the goal is to predict prices of real estate apartments in Switzerland. The results from both the simulation study and application show that our proposed approach results in increased predictive accuracy and more precise estimates.</p>
</div>
<div id="fastai-in-r-preserving-wildlife-with-computer-vision" class="section level3">
<h3><span class="header-section-number">1.3.16</span> FastAI in R: preserving wildlife with computer vision</h3>
<p><em>Jędrzej Świeżewski, Data Scientist at Appsilon</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>In this presentation, we will discuss using the latest techniques in computer vision as an important part of “AI for Good” efforts, namely, enhancing wildlife preservation. We will present how to make use of the latest technical advancements in an R setup even if they are originally implemented in Python.</p>
<p>A topic rightfully receiving growing attention among Machine Learning researchers and practitioners is how to make good use of the power obtained with the advancement of the tools. One of the avenues in these efforts is assisting wildlife conservation by employing computer vision in making observations of wildlife much more effective. We will discuss several of such efforts during the talk.</p>
<p>One of the very promising frameworks for computer vision developed recently is the Fast.ai wrapper of PyTorch, a Python framework used for computer vision among other things. While it incorporates the latest theoretical developments in the field (such as one cycle policy training) it provides an easy to use framework allowing a much wider audience to benefit from the tools, such as AI for Good initiatives run by people who are not formally trained in Machine Learning.</p>
<p>During the presentation we will show how to make use of a model trained using the Python’s fastai library within an R workflow with the use of the reticulate package. We will focus on use cases concerning classifying species of African wildlife based on images from camera traps.</p>
</div>
<div id="shazam-in-r-audio-analysis-using-the-av-package" class="section level3">
<h3><span class="header-section-number">1.3.17</span> Shazam in R? Audio analysis using the ‘av’ package</h3>
<p><em>Jeroen Ooms, rOpenSci, UC Berkeley</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Shazam [1] is a popular smartphone app that can quickly identify a song or movie from a short audio recording. The basic recognition algorithm [2] effectively combines a few statistical methods to fingerprint an audio fragment based on density peaks in the time-frequency graph (spectrogram). These fingerprints are robust against noise and distortion, and can quickly be compared against a database of known songs. Similar approaches are used in audio signal classification to analyze anything from human speech to whale mating calls.</p>
<p>This talk describes how we would implement something like this in R. We use the new rOpenSci package ‘av’ to read high quality audio/video files (mkv, mp3, aac, etc) into frequency data [3]. The av package makes it easy to cut, convert, and downsample audio, and customize FFT parameters, to prepare audio for analysis in R. We can visually inspect the frequency data by plotting the spectrogram, and finally try to calculate some of the spectrogram fingerprint statistics as described by the Shazam paper.</p>
<p>[1] <a href="https://www.shazam.com" class="uri">https://www.shazam.com</a>
[2] <a href="https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf" class="uri">https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf</a>
[3] <a href="https://docs.ropensci.org/av/articles/articles/spectrograms.html" class="uri">https://docs.ropensci.org/av/articles/articles/spectrograms.html</a></p>
</div>
<div id="powering-turing-e-atlas-with-r" class="section level3">
<h3><span class="header-section-number">1.3.18</span> Powering Turing e-Atlas with R</h3>
<p><em>Layik Hama, Alan Turing Institute</em></p>
<p><strong>Track(s):</strong> R Applications, R Production, R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>Turing e-Atlas is a research project under the Urban Analytics research theme at Alan Turing Institute (ATI). The ATI is UK’s national institute for data science and Artificial Intelligence based at the British Library in London.</p>
<p>The research is a grand vision for which we have been trying to take baby steps under the banner of an e-Atlas. And we believe R is positioned to play a foundation role in any scalable solution to analyse and visualize large scale datasets especially geospatial datasets.</p>
<p>The application presented is built using RStudio’s Plumber package which relies on solid libraries to develop web applications. The front-end is made up of Uber’s various visualization packages using Facebook’s React JavaScript framework.</p>
</div>
<div id="r-at-the-service-of-plastic-surgery-a-web-based-shiny-application-evaluating-facial-attractiveness" class="section level3">
<h3><span class="header-section-number">1.3.19</span> R at the service of plastic surgery: a web-based shiny application evaluating facial attractiveness</h3>
<p><em>Lubomír Štěpánek, Biostatistician, Software Developer, Junior Lecturer, PhD Candidate at First Faculty of Medicine, Charles University &amp; Faculty of Biomedical Engineering, Czech Technical University in Prague (CZ)</em></p>
<p><strong>Track(s):</strong> R Life Sciences, R Machine Learning &amp; Models, R Applications, R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>There are plenty of approaches on how to evaluate human facial attractiveness and how to compare facial images of patients taken before and after they underwent aesthetic facial plastic surgery. However, no one of them is complex enough; thus, any ongoing research or tool enabling to analyze image data and opening the computational power to a broader audience could help to make progress in the field.</p>
<p>In this project, we developed a web-based shiny application providing facial image processing, both manual and automated landmarking, following facial geometry computations and machine-learning models. Besides others, it allows determining geometric facial features significantly increasing facial attractiveness after undergoing the given aesthetic facial plastic surgery.</p>
<p>Facial image data of patients undergoing rhinoplasty, i. e. a correction of nose shape of size, were analyzed using the application, which confirmed the meaning and usability of the application. Machine-learning built-in methods helped to identify which facial predictors increase facial attractiveness.</p>
<p>The shiny web framework enables to develop a complex web interface including HTML, CSS, and javascript front-end and R-based back-end bridging C++ library dlib, which performs image computations. Furthermore, the connected shinyjs package offers a user-server clickable interaction useful for the landmarking.</p>
</div>
<div id="manifoldgstat-an-r-package-for-spatial-statistics-of-manifold-data" class="section level3">
<h3><span class="header-section-number">1.3.20</span> Manifoldgstat: an R package for spatial statistics of manifold data</h3>
<p><em>Luca Torriani, MOX, Department of Mathematics, Politecnico di Milano</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>The statistical analysis of data belonging to Riemannian manifolds is becoming increasingly important in many applications, such as shape analysis or diffusion tensor imaging. In many cases, the available data are georeferenced, making spatial dependence a non-negligible data characteristic. Modeling and accounting for it, typically, is not trivial, because of the non-linear geometry of the manifold. In this contribution, we present the Manifoldgstat R package, which provides a set of fast routines allowing to efficiently analyze sets of spatial Riemannian data, based on state-of-the-art statistical methodologies. The package stems from the need to create an efficient and reproducible environment allowing to run extensive simulation studies and bagging algorithms for spatial prediction of symmetric positive definite matrices. The package implements three main algorithms (Pigoli et al, 2016, Menafoglio et al, 2019, Sartori &amp; Torriani, 2019). The latter two are particularly computationally demanding, as they rely on Random Domain Decompositions of the geographical domain. To substantially improve performances, the package exploits dplyr and Rcpp to integrate R with C++ code, where template factories handle all run-time choices. In this communication, we shall revise the characteristics of the three methodologies considered, and the key-points of their implementation.</p>
</div>
<div id="voronoi-linkage-for-spatially-misaligned-data" class="section level3">
<h3><span class="header-section-number">1.3.21</span> Voronoi Linkage for Spatially Misaligned Data</h3>
<p><em>Luís G. Silva e Silva, Food and Agriculture Organization - FAO - Data Scientist</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny, R World</p>
<p><strong>Abstract:</strong></p>
<p>In studies of elections, voting outcomes are point-referenced at voting stations while socioeconomic covariates are areal data available at census tracts. The misaligned spatial structure of these two data sources makes the regression analysis to identify socioeconomic factors that affect the voting outcomes a challenge. Here we propose a novel approach to link these two sources of spatial data through Voronoi tessellation. Our proposal is creating a Voronoi tessellation with respect to the point-referenced data, with this outset, the spatial points become a set of mutually exclusive polygons named Voronoi cells. The extraction of data from the census tracts is proportional to the intersection area of each census tract polygon and Voronoi cells. Consequently, we use 100% of the available information and preserve the polygons’ autocorrelation structure. When visualised through our Shiny App, the method provides a finer spatial resolution than municipalities and facilitates the identification of spatial structures at a most detailed level. The technique is applied for the 2018 Brazilian presidential election data. The tool provides deep access to Brazilian election results by enabling to create general maps, plots, and tables by states and cities.</p>
</div>
<div id="be-proud-of-your-code-tools-and-patterns-for-making-production-ready-clean-r-code" class="section level3">
<h3><span class="header-section-number">1.3.22</span> Be proud of your code! Tools and patterns for making production-ready, clean R code</h3>
<p><em>Marcin Dubel, Software Engineer at Appsilon Data Science</em></p>
<p><strong>Track(s):</strong> R Production, R World</p>
<p><strong>Abstract:</strong></p>
<p>In this talk you’ll learn the tools and best practices for making clean, reproducible R code in a working environment ready to be shared and productionalised. Save your time for maintenance, adjusting and struggling with packages.</p>
<p>R is a great tool for fast data analysis. It’s simplicity in setup combined with powerful features and community support makes it a perfect language for many subject matter experts e.g. in finance or bioinformatics. Yet often what started as a pet project or proof of concept begins to grow and expand, with additional collaborators working on it. It is then crucial that you have your project organised well, reusable, with an environment set, so that the code works every time and on any machine. Otherwise the solution won’t be used by anyone but you. By following a few patterns and with appropriate tools it won’t be overwhelming or disturbing and will highlight the true value of the code.</p>
<p>Both Appsilon and I personally have taken part in many R projects for which the goal was to clean and organise the code as well as the project structure. We would like to share our experience, best practices and useful tools to share code shamelessly.</p>
<p>During the presentation I will show:
setting up the development environment with <strong>packrat</strong>, <strong>renv</strong> and <strong>docker</strong>,
organising the project structure,
the best practices in writing R code, automated with <strong>linter</strong>,
sharing the code using git,
organising workflow with <strong>drake</strong>,
optimising the Shiny apps and data loading with <strong>plumber</strong> and <strong>database</strong>,
preparing the tests and continuous integration <strong>circle CI</strong>.</p>
</div>
<div id="damirseq-2.0-from-high-dimensional-data-to-cost-effective-reliable-prediction-models" class="section level3">
<h3><span class="header-section-number">1.3.23</span> DaMiRseq 2.0: from high dimensional data to cost-effective reliable prediction models</h3>
<p><em>Mattia Chiesa, Senior data scientist @ Centro Cardiologico Monzino IRCCS</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>High dimensional data generated by modern high-throughput platforms pose a great challenge in selecting a small number of informative variables, for biomarker discovery and classification. Machine learning is an appropriate approach to derive general knowledge from data, identifying highly discriminative features and building accurate prediction models. To this end, we developed the R/Bioconductor package DaMiRseq, which (i) helps researchers to filter and normalize high dimensional datasets, arising from RNA-Seq experiments, by removing noise and bias and (ii) exploits a custom machine learning workflow to select the minimum set of robust informative features able to discriminate classes.
Here, we present the version 2.0 of the DaMiRseq package, an extension that provides a flexible and convenient framework for managing high dimensional data such as omics data, large-scale medical histories, or even social media and financial data. Specifically, DaMiRseq 2.0 implements new functions that allow training and testing of several different classifiers and selection of the most reliable one, in terms of classification performance and number of selected features. The resulting classification model can be further used for any prediction purpose. This framework will give users the ability to build an efficient prediction model that can be easily replicated in further related settings.</p>
</div>
<div id="interpretable-and-accessible-deep-learning-for-omics-data-with-r-and-friends" class="section level3">
<h3><span class="header-section-number">1.3.24</span> Interpretable and accessible Deep Learning for omics data with R and friends</h3>
<p><em>Moritz Hess, Research Associate, Institute of Medical Biometry and Statistics, Faculty of Medicine and Medical Center - University of Freiburg</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Recently, generative Deep Learning approaches were shown to have a huge potential for e.g. retrieving compact, latent representations of high-dimensional omics data such as single-cell RNA-Seq data. However, there are no established methods to infer how these latent representations relate to the observed variables, i.e. the genes.</p>
<p>For extracting interpretable patterns from gene expression data that indicate distinct sub-populations in the data, we here employ log-linear models, applied to the synthetic data and corresponding latent representations, sampled from generative deep models, which were trained with single-cell gene expression data.</p>
<p>While omics data are routinely analyzed in R and powerful toolboxes, tailored to omics data are available, there are no established and truely accessible approaches for Deep Learning applications here.</p>
<p>To close this gap, we here demonstrate how easily customizable Deep Learning frameworks, developed for the Julia programming language, can be leveraged in R, to perform accessible and interpretable Deep Learning with omics data.</p>
</div>
<div id="elevating-shiny-module-with-tidymodules" class="section level3">
<h3><span class="header-section-number">1.3.25</span> Elevating shiny module with {tidymodules}</h3>
<p><em>Mustapha Larbaoui, Novartis, Associate Director, Scientific Computing &amp; Consulting</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>Shiny App developers have warmly welcomed the concept of Shiny modules as a way to simplify the app development process through the introduction of reusable building blocks. Shiny modules are similar in spirit to the concept of functions in R, except each is implemented with paired ui and server codes along with their own namespace. The {tidymodules} R package introduces a novel structure that harmonizes module development based on R6 (<a href="https://r6.r-lib.org/" class="uri">https://r6.r-lib.org/</a>), which is an implementation of encapsulated object-oriented programming for R, thus knowledge of R6 is a prerequisite for using {tidymodules} to develop Shiny modules. Some key features of this package are module encapsulation, reference semantics, central module store and an innovative framework for enabling and facilitating cross module – module communication. It does this through the creation of “ports”, both input and output, where users may pass data and information through pipe operators. Because the connections are strictly specified, the module network may be visualized which shows how data move from one module to another. We feel the {tidymodules} framework will simplify the module development process and will reduce the code complexity through programming concepts like inheritance.</p>
</div>
<div id="apfr-average-power-function-and-bayes-fdr-for-robust-brain-networks-construction" class="section level3">
<h3><span class="header-section-number">1.3.26</span> APFr: Average Power Function and Bayes FDR for Robust Brain Networks Construction</h3>
<p><em>Nicolo’ Margaritella, University of Edinburgh</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Brain functional connectivity is widely investigated in neuroscience. In recent years, the study of brain connectivity has been largely aided by graph theory. The link between time series recorded at multiple locations in the brain and a graph is usually an adjacency matrix. This converts a measure of the connectivity between two time series, typically a correlation coefficient, into a binary choice on whether the two brain locations are functionally connected or not. As a result, the choice of a threshold over the correlation coefficient is key.
In the present work, we propose a multiple testing approach to the choice of a suitable threshold using the Bayes false discovery rate (FDR) and a new estimator of the statistical power called average power function (APF) to balance the two types of statistical error.
We show that the proposed APF behaves well in case of independence of the tests and it is reliable under several dependence conditions. Moreover, we propose a robust method for threshold selection using the 5% and 95% percentiles of APF and FDR bootstrap distributions, respectively, to improve stability.
In addition, we developed a R-package called APFr which performs APF and Bayes FDR robust estimation and provides simple examples to improve usability. The package has attracted more than 3200 downloads since its publication online (June 2019) at <a href="https://CRAN.R-project.org/package=APFr" class="uri">https://CRAN.R-project.org/package=APFr</a>.</p>
</div>
<div id="flexible-meta-analysis-of-generalized-additive-models-with-metagam" class="section level3">
<h3><span class="header-section-number">1.3.27</span> Flexible Meta-Analysis of Generalized Additive Models with metagam</h3>
<p><em>Øystein Sørensen, Associate Professor, University of Oslo</em></p>
<p><strong>Track(s):</strong> R Life Sciences, R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Analyzing biomedical data from multiple studies has great potential in terms of increasing statistical power, enabling detection of associations of smaller magnitude than would be possible analyzing each study separately. Restrictions due to privacy or proprietary data as well as more practical concerns can make it hard to share datasets, such that analyzing all data in a single mega-analysis might not be possible. Meta-analytic methods provide a way to overcome this issue, by combining aggregated quantities like model parameters or risk ratios. However, most meta-analytic tools have focused on parametric statistical models, and software for meta-analyzing semi-parametric models like generalized additive models (GAMs) have not been developed. The metagam package attempts to fill this gap: It provides functionality for removing individual participant data from GAM objects such that they can be analyzed in a common location; furthermore metagam enables meta-analysis of the resulting GAM objects, as well as various tools for visualization and statistical analysis. This talk will illustrate use of the metagam package for analysis of the relationship between sleep quality and brain structure using data from six European brain imaging cohorts.</p>
</div>
<div id="controlled-r-development-with-docker" class="section level3">
<h3><span class="header-section-number">1.3.28</span> Controlled R development with Docker</h3>
<p><em>Peter Schmid, R programmer at Mirai Solutions</em></p>
<p><strong>Track(s):</strong> R Production</p>
<p><strong>Abstract:</strong></p>
<p>When deploying productive solutions, it is essential to have full control over the code-base and environment to ensure reproducibility and stability of the setup. Additionally, guaranteeing full equivalence of the development setup to (alternative) target productive stages is a key aspect of well-managed release pipelines.
In the case of R-based projects, this implies fixing and aligning the version of R as well as package and system dependencies. This is, however, often disregarded due to the absence of out-of-the-box solutions, especially in free open source projects.
With this talk we illustrate an approach to manage a version-stable R development environment that, in the context of containerized solutions based on Docker and the Rocker project, allows to regulate your setup, along with laying out a path for trouble-free deployments and releases. It additionally enables the coexistence of multiple dockerized development flavors, to match various target production environments or projects.</p>
<p>Since the approach does not rely on commercial tools, it is particularly apt for open source projects, as we showcase with the concrete example of OasisUI (<a href="https://github.com/OasisLMF/OasisUI" class="uri">https://github.com/OasisLMF/OasisUI</a>): a web-based Shiny app providing a front-end interface to the Oasis LMF platform, an open source natural catastrophe loss modelling framework, freely available at <a href="https://github.com/OasisLMF" class="uri">https://github.com/OasisLMF</a>.</p>
</div>
<div id="correlaidx---building-r-focused-communities-for-social-good-on-the-local-level" class="section level3">
<h3><span class="header-section-number">1.3.29</span> CorrelAidX - Building R-focused Communities for Social Good on the Local Level</h3>
<p><em>Regina Siegers, CorrelAidX Coordination</em></p>
<p><strong>Track(s):</strong> R World</p>
<p><strong>Abstract:</strong></p>
<p>Data scientists with their valuable skills have enormous potential to contribute to the social good. This is also true for the R community - and R users seem to be especially motivated to use their skills for the social good, as the overwhelmingly positive reception of Julien Cornebise’s keynote “AI for Good” at useR2019 (Cornebise 2019) has shown. However, specific strategies for putting the abstract goal “use data science for the social good” into practice are often missing, especially in volunteering contexts like the R community, where resources are often limited.</p>
<p>In our talk, we present formats that we have implemented on the local level to build R-focused, data-for-good communities across Europe. Originating from the German data4good network CorrelAid with its over 1600 members, we have established 9 local CorrelAidX groups in Germany, the Netherlands, and France.</p>
<p>The specific formats build on a three-pillared concept of community building, namely group-bonding, social entrepreneurship and outreach. We present multiple examples that illustrate how our local chapters operate to put data science for good into practice - using the formats of data dialogues, local data4good projects, and CorrelAidX workshops. Lastly, we also outline possibilities to implement such formats in cooperation between CorrelAidX chapters and R community groups such as R user groups or RLadies chapters.</p>
</div>
<div id="interactive-visualization-of-complex-texts" class="section level3">
<h3><span class="header-section-number">1.3.30</span> Interactive visualization of complex texts</h3>
<p><em>Renate Delucchi Danhier, Post-Doc, TU Dortmund</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>Hundreds of speakers may describe the same circumstance - e.g. explaining a fixed route to a goal - without producing two identical texts. The enormous variability of language and the complexity involved in encoding meaning poses a real difficulty for linguists analyzing text databases. In order to aid linguists in identifying patterns to perform comparative research, we developed an interactive shiny app that enables quantitative analysis of text corpora without oversimplifying the structure of language. Route directions are an example of complex texts, in which speakers take cognitive decisions such as segmenting the route, selecting landmarks and organizing spatial concepts into sentences. In the data visualization, symbols and colors representing linguistic concepts are plotted into coordinates that relate the information to fixed points along the route. Six interconnected layers of meaning represent the multi-layered form-to-meaning mapping characteristic of language. The shiny app allows to select and deselect information on these different layers, offering a holistic linguistic analysis way beyond the complexity attempted within traditional linguistics. The result is a kind of visual language in itself that deconstructs the interconnected layers of meaning found in natural language.</p>
</div>
<div id="bnpmix-an-new-package-to-estimate-bayesian-nonparametric-mixtures" class="section level3">
<h3><span class="header-section-number">1.3.31</span> BNPmix: an new package to estimate Bayesian nonparametric mixtures</h3>
<p><em>Riccardo Corradin, Università degli Studi Milano Bicocca</em></p>
<p><strong>Track(s):</strong> NA</p>
<p><strong>Abstract:</strong></p>
<p>Bayesian nonparametric mixtures are flexible models for density estimation and clustering analysis, where the mixture is assumed composed by an infinite number of components. Different strategies were proposed in the last two decades to deal with the estimation of these models, most resorting to Markov chain Monte Carlo (MCMC) methods. Along with the study of a new approach to estimate infinite mixtures, we implemented an efficient R package, named BNPmix. The package include three different MCMC strategies to deal with nonparametric mixtures, and allows for flexible estimation of the models by choosing the algorithm, and by tuning model/algorithm-specific parameters. Routines for post-processing the results are also available. In order to face the computational complexity of the problem, the main functions are written in C++ and interfaced with R through the Rcpp and RcppArmadillo packages. The BNPmix packages is integrated in the R environment for Bayesian analysis and graphical visualization.</p>
</div>
<div id="connector-a-computational-approach-to-study-intratumor-heterogeneity." class="section level3">
<h3><span class="header-section-number">1.3.32</span> CONNECTOR: a computational approach to study intratumor heterogeneity.</h3>
<p><em>Simone Pernice, Ph.D student at Department of Computer Science of the University of Turin</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Literature is characterized by a broad class of mathematical models which can be used for fitting cancer growth time series, but with no a global consensus or biological evidence that can drive the choice of the correct model. The conventional perception is that the mechanistic models enable the biological understanding of the systems under study. However, there is no way that these models can capture the variability characterizing the cancer progression, especially because of the irregularity and sparsity of the available data.
For this reason, we propose CONNECTOR, an R package built on the model-based approach for clustering functional data. Such method is based on the clustering and fitting of the data through a combination of cubic natural splines basis with coefficients treated as random variables. Our approach is particularly effective when the observations are sparse and irregularly spaced, as growth curves usually are. CONNECTOR provides a tool set to easily guide through the parameters selection, i.e., (i) the dimension of the spline basis, (ii) the dimension of the mean space and (iii) the number of clusters to fit, to be properly chosen before fitting. The effectiveness of CONNECTOR is evaluated on growth curves of Patient Derived Xenografts (PDXs) of ovarian cancer. Genomic analyses of PDXs allowed correlating fitted and clustered PDX growth curves to cell population distribution.</p>
</div>
<div id="gwqs-an-r-package-for-linear-and-generalized-weighted-quantile-sum-wqs-regression" class="section level3">
<h3><span class="header-section-number">1.3.33</span> gWQS: An R Package for Linear and Generalized Weighted Quantile Sum (WQS) Regression</h3>
<p><em>Stefano Renzetti, PhD Student at Università degli Studi di Milano</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Weighted Quantile Sum (WQS) regression is a statistical model for multivariate regression in high-dimensional datasets commonly encountered in environmental exposures. The model constructs a weighted index estimating the mixture effect associated with all predictor variables on an outcome. The package gWQS extends WQS regression to applications with continuous, categorical and count outcomes. We provide four examples to illustrate the usage of the package.</p>
</div>
<div id="rlinkedcharts-a-novel-approach-for-simple-but-powerful-interactive-data-analysis" class="section level3">
<h3><span class="header-section-number">1.3.34</span> R/LinkedCharts: A novel approach for simple but powerful interactive data analysis</h3>
<p><em>Svetlana Ovchinnikova, Doctoral student, Zentrum für Molekulare Biologie der Universität Heidelberg</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>In exploratory data analysis, one usually jumps back and forth between visualizations that provide overview of the whole data and others that dive into details. In data quality assessment, for example, it might be very helpful to have one chart showing a summary statistic for all samples, and clicking on one of the data points would display details on this sample in a second plot. Setting up such interactively linked charts is usually cumbersome and time-consuming to use them in ad hoc analysis. We present R/LinkedChart, a framework that renders this tasks radically simple: Producing linked charts is as quickly done as is producing conventional static plots in R, requiring a data scientist to write only very few lines of simple R code to obtain complex and general visualization. We expect that the convinience of our new tool will enable data scientists and bioinformaticians to perform much deeper and more thorough EDA with much less effort. Furthermore, R/LinkedChart apps, typically first written as quick-and-dirty hacks, can also be polished to provide interactive data access in publication quality, thus contributing to open science.</p>
</div>
<div id="transparent-journalism-through-the-power-of-r" class="section level3">
<h3><span class="header-section-number">1.3.35</span> Transparent Journalism Through the Power of R</h3>
<p><em>Tatjana Kecojevic, SisterAnalyst.org; founder and director</em></p>
<p><strong>Track(s):</strong> R World</p>
<p><strong>Abstract:</strong></p>
<p>This study examines the often-tricky process of delivering data literacy programmes to professionals with most to gain from a deeper understanding of data analysis. As such, the author discusses the process of building and delivering training strategies to journalists in regions where press freedom is constrained by numerous factors, not least of all institutionalised corruption.</p>
<p>Reporting stories that are supplemented with transparent procedural systems are less likely to be contradicted and challenged by vested interest actors. Journalists are able to present findings supported by charts and info graphics, but these are open to translation. Therefore, most importantly, the data and code of the applied analytical methodology should also be available for scrutiny and is less likely to be subverted or prohibited.</p>
<p>As part of creating an accessible programme geared to acquiring skills necessary for data journalism, the author takes a step-by-step approach to discussing the actualities of building online platforms for training purposes. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods in a transparent and reproducible way is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand.</p>
<p>The resulting ‘R toolbox for journalists’ is an accessible open-source resource. It can also be adapted to accommodate the need to provide a deeper understanding of the potential for data proficiency to other professions.</p>
<p>The accessibility of R allows for users to build support communities, which in the case of journalists is essential for information gathering. Establishing and implementing transparent channels of communication is the key to scrupulous journalism and is why R is so applicable to this objective.</p>
</div>
<div id="deduplicating-real-estate-ads-using-naive-bayes-record-linkage" class="section level3">
<h3><span class="header-section-number">1.3.36</span> Deduplicating real estate ads using Naive Bayes record linkage</h3>
<p><em>Thomas Maier, Datahouse AG</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>We demonstrate in this talk, how we used a containerized R and PostgreSQL data pipeline to deduplicate 60 million real estate ads from Germany and Switzerland using a multi-step Naive Bayes record linkage approach. Real estate platforms publish millions of rental flat and condominium ads yearly. A given region or country of interest is normally covered by various competing platforms, leading to multiple published ads for a single real world object. Because quantifying and modeling the real estate market requires unbiased input data, our aim was to deduplicate real estate ads using Naive Bayes record linkage. We used commercially available German and Swiss real estate ad data from 2012 to 2019 consisting of approximately 60 million individual records. After multiple data cleaning and preparation steps we employed a Naive Bayes weighting of 12-14 variables to calculate similarity scores between ads and determined a linkage threshold based on expert judgment. The deduplication pipeline consisted of three steps: linking ads based on identity comparisons, linking similar ads within small regional areas (municipalities) and linking similar ads within large regional areas (cantons, states). The pipeline was deployed as a containerized setup with in-memory calculations in R and out-of-memory calculations and data storage in PostgreSQL. Deduplication linked the around 60 million ads to around 14 million object groups (Germany: 10 millions, Switzerland: 4 millions). The distribution of similarity scores showed high separation power and the resulting object groups displayed high homogeneity in geographic location and price distribution. Furthermore, yearly results corresponded well with published relocation rates. Using Naive Bayes record linkage to deduplicate real estate ads resulted in a sensible grouping of ads into object groups (rental flats, condominiums). We were able to combine similarities across different variables into a single similarity score. An advantage of the Naive Bayes approach is the high interpretability of the influence of individual variables. However, by manually determining the linkage threshold our results are heavily influenced by possible expert biases. The containerized R and PostgreSQL setup proved it’s portability and scaling capabilities. The same approach could easily be transferred to other domains requiring deduplication of multivariate data sets.</p>
</div>
<div id="global-poverty-monitoring-at-scale-using-r" class="section level3">
<h3><span class="header-section-number">1.3.37</span> Global Poverty Monitoring at scale using R</h3>
<p><em>Tony Fujs, Data Scientist, World Bank</em></p>
<p><strong>Track(s):</strong> R Production, R Applications</p>
<p><strong>Abstract:</strong></p>
<p>The World Bank has been trying to assess and monitor the extent of extreme poverty in the world since 1979, and is currently responsible for reporting on the first Sustainable Development Goal, to end extreme poverty by 2030.
The World Bank is using R to modernize its poverty data infrastructure, streamline the production of poverty statistics, and make this production process more transparent and less error prone.</p>
<p>In this presentation, we will describe:
• How we leveraged R to address some of the technical and methodological challenges of producing Global Poverty estimates, and streamline the production of poverty statistics
• How we integrated best software development practices in the creation of our R packages, and worked closely with our IT team to deploy our R packages in production
• How we are promoting transparency by publishing our tools as open-source, and fostering closer collaboration with the research and open-source community
• We will also share some of the challenges encountered along the way</p>
</div>
<div id="mixed-interactive-debugging-of-r-and-native-code-with-fastr-and-vistual-studio-code" class="section level3">
<h3><span class="header-section-number">1.3.38</span> Mixed interactive debugging of R and native code with FastR and Vistual Studio Code</h3>
<p><em>Zbyněk Šlajchrt, Oracle Labs, Ph.D.</em></p>
<p><strong>Track(s):</strong> R World</p>
<p><strong>Abstract:</strong></p>
<p>Interactive debuggers are one of the most useful tools to aid software development. The two most used approaches for interactive debugging in the R ecosystem, the built-in debugger and R Studio, do not support interactive debugging of both R and C code of R packages at the same time and in one tool. FastR is an open source alternative R implementation that, apart from being compatible with GNU-R, puts emphasis on the performance of R execution and tooling support. FastR is part of the multilingual virtual machine (GraalVM) that, among other things, provides language agnostic support for interactive debugging. One of the other projects built on top of GraalVM is a C/C++ interpreter. FastR can be configured to run the native code of selected R packages using this C/C++ interpreter, which should yield the same behavior, but since both languages are now running in one system, it opens up many exciting possibilities including seamless cross-language debugging. In the talk, we will demonstrate how to configure Visual Studio Code and FastR for cross-language interactive debugging and how to debug a sample R package with native code.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="poster.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="shiny-demo.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
