<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.2 Poster | eRum2020 Program</title>
  <meta name="description" content="1.2 Poster | eRum2020 Program" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="1.2 Poster | eRum2020 Program" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.2 Poster | eRum2020 Program" />
  
  
  

<meta name="author" content="eRum2020 organizing Committee" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lightning-talk.html"/>
<link rel="next" href="regular-talk.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> eRum2020 Program Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="lightning-talk.html"><a href="lightning-talk.html"><i class="fa fa-check"></i><b>1.1</b> Lightning talk</a><ul>
<li class="chapter" data-level="1.1.1" data-path="lightning-talk.html"><a href="lightning-talk.html#an-enriched-disease-risk-assessment-model-based-on-historical-blood-donors-records"><i class="fa fa-check"></i><b>1.1.1</b> An enriched disease risk assessment model based on historical blood donors records</a></li>
<li class="chapter" data-level="1.1.2" data-path="lightning-talk.html"><a href="lightning-talk.html#next-generation-supply-chain-planning-with-r-a-case-study"><i class="fa fa-check"></i><b>1.1.2</b> Next Generation Supply Chain Planning With R: A Case Study</a></li>
<li class="chapter" data-level="1.1.3" data-path="lightning-talk.html"><a href="lightning-talk.html#rdwd-r-interface-to-german-weather-service-data"><i class="fa fa-check"></i><b>1.1.3</b> rdwd: R interface to German Weather Service data</a></li>
<li class="chapter" data-level="1.1.4" data-path="lightning-talk.html"><a href="lightning-talk.html#tv-show-data-frames-in-the-browser"><i class="fa fa-check"></i><b>1.1.4</b> tv: Show Data Frames in the Browser</a></li>
<li class="chapter" data-level="1.1.5" data-path="lightning-talk.html"><a href="lightning-talk.html#predicting-the-euro-2020-results-using-tournament-rank-probabilities-scores-from-the-soccer-package"><i class="fa fa-check"></i><b>1.1.5</b> Predicting the Euro 2020 results using tournament rank probabilities scores from the socceR package</a></li>
<li class="chapter" data-level="1.1.6" data-path="lightning-talk.html"><a href="lightning-talk.html#r-in-medical-research-users-and-stakeholders"><i class="fa fa-check"></i><b>1.1.6</b> R in Medical Research: UseRs and StakeholdeRs</a></li>
<li class="chapter" data-level="1.1.7" data-path="lightning-talk.html"><a href="lightning-talk.html#ultra-fast-penalized-regressions-with-r-package-bigstatsr"><i class="fa fa-check"></i><b>1.1.7</b> Ultra fast penalized regressions with R package {bigstatsr}</a></li>
<li class="chapter" data-level="1.1.8" data-path="lightning-talk.html"><a href="lightning-talk.html#supporting-twitter-analytics-application-with-graph-databases-and-the-arangodb-package"><i class="fa fa-check"></i><b>1.1.8</b> Supporting Twitter analytics application with graph-databases and the aRangodb package</a></li>
<li class="chapter" data-level="1.1.9" data-path="lightning-talk.html"><a href="lightning-talk.html#deep-learning-and-time-series-approaches-for-improvement-of-vehicle-distribution-process"><i class="fa fa-check"></i><b>1.1.9</b> Deep learning and time series approaches for improvement of vehicle distribution process</a></li>
<li class="chapter" data-level="1.1.10" data-path="lightning-talk.html"><a href="lightning-talk.html#what-are-the-potato-eaters-eating"><i class="fa fa-check"></i><b>1.1.10</b> What are the potato eaters eating</a></li>
<li class="chapter" data-level="1.1.11" data-path="lightning-talk.html"><a href="lightning-talk.html#dm-working-with-relational-data-models-in-r"><i class="fa fa-check"></i><b>1.1.11</b> dm: working with relational data models in R</a></li>
<li class="chapter" data-level="1.1.12" data-path="lightning-talk.html"><a href="lightning-talk.html#explaining-black-box-models-with-xspliner-to-make-deliberate-business-decisions"><i class="fa fa-check"></i><b>1.1.12</b> Explaining black-box models with xspliner to make deliberate business decisions</a></li>
<li class="chapter" data-level="1.1.13" data-path="lightning-talk.html"><a href="lightning-talk.html#using-open-access-data-to-derive-genome-composition-of-emerging-viruses"><i class="fa fa-check"></i><b>1.1.13</b> Using open-access data to derive genome composition of emerging viruses</a></li>
<li class="chapter" data-level="1.1.14" data-path="lightning-talk.html"><a href="lightning-talk.html#a-principal-component-analysis-based-method-to-detect-biomarker-captation-from-vibrational-spectra"><i class="fa fa-check"></i><b>1.1.14</b> A principal component analysis based method to detect biomarker captation from vibrational spectra</a></li>
<li class="chapter" data-level="1.1.15" data-path="lightning-talk.html"><a href="lightning-talk.html#emojis-show-your-emotions"><i class="fa fa-check"></i><b>1.1.15</b> Emojis; show your emotions! 😀</a></li>
<li class="chapter" data-level="1.1.16" data-path="lightning-talk.html"><a href="lightning-talk.html#an-innovative-way-to-support-your-sales-force"><i class="fa fa-check"></i><b>1.1.16</b> An innovative way to support your sales force</a></li>
<li class="chapter" data-level="1.1.17" data-path="lightning-talk.html"><a href="lightning-talk.html#ptmixed-an-r-package-for-flexible-modelling-of-longitudinal-overdispersed-count-data"><i class="fa fa-check"></i><b>1.1.17</b> ptmixed: an R package for flexible modelling of longitudinal overdispersed count data</a></li>
<li class="chapter" data-level="1.1.18" data-path="lightning-talk.html"><a href="lightning-talk.html#one-way-non-normal-anova-in-reliability-analysis-using-with-doex"><i class="fa fa-check"></i><b>1.1.18</b> One-way non-normal ANOVA in reliability analysis using with doex</a></li>
<li class="chapter" data-level="1.1.19" data-path="lightning-talk.html"><a href="lightning-talk.html#towards-more-structured-data-quality-assessment-in-the-process-mining-field-the-daqapo-package"><i class="fa fa-check"></i><b>1.1.19</b> Towards more structured data quality assessment in the process mining field: the DaQAPO package</a></li>
<li class="chapter" data-level="1.1.20" data-path="lightning-talk.html"><a href="lightning-talk.html#analyzing-preference-data-with-the-bayesmallows-package"><i class="fa fa-check"></i><b>1.1.20</b> Analyzing Preference Data with the BayesMallows Package</a></li>
<li class="chapter" data-level="1.1.21" data-path="lightning-talk.html"><a href="lightning-talk.html#supporting-r-in-the-binder-community"><i class="fa fa-check"></i><b>1.1.21</b> Supporting R in the Binder Community</a></li>
<li class="chapter" data-level="1.1.22" data-path="lightning-talk.html"><a href="lightning-talk.html#flexible-deep-learning-via-the-juliaconnector"><i class="fa fa-check"></i><b>1.1.22</b> Flexible deep learning via the JuliaConnectoR</a></li>
<li class="chapter" data-level="1.1.23" data-path="lightning-talk.html"><a href="lightning-talk.html#time-series-missing-data-visualizations"><i class="fa fa-check"></i><b>1.1.23</b> Time Series Missing Data Visualizations</a></li>
<li class="chapter" data-level="1.1.24" data-path="lightning-talk.html"><a href="lightning-talk.html#effectclass-an-r-package-to-interpret-effects-and-visualise-uncertainty"><i class="fa fa-check"></i><b>1.1.24</b> effectclass: an R package to interpret effects and visualise uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="poster.html"><a href="poster.html"><i class="fa fa-check"></i><b>1.2</b> Poster</a><ul>
<li class="chapter" data-level="1.2.1" data-path="poster.html"><a href="poster.html#modified-likelihood-ratio-model-for-handwriting-recognition-in-forensic-science."><i class="fa fa-check"></i><b>1.2.1</b> Modified likelihood ratio model for handwriting recognition in forensic science.</a></li>
<li class="chapter" data-level="1.2.2" data-path="poster.html"><a href="poster.html#the-r-package-flexreg-regression-mixture-models-for-bounded-responses"><i class="fa fa-check"></i><b>1.2.2</b> The R-package ‘FlexReg’: regression mixture models for bounded responses</a></li>
<li class="chapter" data-level="1.2.3" data-path="poster.html"><a href="poster.html#using-r-for-analysis-of-microscopic-images-of-streptomyces-growth-and-chromosome-distribution"><i class="fa fa-check"></i><b>1.2.3</b> Using R for analysis of microscopic images of Streptomyces growth and chromosome distribution</a></li>
<li class="chapter" data-level="1.2.4" data-path="poster.html"><a href="poster.html#a-flexible-dashboard-for-monitoring-platform-trials"><i class="fa fa-check"></i><b>1.2.4</b> A flexible dashboard for monitoring platform trials</a></li>
<li class="chapter" data-level="1.2.5" data-path="poster.html"><a href="poster.html#prda-package-enhancing-statistical-inference-via-prospective-and-retrospective-design-analysis."><i class="fa fa-check"></i><b>1.2.5</b> PRDA package: Enhancing Statistical Inference via Prospective and Retrospective Design Analysis.</a></li>
<li class="chapter" data-level="1.2.6" data-path="poster.html"><a href="poster.html#corpfinder--a-new-application-to-identify-large-corporate-risks"><i class="fa fa-check"></i><b>1.2.6</b> CorpFinder- a new application to identify Large Corporate Risks</a></li>
<li class="chapter" data-level="1.2.7" data-path="poster.html"><a href="poster.html#automate-flexdashboard-with-github"><i class="fa fa-check"></i><b>1.2.7</b> Automate flexdashboard with GitHub</a></li>
<li class="chapter" data-level="1.2.8" data-path="poster.html"><a href="poster.html#it-process-optimization-in-data-lake-via-r"><i class="fa fa-check"></i><b>1.2.8</b> IT Process Optimization in Data Lake via R</a></li>
<li class="chapter" data-level="1.2.9" data-path="poster.html"><a href="poster.html#level-up-your-tables-with-tablehtml-in-r"><i class="fa fa-check"></i><b>1.2.9</b> Level up your tables with tableHTML in R</a></li>
<li class="chapter" data-level="1.2.10" data-path="poster.html"><a href="poster.html#guidance-for-teaching-r-to-non-programmers"><i class="fa fa-check"></i><b>1.2.10</b> Guidance for teaching R to non-programmers</a></li>
<li class="chapter" data-level="1.2.11" data-path="poster.html"><a href="poster.html#transparent-presentation-of-uncertain-lotteries-using-deals"><i class="fa fa-check"></i><b>1.2.11</b> Transparent presentation of uncertain lotteries using {deals}</a></li>
<li class="chapter" data-level="1.2.12" data-path="poster.html"><a href="poster.html#newwave-a-scalable-r-package-for-the-dimensionality-reduction-of-single-cell-rna-seq"><i class="fa fa-check"></i><b>1.2.12</b> NewWave: a scalable R package for the dimensionality reduction of single-cell RNA-seq</a></li>
<li class="chapter" data-level="1.2.13" data-path="poster.html"><a href="poster.html#orf-ordered-random-forests"><i class="fa fa-check"></i><b>1.2.13</b> orf: Ordered Random Forests</a></li>
<li class="chapter" data-level="1.2.14" data-path="poster.html"><a href="poster.html#biomarker-discovery-by-the-leveraging-of-omic-and-clinical-data-using-biomarkerbox."><i class="fa fa-check"></i><b>1.2.14</b> Biomarker discovery by the leveraging of omic and clinical data using biomarkeRbox.</a></li>
<li class="chapter" data-level="1.2.15" data-path="poster.html"><a href="poster.html#an-package-for-bayesian-analysis-of-structured-time-series-models-with"><i class="fa fa-check"></i><b>1.2.15</b> An  package for Bayesian analysis of structured time series models with </a></li>
<li class="chapter" data-level="1.2.16" data-path="poster.html"><a href="poster.html#bridging-the-gap-between-r-and-computer-vision"><i class="fa fa-check"></i><b>1.2.16</b> Bridging the gap between R and computer vision</a></li>
<li class="chapter" data-level="1.2.17" data-path="poster.html"><a href="poster.html#how-r-hub-can-help-you-develop-and-maintain-your-r-packages"><i class="fa fa-check"></i><b>1.2.17</b> How R-hub can help you develop and maintain your R packages</a></li>
<li class="chapter" data-level="1.2.18" data-path="poster.html"><a href="poster.html#power-supply-health-status-monitoring-dashboard"><i class="fa fa-check"></i><b>1.2.18</b> Power Supply health status monitoring dashboard</a></li>
<li class="chapter" data-level="1.2.19" data-path="poster.html"><a href="poster.html#first-year-ict-students-dropout-predicting-with-r-models"><i class="fa fa-check"></i><b>1.2.19</b> First-year ICT students dropout predicting with R models</a></li>
<li class="chapter" data-level="1.2.20" data-path="poster.html"><a href="poster.html#benchmark-percentage-disjoint-data-splitting-in-cross-validation-for-assessing-the-skill-of-machine"><i class="fa fa-check"></i><b>1.2.20</b> Benchmark Percentage Disjoint Data Splitting in Cross Validation for Assessing the Skill of Machine</a></li>
<li class="chapter" data-level="1.2.21" data-path="poster.html"><a href="poster.html#integrating-professional-software-engineering-practices-in-medical-research-software"><i class="fa fa-check"></i><b>1.2.21</b> Integrating professional software engineering practices in medical research software</a></li>
<li class="chapter" data-level="1.2.22" data-path="poster.html"><a href="poster.html#a-three-parameter-gompertz-lindley-distribution-its-properties-and-applications"><i class="fa fa-check"></i><b>1.2.22</b> A Three-Parameter Gompertz-Lindley Distribution: Its Properties And Applications</a></li>
<li class="chapter" data-level="1.2.23" data-path="poster.html"><a href="poster.html#partitional-clustering-with-extensions"><i class="fa fa-check"></i><b>1.2.23</b> Partitional clustering with extensions</a></li>
<li class="chapter" data-level="1.2.24" data-path="poster.html"><a href="poster.html#dealing-with-changing-administrative-boundaries-the-case-of-swiss-municipalities"><i class="fa fa-check"></i><b>1.2.24</b> Dealing with changing administrative boundaries: The case of Swiss municipalities</a></li>
<li class="chapter" data-level="1.2.25" data-path="poster.html"><a href="poster.html#baddea-an-r-package-for-measuring-firms-efficiency-adjusted-by-undesirable-outputs"><i class="fa fa-check"></i><b>1.2.25</b> badDEA: An R package for measuring firms’ efficiency adjusted by undesirable outputs</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="regular-talk.html"><a href="regular-talk.html"><i class="fa fa-check"></i><b>1.3</b> Regular talk</a><ul>
<li class="chapter" data-level="1.3.1" data-path="regular-talk.html"><a href="regular-talk.html#design-patterns-for-big-shiny-apps"><i class="fa fa-check"></i><b>1.3.1</b> Design Patterns For Big Shiny Apps</a></li>
<li class="chapter" data-level="1.3.2" data-path="regular-talk.html"><a href="regular-talk.html#using-xgboost-plumber-and-docker-in-production-to-power-a-new-banking-product"><i class="fa fa-check"></i><b>1.3.2</b> Using XGBoost, Plumber and Docker in production to power a new banking product</a></li>
<li class="chapter" data-level="1.3.3" data-path="regular-talk.html"><a href="regular-talk.html#astronomical-source-detection-and-background-separation-a-bayesian-nonparametric-approach"><i class="fa fa-check"></i><b>1.3.3</b> Astronomical source detection and background separation: a Bayesian nonparametric approach</a></li>
<li class="chapter" data-level="1.3.4" data-path="regular-talk.html"><a href="regular-talk.html#validation-of-visual-inference-methods-using-deep-learning-in-r"><i class="fa fa-check"></i><b>1.3.4</b> Validation of visual inference methods using deep learning in R</a></li>
<li class="chapter" data-level="1.3.5" data-path="regular-talk.html"><a href="regular-talk.html#high-dimensional-sampling-and-volume-computation"><i class="fa fa-check"></i><b>1.3.5</b> High dimensional sampling and volume computation</a></li>
<li class="chapter" data-level="1.3.6" data-path="regular-talk.html"><a href="regular-talk.html#fake-news-ai-on-the-battle-ground"><i class="fa fa-check"></i><b>1.3.6</b> Fake News: AI on the battle ground</a></li>
<li class="chapter" data-level="1.3.7" data-path="regular-talk.html"><a href="regular-talk.html#automation-of-file-monitoring-in-a-data-lake-for-large-scale-systems"><i class="fa fa-check"></i><b>1.3.7</b> Automation of File Monitoring in a Data Lake for Large Scale Systems</a></li>
<li class="chapter" data-level="1.3.8" data-path="regular-talk.html"><a href="regular-talk.html#from-consulting-to-open-source-and-back"><i class="fa fa-check"></i><b>1.3.8</b> From consulting to open-source and back</a></li>
<li class="chapter" data-level="1.3.9" data-path="regular-talk.html"><a href="regular-talk.html#how-to-apply-r-in-a-hospital-environment-on-standard-available-hospital-wide-data"><i class="fa fa-check"></i><b>1.3.9</b> How to apply R in a hospital environment on standard available hospital-wide data</a></li>
<li class="chapter" data-level="1.3.10" data-path="regular-talk.html"><a href="regular-talk.html#polite-web-etiquette-for-r-users"><i class="fa fa-check"></i><b>1.3.10</b> {polite}: web etiquette for R users</a></li>
<li class="chapter" data-level="1.3.11" data-path="regular-talk.html"><a href="regular-talk.html#hydrological-modelling-and-r"><i class="fa fa-check"></i><b>1.3.11</b> Hydrological Modelling and R</a></li>
<li class="chapter" data-level="1.3.12" data-path="regular-talk.html"><a href="regular-talk.html#genetonic-enjoy-rna-seq-data-analysis-responsibly"><i class="fa fa-check"></i><b>1.3.12</b> GeneTonic: enjoy RNA-seq data analysis, responsibly</a></li>
<li class="chapter" data-level="1.3.13" data-path="regular-talk.html"><a href="regular-talk.html#a-simple-and-flexible-inactivitysleep-detection-r-package"><i class="fa fa-check"></i><b>1.3.13</b> A simple and flexible inactivity/sleep detection R package</a></li>
<li class="chapter" data-level="1.3.14" data-path="regular-talk.html"><a href="regular-talk.html#progressr-an-inclusive-unifying-api-for-progress-updates"><i class="fa fa-check"></i><b>1.3.14</b> progressr: An Inclusive, Unifying API for Progress Updates</a></li>
<li class="chapter" data-level="1.3.15" data-path="regular-talk.html"><a href="regular-talk.html#varycoef-modeling-spatially-varying-coefficients"><i class="fa fa-check"></i><b>1.3.15</b> varycoef: Modeling Spatially Varying Coefficients</a></li>
<li class="chapter" data-level="1.3.16" data-path="regular-talk.html"><a href="regular-talk.html#fastai-in-r-preserving-wildlife-with-computer-vision"><i class="fa fa-check"></i><b>1.3.16</b> FastAI in R: preserving wildlife with computer vision</a></li>
<li class="chapter" data-level="1.3.17" data-path="regular-talk.html"><a href="regular-talk.html#shazam-in-r-audio-analysis-using-the-av-package"><i class="fa fa-check"></i><b>1.3.17</b> Shazam in R? Audio analysis using the ‘av’ package</a></li>
<li class="chapter" data-level="1.3.18" data-path="regular-talk.html"><a href="regular-talk.html#powering-turing-e-atlas-with-r"><i class="fa fa-check"></i><b>1.3.18</b> Powering Turing e-Atlas with R</a></li>
<li class="chapter" data-level="1.3.19" data-path="regular-talk.html"><a href="regular-talk.html#r-at-the-service-of-plastic-surgery-a-web-based-shiny-application-evaluating-facial-attractiveness"><i class="fa fa-check"></i><b>1.3.19</b> R at the service of plastic surgery: a web-based shiny application evaluating facial attractiveness</a></li>
<li class="chapter" data-level="1.3.20" data-path="regular-talk.html"><a href="regular-talk.html#manifoldgstat-an-r-package-for-spatial-statistics-of-manifold-data"><i class="fa fa-check"></i><b>1.3.20</b> Manifoldgstat: an R package for spatial statistics of manifold data</a></li>
<li class="chapter" data-level="1.3.21" data-path="regular-talk.html"><a href="regular-talk.html#voronoi-linkage-for-spatially-misaligned-data"><i class="fa fa-check"></i><b>1.3.21</b> Voronoi Linkage for Spatially Misaligned Data</a></li>
<li class="chapter" data-level="1.3.22" data-path="regular-talk.html"><a href="regular-talk.html#be-proud-of-your-code-tools-and-patterns-for-making-production-ready-clean-r-code"><i class="fa fa-check"></i><b>1.3.22</b> Be proud of your code! Tools and patterns for making production-ready, clean R code</a></li>
<li class="chapter" data-level="1.3.23" data-path="regular-talk.html"><a href="regular-talk.html#damirseq-2.0-from-high-dimensional-data-to-cost-effective-reliable-prediction-models"><i class="fa fa-check"></i><b>1.3.23</b> DaMiRseq 2.0: from high dimensional data to cost-effective reliable prediction models</a></li>
<li class="chapter" data-level="1.3.24" data-path="regular-talk.html"><a href="regular-talk.html#interpretable-and-accessible-deep-learning-for-omics-data-with-r-and-friends"><i class="fa fa-check"></i><b>1.3.24</b> Interpretable and accessible Deep Learning for omics data with R and friends</a></li>
<li class="chapter" data-level="1.3.25" data-path="regular-talk.html"><a href="regular-talk.html#elevating-shiny-module-with-tidymodules"><i class="fa fa-check"></i><b>1.3.25</b> Elevating shiny module with {tidymodules}</a></li>
<li class="chapter" data-level="1.3.26" data-path="regular-talk.html"><a href="regular-talk.html#apfr-average-power-function-and-bayes-fdr-for-robust-brain-networks-construction"><i class="fa fa-check"></i><b>1.3.26</b> APFr: Average Power Function and Bayes FDR for Robust Brain Networks Construction</a></li>
<li class="chapter" data-level="1.3.27" data-path="regular-talk.html"><a href="regular-talk.html#flexible-meta-analysis-of-generalized-additive-models-with-metagam"><i class="fa fa-check"></i><b>1.3.27</b> Flexible Meta-Analysis of Generalized Additive Models with metagam</a></li>
<li class="chapter" data-level="1.3.28" data-path="regular-talk.html"><a href="regular-talk.html#controlled-r-development-with-docker"><i class="fa fa-check"></i><b>1.3.28</b> Controlled R development with Docker</a></li>
<li class="chapter" data-level="1.3.29" data-path="regular-talk.html"><a href="regular-talk.html#correlaidx---building-r-focused-communities-for-social-good-on-the-local-level"><i class="fa fa-check"></i><b>1.3.29</b> CorrelAidX - Building R-focused Communities for Social Good on the Local Level</a></li>
<li class="chapter" data-level="1.3.30" data-path="regular-talk.html"><a href="regular-talk.html#interactive-visualization-of-complex-texts"><i class="fa fa-check"></i><b>1.3.30</b> Interactive visualization of complex texts</a></li>
<li class="chapter" data-level="1.3.31" data-path="regular-talk.html"><a href="regular-talk.html#bnpmix-an-new-package-to-estimate-bayesian-nonparametric-mixtures"><i class="fa fa-check"></i><b>1.3.31</b> BNPmix: an new package to estimate Bayesian nonparametric mixtures</a></li>
<li class="chapter" data-level="1.3.32" data-path="regular-talk.html"><a href="regular-talk.html#connector-a-computational-approach-to-study-intratumor-heterogeneity."><i class="fa fa-check"></i><b>1.3.32</b> CONNECTOR: a computational approach to study intratumor heterogeneity.</a></li>
<li class="chapter" data-level="1.3.33" data-path="regular-talk.html"><a href="regular-talk.html#gwqs-an-r-package-for-linear-and-generalized-weighted-quantile-sum-wqs-regression"><i class="fa fa-check"></i><b>1.3.33</b> gWQS: An R Package for Linear and Generalized Weighted Quantile Sum (WQS) Regression</a></li>
<li class="chapter" data-level="1.3.34" data-path="regular-talk.html"><a href="regular-talk.html#rlinkedcharts-a-novel-approach-for-simple-but-powerful-interactive-data-analysis"><i class="fa fa-check"></i><b>1.3.34</b> R/LinkedCharts: A novel approach for simple but powerful interactive data analysis</a></li>
<li class="chapter" data-level="1.3.35" data-path="regular-talk.html"><a href="regular-talk.html#transparent-journalism-through-the-power-of-r"><i class="fa fa-check"></i><b>1.3.35</b> Transparent Journalism Through the Power of R</a></li>
<li class="chapter" data-level="1.3.36" data-path="regular-talk.html"><a href="regular-talk.html#deduplicating-real-estate-ads-using-naive-bayes-record-linkage"><i class="fa fa-check"></i><b>1.3.36</b> Deduplicating real estate ads using Naive Bayes record linkage</a></li>
<li class="chapter" data-level="1.3.37" data-path="regular-talk.html"><a href="regular-talk.html#global-poverty-monitoring-at-scale-using-r"><i class="fa fa-check"></i><b>1.3.37</b> Global Poverty Monitoring at scale using R</a></li>
<li class="chapter" data-level="1.3.38" data-path="regular-talk.html"><a href="regular-talk.html#mixed-interactive-debugging-of-r-and-native-code-with-fastr-and-vistual-studio-code"><i class="fa fa-check"></i><b>1.3.38</b> Mixed interactive debugging of R and native code with FastR and Vistual Studio Code</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="shiny-demo.html"><a href="shiny-demo.html"><i class="fa fa-check"></i><b>1.4</b> Shiny demo</a><ul>
<li class="chapter" data-level="1.4.1" data-path="shiny-demo.html"><a href="shiny-demo.html#visualising-and-modelling-bike-sharing-mobility-usage-in-the-city-of-milan"><i class="fa fa-check"></i><b>1.4.1</b> Visualising and Modelling Bike Sharing Mobility usage in the city of Milan</a></li>
<li class="chapter" data-level="1.4.2" data-path="shiny-demo.html"><a href="shiny-demo.html#media-shiny-marketing-mix-models-builder"><i class="fa fa-check"></i><b>1.4.2</b> Media Shiny: Marketing Mix Models Builder</a></li>
<li class="chapter" data-level="1.4.3" data-path="shiny-demo.html"><a href="shiny-demo.html#espres-a-shiny-web-tool-to-support-river-basin-management-planning-in-european-watersheds"><i class="fa fa-check"></i><b>1.4.3</b> ESPRES: A shiny web tool to support River Basin Management planning in European Watersheds</a></li>
<li class="chapter" data-level="1.4.4" data-path="shiny-demo.html"><a href="shiny-demo.html#how-green-is-your-portfolio-tracking-c02-footprint-in-the-insurance-sector"><i class="fa fa-check"></i><b>1.4.4</b> How green is your portfolio ? Tracking C02 footprint in the insurance sector</a></li>
<li class="chapter" data-level="1.4.5" data-path="shiny-demo.html"><a href="shiny-demo.html#decision-support-for-maritime-spatial-planning"><i class="fa fa-check"></i><b>1.4.5</b> Decision support for maritime spatial planning</a></li>
<li class="chapter" data-level="1.4.6" data-path="shiny-demo.html"><a href="shiny-demo.html#automated-receptive-and-interactive-a-classroom-based-data-generation-exercise-using-shiny"><i class="fa fa-check"></i><b>1.4.6</b> Automated, receptive and interactive: a classroom-based data generation exercise using Shiny</a></li>
<li class="chapter" data-level="1.4.7" data-path="shiny-demo.html"><a href="shiny-demo.html#tsviz-a-data-scientist-friendly-addin-for-rstudio"><i class="fa fa-check"></i><b>1.4.7</b> tsviz: a data-scientist-friendly addin for RStudio</a></li>
<li class="chapter" data-level="1.4.8" data-path="shiny-demo.html"><a href="shiny-demo.html#mobility-scan"><i class="fa fa-check"></i><b>1.4.8</b> Mobility scan</a></li>
<li class="chapter" data-level="1.4.9" data-path="shiny-demo.html"><a href="shiny-demo.html#developing-shiny-applications-to-facilitate-precision-agriculture-workflows"><i class="fa fa-check"></i><b>1.4.9</b> Developing Shiny applications to facilitate precision agriculture workflows</a></li>
<li class="chapter" data-level="1.4.10" data-path="shiny-demo.html"><a href="shiny-demo.html#guinterp-a-shiny-gui-to-support-spatial-interpolation"><i class="fa fa-check"></i><b>1.4.10</b> “GUInterp”: a Shiny GUI to support spatial interpolation</a></li>
<li class="chapter" data-level="1.4.11" data-path="shiny-demo.html"><a href="shiny-demo.html#scoring-the-implicit-association-test-has-never-been-easier-dscoreapp"><i class="fa fa-check"></i><b>1.4.11</b> Scoring the Implicit Association Test has never been easier: DscoreApp</a></li>
<li class="chapter" data-level="1.4.12" data-path="shiny-demo.html"><a href="shiny-demo.html#interactive-project-management-tool-using-shiny"><i class="fa fa-check"></i><b>1.4.12</b> Interactive project management tool using shiny</a></li>
<li class="chapter" data-level="1.4.13" data-path="shiny-demo.html"><a href="shiny-demo.html#rtrhexng-hexagon-sticker-app-for-rtrng"><i class="fa fa-check"></i><b>1.4.13</b> rTRhexNG: Hexagon sticker app for rTRNG</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">eRum2020 Program</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="poster" class="section level2">
<h2><span class="header-section-number">1.2</span> Poster</h2>
<div id="modified-likelihood-ratio-model-for-handwriting-recognition-in-forensic-science." class="section level3">
<h3><span class="header-section-number">1.2.1</span> Modified likelihood ratio model for handwriting recognition in forensic science.</h3>
<p><em>Adeyinka Abiodun, The University of Ibadan, Oyo state Nigeria, Research student</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>My session will focus on Forensic Handwriting recognition in which I adopted the Modified Likelihood Ratio (LR) Method in order to quantify the strength of evidence in a forensic investigation. Existing methods for estimating LR in handwriting identification employed nuisance parameters resulting into high rate of inconclusiveness and disagreement among forensic investigators. Currently, LR procedures rely on the choice of appropriate denominators that limit the repeatability and reproducibility of the estimated LR. Therefore, my work adopted a modified LR devoid of nuisance parameter and capable of generating consistent estimate.</p>
<p>A total of 230 document writers were purposively selected to produce 10 paged true and disguised documents over a period of six months. Similar procedure was carried out to produce forged document for the corresponding true counterparts. I used C-means to cluster handwriting into characters based on segmented words. Local binary pattern was used to extract features from the clustered characters and extracted features were fed into a Back Propagation Neural Network (BPNN) to learn the handwriting pattern. I also developed an exhaustive mapping algorithm with bias function to replace the hitherto randomly selected denominator for the LR estimation. My model was trained and tested for repeatability and reproducibility via accuracy and discriminating power for both hypothesis of prosecutor (Hp) and hypothesis of defense (Hd).The modified LR using Kernel Density Estimator (KDE) and Logistic Regression (LoR) estimator outperformed the existing procedures in literature.</p>
</div>
<div id="the-r-package-flexreg-regression-mixture-models-for-bounded-responses" class="section level3">
<h3><span class="header-section-number">1.2.2</span> The R-package ‘FlexReg’: regression mixture models for bounded responses</h3>
<p><em>Agnese Maria Di Brisco, Postdoctoral researcher</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>The analysis of data defined on bounded intervals (such as proportions or rates) is challenging since classical linear regression models are unsuitable. A fruitful alternative to data transformation is the definition of a regression model based on distributions with proper support, like the beta, the flexible beta (FB) and the variance inflated beta (VIB), where the latter two are mixtures of betas showing a good fit even in the presence of multimodality, heavy tails and/or outliers.
This talk illustrates the FlexReg package, which allows to fit Beta, FB, and VIB regression models. The core function of the package is ‘flexreg’, which performs a Bayesian estimation via Hamiltonian Monte Carlo (HMC) algorithm through rstan package. Among the many arguments of the function, there are the model (Beta, FB or VIB) and the link functions for the mean and for the precision parameters. Also, the function allows specifying several prior distributions, the hyperparameters, and many settings of the HMC algorithm such as the number of iterations and of chains.
The FlexReg package includes several functions to compute fitting criteria, posterior predictive distributions and residuals. At last, functions that provide simple and clear plots of the regression curves, posterior predictive and residuals are available. All the features of the FlexReg package are illustrated through data from the literature.</p>
</div>
<div id="using-r-for-analysis-of-microscopic-images-of-streptomyces-growth-and-chromosome-distribution" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Using R for analysis of microscopic images of Streptomyces growth and chromosome distribution</h3>
<p><em>Agnieszka Strzałka, University of Wrocław</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Streptomyces are soil dwelling bacterias known as producers of antibiotics. However, Streptomyces life cycle is more complicated than most model bacterial species. It starts from a single spore, which after germination creates a branched mycelium comprised of elongated cells possessing multiple copies of chromosome. Only after nutrients depletion another type of cell is produced which divides into spores containing only one copy of the chromosome. In order to study chromosomes distribution we have used a fluorescently labelled protein which binds to the chromosome. This allowed us to follow the movement of chromosomes in the cell.</p>
<p>Most software dedicated for analysis of fluorescent microscopy data were created for rod shaped bacteria and are unsuitable for branching bacteria such as Streptomyces. To solve this problem our workflow combined usage of ImageJ and R, which allowed us to collect fluorescence intensity data along the cell and then to find local maximas of fluorescence signal using R Peaks package. We have also developed a method for analysis of segregation defects occurring at the last stage of Streptomyces growth.</p>
<p>With this approach we have shown that Streptomyces chromosomes follow the tip of the growing cell, with the first chromosome being the most tightly tethered, and that various factors such as DNA supercoiling can affect chromosome distribution in the cell.</p>
</div>
<div id="a-flexible-dashboard-for-monitoring-platform-trials" class="section level3">
<h3><span class="header-section-number">1.2.4</span> A flexible dashboard for monitoring platform trials</h3>
<p><em>Alessio Crippa, Karolinska Institutet, postdoc</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>The Data and Safety Monitoring Board (DSMB) is an essential component for a successful clinical trial. It consists of an independent group of experts that periodically revise and evaluate the accumulating data from an ongoing trial to assess patients’ safety, study progress, and drug efficacy. Based on their evaluation, a recommendation to continue, modify or stop the trial will be delivered to the trial’s sponsor. It is essential to provide the DSMB with the best delivery visualization tools for monitoring on a regular basis the live data from the study trial.
We designed and developed an interactive dashboard using flexdashboard for R as a helping tool for assisting the DSMB in the evaluation of the results of the ProBio study, a clinical platform for improving treatment decision in patients with metastatic castrate resistant prostate cancer. We will focus on the customized structure for best displaying the most interesting variables and the adoption of interactive tools as a particularly useful aid for the assessment of the ongoing data. We will also cover the connection to the data sources, the automatic generation process, and the selected permission for the people in the DSMB to access the dashboard.</p>
</div>
<div id="prda-package-enhancing-statistical-inference-via-prospective-and-retrospective-design-analysis." class="section level3">
<h3><span class="header-section-number">1.2.5</span> PRDA package: Enhancing Statistical Inference via Prospective and Retrospective Design Analysis.</h3>
<p><em>Angela Andreella, University of Padua</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>There is a growing recognition of the importance of power analysis and calculation of the appropriate sample size when planning a research experiment. However, power analysis is not the only relevant aspect of the design of an experiment. Other inferential risks, such as the probability of estimating the effect in the wrong direction or the average overestimation of the actual effects, are also important. The evaluation of these inferential risks as well as the statistical power, in what Gelman and Carlin (2014) defined as Design Analysis, may help researchers to make informed choices both when planning an experiment or evaluating study results.
We introduce the PRDA (Prospective and Retrospective Design Analysis) package that allows researchers to carry a Design Analysis under different experimental scenarios (Altoè et al., 2020). Considering a plausible effect size (or its prior distribution) researchers can evaluate either the inferential risks for given sample size or the required sampled size to obtain a given statistical power.
Previously, PRDA functions were limited to mean differences between groups considering Cohen’s d in the Null significance Hypothesis Testing (NHST) framework. Now, we present the newly developed features that include other effect sizes (such as Pearson’s correlation) as well as Bayes Factor hypothesis testing.</p>
</div>
<div id="corpfinder--a-new-application-to-identify-large-corporate-risks" class="section level3">
<h3><span class="header-section-number">1.2.6</span> CorpFinder- a new application to identify Large Corporate Risks</h3>
<p><em>Antoine Logean, Swiss Re, Casualty R&amp;D Department, Senior Data Scientist</em></p>
<p><strong>Track(s):</strong> R Production, R Dataviz &amp; Shiny, R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Large corporations are often viewed as being responsible for major losses to business, nature, or our societies overall. The importance of large institutions is not limited to a handful of market competitors and well-known brands. Fueled by major scandals, their subsidiaries are also considered to be harmful. Recently, this perception has notably increased the willingness to file lawsuits against little-known companies whose ultimate parent entities are well-known brands.
From the insurance perspective, this trend is also alarming. The impact of large corporations materializes through exposures to potentially high monetary losses. Because of the complex corporate ownership relationships, the full extent of exposure to the so-called Large Corporate Risks (LCR), however, may remain hidden to insurance analysts.
To tackle this problem, Swiss Re’s Casualty R&amp;D department is developing a new R-Shiny-based application that allows to uncover these Large Corporate Risks in insurance portfolios. The application uses a fuzzy matching algorithm to analyze and compare the company names with an internal list of well-known corporate institutions and their daughter companies. The insurance analysts can download summary statistics and visualizations of the LCR results.
During the talk, we will present the development and data analysis steps required for the application and discuss the initial quantitative findings to Large Corporate Risks.</p>
</div>
<div id="automate-flexdashboard-with-github" class="section level3">
<h3><span class="header-section-number">1.2.7</span> Automate flexdashboard with GitHub</h3>
<p><em>Binod Jung Bogati, Data Analyst Intern at VIN</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>flexdashboard is a great tool for building an interactive dashboard in R. We can host it for free on GitHub Pages, Rpubs and many other places.</p>
<p>Hosted flexdashboard is static so changes in our data we need to manually update and publish every time. If we want to auto-update we may need to integrate Shiny. However, it may not be suitable for every case.</p>
<p>To overcome this, we have a solution called GitHub Action. It’s a feature from GitHub which automates our tasks in a convenient way.</p>
<p>With the help of GitHub Actions, we can automate our flexdashboard (Rmarkdown) updates. It builds a container that runs our R scripts. We can trigger it every time we push on GitHub or schedule it every X minutes/hours/days/month.</p>
<p>If you want to learn more about the GitHub Action. And also know how to automate updates on your flexdashboard. Please do come and join me.</p>
</div>
<div id="it-process-optimization-in-data-lake-via-r" class="section level3">
<h3><span class="header-section-number">1.2.8</span> IT Process Optimization in Data Lake via R</h3>
<p><em>Ceyda Sol, INGTECH NL , Data analyst</em></p>
<p><strong>Track(s):</strong> R Production, R Applications, R World</p>
<p><strong>Abstract:</strong></p>
<p>This poster refers to an IT case study application in the area of finance industry. A different machine learning models are applied in order to estimate the file arrival times to the servers of a Data Lake. The aim is to be able to trigger alert mechanisms in case of a missing delivery and optimize the workload of the servers. This study is significant in the sense of satisfying the on-time regulatory reporting requirements of the other departments in the bank. The results indicates that the model is able to estimate the time arrivals with a 90% correctness.</p>
</div>
<div id="level-up-your-tables-with-tablehtml-in-r" class="section level3">
<h3><span class="header-section-number">1.2.9</span> Level up your tables with tableHTML in R</h3>
<p><em>Dana Jomer, IT Power Services, Data Scientist</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>One of the most important tasks in data science is to communicate findings to a non-technical audience. Data visualisation is a powerful tool to make insights actionable. The R ecosystem has wonderful tools, such as ggplot2, to create powerful visualisations. While working on a project that would automate a planning process done manually by a business analyst using Excel, we were faced with a challenge to come up with a report that would look and feel like Excel but could be easily automated. Since the planning algorithm was implemented in R, it was only natural to use it for this purpose. In the process of creating the report, we found that when it comes to showing tables that can be easily styled to make them visually appealing or even to highlight certain values based on some logic, that should not be hard coded and ready to be automated, tableHTML would be the perfect package to do just that.
tableHTML is an R package to create and style HTML tables with CSS from a data.frame or matrix. These tables can be exported and used in any application that accepts HTML, e.g. shiny or rmarkdown. In this talk, we will show how tableHTML can be used in a business context where Excel is commonly used in the business to interact with data in tabular form, how to adjust the appearance to align it to the corporate identity guidelines, and how conditional formatting can help to put emphasis on the most important information.</p>
</div>
<div id="guidance-for-teaching-r-to-non-programmers" class="section level3">
<h3><span class="header-section-number">1.2.10</span> Guidance for teaching R to non-programmers</h3>
<p><em>Dean Langan, Senior Teaching Fellow (University College London, UK)</em></p>
<p><strong>Track(s):</strong> R Life Sciences, R World</p>
<p><strong>Abstract:</strong></p>
<p>The Centre for Applied Statistics Courses (CASC) at University College London (UCL) provide short courses on statistics and statistical software packages. Popular day-courses include a well-established ‘Introduction to R’ course and the newly developed ‘Further Topics in R’. In the latter, attendees are taught intermediate-level topics such as loops and conditional statements. Attendees range from postgraduate students, academic researchers and data analysts in the private sector without a strong background in statistics or programming. First, we highlight some issues with providing our training course to this demographic, derived from our experience and from anonymous online feedback. Second, we discuss some of our solutions to these issues that have shaped our course over time. For example, one issue is catering to a wide audience from differing fields, different levels of computer literacy and approaches to learning. To address this, we prepare for a high level of flexibility on the day and include intermittent practical exercises to get real time feedback on the abilities of attendees. Finally, we reviewed the experiences of other teachers on similar courses documented online and compared these experiences with our own. We offer guidance to other teachers running or developing courses for intermediate-level R programming.</p>
</div>
<div id="transparent-presentation-of-uncertain-lotteries-using-deals" class="section level3">
<h3><span class="header-section-number">1.2.11</span> Transparent presentation of uncertain lotteries using {deals}</h3>
<p><em>Dmytro Perepolkin, Lund University</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>Making decisions under uncertainty is hard. Scholars of human decision making have identified a number of decision paradoxes, related to how we perceive and choose among uncertain prospects (e.g. Allais Paradox and Ellsberg Paradox). Recently a few remedies have been proposed which help alleviate some risk and/or ambiguity aversion by careful choice of visual presentation of uncertainty. We introduce R package which makes it easy to create, transform and visualize uncertain lotteries and extend the previously published results for continuous variables.</p>
</div>
<div id="newwave-a-scalable-r-package-for-the-dimensionality-reduction-of-single-cell-rna-seq" class="section level3">
<h3><span class="header-section-number">1.2.12</span> NewWave: a scalable R package for the dimensionality reduction of single-cell RNA-seq</h3>
<p><em>Federico Agostinis, Università degli studi di Padova, Fellowship</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>The fast development of single cell sequencing technologies in the recent
years has generated a gap between the throughput of the experiments and the
capability of analizing the generated data.
One recent method for dimensionality reduction of single-cell RNA-seq data is
zinbwave, it uses zero inflated negative binomial likelihood function optimization
to find biological meaningful latent factors and remove batch effect. Zinbwave
has optimal performance but has some scalability issues due to large memory
usage. To address this, we developed an R package with new software architec-
ture extending zinbwave.
In this package, we implement mini-batch stochastic gradient descent and the
possibility of working with HDF5 files. We decide to use a negative binomial
model following the observation that droplet sequencing technologies do not
induce zero inflation in the data. Thanks to these improvements and the possi-
bility of massively parallelize the estimation process using PSOCK clusters, we
are able to speed up the computations with the same or even better results than
zinbwave. This type of parallelization can be used on multiple hardware setups,
ranging from simple laptops to dedicated server clusters. This, paired with the
ability to work with out-of-memory data, enables us to analyze datasets with
milions of cells.</p>
</div>
<div id="orf-ordered-random-forests" class="section level3">
<h3><span class="header-section-number">1.2.13</span> orf: Ordered Random Forests</h3>
<p><em>Gabriel Okasa, Research Assistant and PhD Candidate at the Swiss Institute for Empirical Economic Research, University of St. Gallen, Switzerland</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>The R package ‘orf’ is a software implementation of the Ordered Forest estimator as developed in Lechner and Okasa (2019). The Ordered Forest flexibly estimates the conditional class probabilities of models involving categorical outcomes with an inherent ordering structure, known as ordered choice models. Additionally to common machine learning algorithms, the Ordered Forest enables estimation of marginal effects together with statistical inference and thus provides comparable output as in standard econometric models. Accordingly, the ‘orf’ package provides generic R functions to estimate, predict, plot, print and summarize the estimation output of the Ordered Forest along with various options for specific forest-related tuning parameters. Finally, computational speed is ensured as the core forest algorithm relies on the fast C++ forest implementation from the ranger package (Wright and Ziegler 2017).</p>
</div>
<div id="biomarker-discovery-by-the-leveraging-of-omic-and-clinical-data-using-biomarkerbox." class="section level3">
<h3><span class="header-section-number">1.2.14</span> Biomarker discovery by the leveraging of omic and clinical data using biomarkeRbox.</h3>
<p><em>Giulio Ferrero, University of Turin</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>The diffusion of sequencing technologies rapidly increased the availability of omic data useful to identify novel molecular biomarkers of human pathological phenotypes. However, omic information remains far to be easily integrated with clinical parameters as well as with lifestyle, dietary information and other meta-data. To overcome this problem, we designed biomarkeRbox, an R pipeline for the joined integrative analyses of omic data (e.g. transcriptomic) with clinical and other meta-data. The pipeline includes different modules designed for: i) data quality control and preprocessing; ii) covariate analysis; iii) omic data analysis; and iv) the joined analysis of omic and covariate data to identify candidate biomarkers for the studied disease(s) using machine learning techniques. The pipeline can be either fully automatized with the generation of analysis reports with R markdown or customized using an ad-hoc Shiny interface. The pipeline has been recently tested in a case-control study in which microRNA profiles have been investigated as biomarkers for colorectal cancer subtypes.</p>
</div>
<div id="an-package-for-bayesian-analysis-of-structured-time-series-models-with" class="section level3">
<h3><span class="header-section-number">1.2.15</span> An  package for Bayesian analysis of structured time series models with </h3>
<p><em>Izhar Asael Alonzo Matamoros, Universidad Nacional Autónoma De Honduras</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Applications</p>
<p><strong>Abstract:</strong></p>
<p> is an R package for Bayesian analysis of time series models using Stan. The package offers a dynamic way to choose your model, define priors in a wide range of distributions, check model’s fit, and forecast with the m-steps ahead predictive distribution. The users can widely choose between implemented models such as . Every model constructor in varstan defines weakly informative priors, but prior specifications can be changed in a dynamic and flexible way, so the prior distributions reflect the parameter’s initial beliefs.\</p>
<p>For model selection, the package offers the classical information criteria, such as AIC, AICc, BIC, DIC, Bayes factor, and more recent criteria such as Widely-applicable information criteria (), and the Bayesian leave one out cross-validation (). In addition, a Bayesian version for automatic order selection in seasonal ARIMA and dynamic regression models can be used as an initial step for the time series analysis.\</p>
</div>
<div id="bridging-the-gap-between-r-and-computer-vision" class="section level3">
<h3><span class="header-section-number">1.2.16</span> Bridging the gap between R and computer vision</h3>
<p><em>Lubomír Štěpánek, Biostatistician, Software Developer, Junior Lecturer, PhD Candidate at First Faculty of Medicine, Charles University &amp; Faculty of Biomedical Engineering, Czech Technical University in Prague (CZ)</em></p>
<p><strong>Track(s):</strong> R Applications, R Dataviz &amp; Shiny, R Life Sciences, R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>R language is, without any doubt, a powerful tool well suitable for the vast majority of most analytical tasks arising from an everyday routine of a statistician, scientist, or data analyst. However, there are fields of data processing that R is not so much usable for, and, in addition, other languages or tools such as Python or Octave are preferred to handle them more natively. One of the fields is image processing and particularly facial image processing or computer vision, where there are barely original R packages currently available.</p>
<p>There are, of course, some approaches how to handle missing libraries in R. In general, an API library could bridge the gap between R on the one hand, and the unique tool (usually built in a different programming language) on the other hand. In this work, we tried to rethink the problem and go even further such that we have developed a web-based shiny application providing facial image processing and especially automated facial landmarking. The facial landmarking is powered using C++ library dlib, dedicated to machine-learning based computer vision algorithms, but due to its origin, it is primarily available only for C++ speaking users.</p>
<p>The connection between R, C++ dlib library, and well-known R package shiny could open R functionality and computing power to a broader audience using a comfortable and clickable way.</p>
</div>
<div id="how-r-hub-can-help-you-develop-and-maintain-your-r-packages" class="section level3">
<h3><span class="header-section-number">1.2.17</span> How R-hub can help you develop and maintain your R packages</h3>
<p><em>Maëlle Salmon, R-hub</em></p>
<p><strong>Track(s):</strong> R World</p>
<p><strong>Abstract:</strong></p>
<p>R-hub is a collection of services for all package developers, from newbies to seasoned maintainers. It is funded by the R Consortium as a top level project. This poster will showcase a roundup of R-hub services.</p>
<p>On the tech part, R-hub’s flagship product is its package builder: Run <code>R CMD check</code> on 15 different platforms (operating systems, R versions…), from R! The package builder and its companion R package <code>rhub</code> can help prepare a CRAN submission, as well as debug a CRAN check failure by mimicking CRAN platforms. As of December 2019, it had been used nearly 85,000 times, by 2507 users to check 4418 unique packages. Other R-hub services include a package for finding R packages on CRAN (<code>pkgsearch</code>), badges for CRAN packages, and a mirror of CRAN source code.</p>
<p>On the guidance part, R-hub has recently gained a docs website. Furthermore, R-hub also has an active blog illustrating R-hub services, as well as other topics relevant to package developers. Last but not least, the R-hub project has three communication channels: a gitter channel, a GitHub issue tracker, and a Twitter account.</p>
<p>Come see this poster to learn more about R-hub and to tell us about your experience!</p>
</div>
<div id="power-supply-health-status-monitoring-dashboard" class="section level3">
<h3><span class="header-section-number">1.2.18</span> Power Supply health status monitoring dashboard</h3>
<p><em>Marco Calderisi, Kode srl, CTO</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>The Primis project dashboard allows to perform an analysis on the health status of power supplies boards on two levels:
(1) analysis of a specific board, to check its status and the presence of any anomalies,
(2) analysis of multiple boards within a single Power Supply, to check if the set of boards reveals abnormal behavior and if some boards behave in a distinctly different way from the others.
The analysis algorithms and the web application were created using the programming language R, and in particular the Shiny library.
The application is therefore divided into two parts that reflect these different types of analysis, called respectively “Product View” (analysis and diagnostics of a specific board) and “Product Comparison” (comparison analysis between multiple boards of the same Power Supply). Both analyzes can be carried out on an arbitrary time interval, selectable through a special application menu.
The analysis is carried out by means of:
(1) univariate analysis, focusing on a specific parameter of one or more channels and displaying aggregate information regarding the status of the board in the entire observation period
(2) multivariate analysis, that is the application of multivariate algorithms that allows to perform an overall analysis of the board, taking into account all the variables simultaneously.</p>
</div>
<div id="first-year-ict-students-dropout-predicting-with-r-models" class="section level3">
<h3><span class="header-section-number">1.2.19</span> First-year ICT students dropout predicting with R models</h3>
<p><em>Natalja Maksimova, Virumaa College of Tallinn University of Technology, lecturer</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>The aim of this study is to find how it is possible to predict first-year ICT students dropout in one Estonian college, Virumaa College of Tallinn University of Technology (TalTech) and possibly to engage methods to decrease dropout rate. We perform three approaches of machine learning using R tools: logistic regressions, decision trees and Naive Bayes to predict. The models are computed on the basis of the TalTech study information system data. As a result, we propose a methodical approach that may be realized in practice at other institutions.
All applied methods yield high prediction with more than 85% accuracies. In the same time some influencing and non-influencing factors were found in predicting ICT students’ dropout.</p>
</div>
<div id="benchmark-percentage-disjoint-data-splitting-in-cross-validation-for-assessing-the-skill-of-machine" class="section level3">
<h3><span class="header-section-number">1.2.20</span> Benchmark Percentage Disjoint Data Splitting in Cross Validation for Assessing the Skill of Machine</h3>
<p><em>Olalekan Joseph Akintande, University of Ibadan, Ph.D. Student</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>The controversies surrounding dataset splitting technique and folklore of what has been or what should be, remain an open debate. Several authors (bloggers, researchers, and data scientists) in the field of machine learning and similar research areas, have proposed various arbitrary percentage disjoint dataset splitting (DDS) options for validating the skill of machine learning algorithms and by extension the appropriate percentage DDS based on cross-validation techniques. In this work, we propose benchmarks for which the percentage DDS procedure should be based. These benchmarks are founded on various training sizes (m) and serve as the basis and justification for the choice of an appropriate percentage DDS for assessing the skill of ML algorithms and related fields, on the concept of cross-validation techniques.</p>
</div>
<div id="integrating-professional-software-engineering-practices-in-medical-research-software" class="section level3">
<h3><span class="header-section-number">1.2.21</span> Integrating professional software engineering practices in medical research software</h3>
<p><em>Patricia Ryser-Welch, Newcastle University, Population Health Science Institute, Research Associate,</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Health data sets are getting bigger, more complex, and are increasingly being linked up with other data sources. With this trend there is an increasing risk of patient identification and disclosure. Two different ways of mitigating this risk are to use a federated analysis approach or to use a data safe haven.</p>
<p>DataSHIELD (www.datashield.ac.uk) is an established federated data analysis tool that is used in the medical sciences. This software has a variety of methods to reduce the risk of disclosure built in. Here we describe the steps we are taking to apply modern software engineering methodologies to DataSHIELD. The upcoming Medical Devices legislation requires that software has more rigourous testing done on it. While this legislation does not directly apply to software used for research, we think it is important the ideas behind this do filter down to research software. For us these principles include testing that functions work, as well as testing that they produce the correct answers. Using a static standard data set to test against (that is publicly available) is also an important aspect. This work is being done in a continuous integraion framework using Microsoft Azure. Additionally all our software is developed as open source.</p>
<p>In addition to the protection DataSHIELD provides on its own we are also integrating it into our Trusted Research Environment as part of Connected Health Cities North East and North Cumbria. This will give an extra level of protection to data that may automatically flow from multiple data sources. Additionally, as analysis can be done in a federated way it means that that data does not need to leave its data controller’s environment. This opens up the possibility of analysis happening across trusts and regions.</p>
</div>
<div id="a-three-parameter-gompertz-lindley-distribution-its-properties-and-applications" class="section level3">
<h3><span class="header-section-number">1.2.22</span> A Three-Parameter Gompertz-Lindley Distribution: Its Properties And Applications</h3>
<p><em>Peter Koleoso, Department of Statistics, University of Ibadan, Oyo State, Nigeria.</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Life Sciences, R Applications</p>
<p><strong>Abstract:</strong></p>
<p>This research proposed a three-parameter probability distribution called Gompertz-Lindley distribution using Gompertz generalized (Gompertz-G) family of distributions. The distribution was proposed as an improvement on the Lindley distribution, for greater flexibility when modelling real life and survival data. The mathematical properties of the distribution such as moment, moment generating function, survival function and hazard function were derived. The parameters of the distribution were estimated using the method of maximum likelihood and the distribution was applied to model the strength of glass fibres. The maximum likelihood estimation and the modelling were carried out using R software. The proposed Gompertz-Lindley distribution performed best in modelling the strength of glass fibres (AIC = 62.8537), followed by Generalized Lindley distribution (AIC = 77.6237). The Lindley distribution had the least performance with the highest information criterion values.</p>
</div>
<div id="partitional-clustering-with-extensions" class="section level3">
<h3><span class="header-section-number">1.2.23</span> Partitional clustering with extensions</h3>
<p><em>Tero Lähderanta, Research Unit of Mathematical Sciences, University of Oulu</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Clustering is one of the most well-known machine learning methods. The most used algorithm for clustering is k-means, where the clusters are formed by minimizing the squared Euclidean distances between the data points and cluster centers. Very similar optimization problem can be found in location-allocation literature, which tackle the problem of placing facilities into a 2d region with demand points, while simultaneously minimizing the average distance from each point to its facility. Both frameworks are very topical problems today as the amount of spatial data is ever-growing.</p>
<p>In both frameworks, the demand for different types of extensions in the optimization task is high due to the variety of the problems. The most common extensions are the inclusion of capacity limits for cluster sizes, outliers and different distance metrics and membership types. There already exists many extension approaches which focus on a single extension. However, in many scenarios multiple simultaneous extensions are needed.</p>
<p>Our approach enables such simultaneous use of different extensions by combining constraints and penalties to a single objective function. Furthermore, we provide an easy-to-use R package rpack to formulate and solve problems from both frameworks with above-mentioned extensions. Secondly, we show examples of real-world scenarios where the package can be used.</p>
</div>
<div id="dealing-with-changing-administrative-boundaries-the-case-of-swiss-municipalities" class="section level3">
<h3><span class="header-section-number">1.2.24</span> Dealing with changing administrative boundaries: The case of Swiss municipalities</h3>
<p><em>Tobias Schieferdecker, Daily dealings with data at cynkra</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Switzerland’s municipalities are frequently merged or reorganized, in an attempt to reduce costs and increase efficiency.
These mergers create a substantial problem for data analysis.
Often it is desirable to study a municipality over time.
But in order to create a time series for a region of interest, its borders should stay constant.
Our goal is to provide R-functions that allow an easy and consistent handling of these mergers.
We create a mapping table for municipality ID’s for a specified period of time, that allows us to track the mergers over time.
We also provide weights, such as population as well as area of the municipalities, to facilitate the construction of weighted time series.
Various other municipality mutations are also taken into account.</p>
<p>We are creating two R packages: an infrastructure package that handles the task of keeping the data up to date; and a user package that contains the functions to deal with the mergers.</p>
</div>
<div id="baddea-an-r-package-for-measuring-firms-efficiency-adjusted-by-undesirable-outputs" class="section level3">
<h3><span class="header-section-number">1.2.25</span> badDEA: An R package for measuring firms’ efficiency adjusted by undesirable outputs</h3>
<p><em>Yann Desjeux, INRAE, France</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Growing concerns on the detrimental effects of human production activities on the environment, e.g. air, soil and water pollution, have triggered the development of new performance indicators (including productivity and efficiency measures) accounting for such undesirable impacts. Firms can now be benchmarked not only in terms of economic performance, but also in terms of environmental performance linked to production. In the performance benchmarking literature, and more specifically the one on the non-parametric approach Data Envelopment Analysis (DEA), several methodologies have been developed to consider these impacts as undesirable (or bad) outputs. Related empirical applications in the literature, performed with various software, show that conclusions differ depending on the way these undesirable outputs are introduced. However none of these methodologies are routinely developed in R. In this context, we developed the badDEA package in order to provide a consistent and single framework where users (students, researchers, practitioners) can find the major methodologies proposed in the literature to compute efficiency measures that are adjusted by undesirable outputs. In this presentation, we will describe the aim, structure and options of the badDEA package, unfolding all the methodologies in their different variants and providing a promising tool for decision-making.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lightning-talk.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regular-talk.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
